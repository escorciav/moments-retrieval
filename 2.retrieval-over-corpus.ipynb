{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "## Task 2: Retrieval over corpus\n",
    "Compute pdist2\n",
    "\n",
    "- Extract features from MCN\n",
    "\n",
    "    `bash dump_features.sh`\n",
    "    \n",
    "- Move features\n",
    "\n",
    "    `mv ../localizing-moments/results/*.hdf5 data/interim/mcn/features/`\n",
    "\n",
    "- Class to parse and interact with corpus\n",
    "    - ~~reading hdf5~~\n",
    "    - ~~make dictionary~~\n",
    "    - ~~~make corpus matrix~~\n",
    "    - ~~Method to return video and segment~~\n",
    "    - ~~grab possible segments~~\n",
    "    - optional: reading ground-truth\n",
    "\n",
    "- Compute distance and retrieve sorted samples for a given vector query\n",
    "    - optional: batch computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3076 (146, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([432, 402, 975, ..., 934, 226,   7]),\n",
       " array([ 2,  4,  3, ..., 10, 10, 10]),\n",
       " array([0.38135153, 0.39323872, 0.40213436, ..., 1.4829735 , 1.550116  ,\n",
       "        1.5524127 ], dtype=float32))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "POSSIBLE_SEGMENTS = [(0,0), (1,1), (2,2), (3,3), (4,4), (5,5)]\n",
    "for i in itertools.combinations(range(6), 2):\n",
    "    POSSIBLE_SEGMENTS.append(i)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    \"\"\"Corpus of videos with clips of interest to index\n",
    "    \n",
    "    TODO\n",
    "        batch indexing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, segments=POSSIBLE_SEGMENTS):\n",
    "        self.segments = np.array(segments)\n",
    "        self._create_repo(filename)\n",
    "        self._create_feature_matrix()\n",
    "    \n",
    "    def ind_to_repo(self, i):\n",
    "        \"retrieve video and segment index\"\n",
    "        # purpose: given index in matrix return video\n",
    "        video_index = i // self.T\n",
    "        segment_index = i % self.T\n",
    "        return video_index, segment_index\n",
    "    \n",
    "    def search(self, x):\n",
    "        distance = ((self.features - x)**2).sum(axis=1)\n",
    "        distance_sorted_idx = np.argsort(distance)\n",
    "        distance_sorted = distance[distance_sorted_idx]\n",
    "        video_idx, segment_idx = self.ind_to_repo(distance_sorted_idx)\n",
    "        return video_idx, segment_idx, distance_sorted\n",
    "    \n",
    "    def _create_repo(self, filename):\n",
    "        \"read hdf5 and make corpus repo\"\n",
    "        self.container = OrderedDict()\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            for k, v in f.items():\n",
    "                self.container[k] = v[:]\n",
    "        self.videos = np.array(list(self.container.keys()))\n",
    "        self.T, self.D = self._grab_sample_value().shape\n",
    "        self.num_videos = len(self.videos)\n",
    "        assert self.T == len(self.segments)\n",
    "    \n",
    "    def _create_feature_matrix(self):\n",
    "        \"make corpus matrix\"\n",
    "        # purpose: perform search without loop over repo\n",
    "        dtype = self._grab_sample_value().dtype\n",
    "        self.features = np.empty((self.num_videos * self.T, self.D),\n",
    "                               dtype=dtype)\n",
    "        for i, (_, v) in enumerate(self.container.items()):\n",
    "            r_start = i * self.T\n",
    "            r_end = r_start + self.T\n",
    "            self.features[r_start:r_end, :] = v\n",
    "    \n",
    "    def _grab_sample_value(self, idx=0):\n",
    "        sample_key = self.videos[idx]\n",
    "        return self.container[sample_key]\n",
    "    \n",
    "\n",
    "file_corpus = 'data/interim/mcn/features/corpus_val.hdf5'\n",
    "val_corpus = Corpus(file_corpus)\n",
    "# test video and segment mapping\n",
    "index = np.random.randint(len(val_corpus.features))\n",
    "print(index, val_corpus.ind_to_repo(index))\n",
    "\n",
    "file_queries = 'data/interim/mcn/features/queries_val.hdf5'\n",
    "file_annotations = 'data/raw/val_data.json'\n",
    "\n",
    "with h5py.File(file_queries, 'r') as fid:\n",
    "    sample_key = list(fid.keys())[0]\n",
    "    sample_value = fid[sample_key][:]\n",
    "\n",
    "val_corpus.search(sample_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement evaluation code\n",
    "\n",
    "The metric should reflect the probability of finding a relevant moment, clip inside a video, for a given query `q` among `k` possible moments in the entire corpus.\n",
    "\n",
    "Given that he have multiple annotations per query inside the same video (accounting for inherent temporal ambiguity), we need a \"concensus\" or \"thresholding\" criteria to assess that a given clip is relevant among a pool of annotations.\n",
    "\n",
    "Note: This is not related to the tIOU threshold. Only a single, golden, annotation would circumvent this problem.\n",
    "\n",
    "Concensus - thresholding strategies:\n",
    "- max. Assigns a true if the prediction match any annotation.\n",
    "Makes problem easier as the chance increases proportionally to ambiguity of query. It's sensitive to outliers in annotation process. Probably relevant for tIOU.\n",
    "\n",
    "Metrics\n",
    "\n",
    "- R@k,c\n",
    "\n",
    "    prob of finding a moment on `top-k`.\n",
    "    Here, the moment is relevant when its average rank on the best 3 out of 4 annotations is lower or equal than `j`.\n",
    "\n",
    "- mIOU\n",
    "\n",
    "   \n",
    "- mRank\n",
    "    mean rank.\n",
    "\n",
    "\n",
    "~~Among the thresholding strategies, we have:~~\n",
    "\n",
    "~~- average. Makes problem harder as it forces to agree with multiple annotators. It's also sensitive to outliers.~~\n",
    "\n",
    "~~- average over a subset of annotations. As above but accounting for outliers.~~\n",
    "\n",
    "~~Thresholding implies to compute a given metric a measure the~~\n",
    "~~For consistency with DiDeMo standard, we opt for thresholding:~~\n",
    "\n",
    "Implementation note: given multiple annotations for each query is not conveninent to deal with the raw indexes from the feature matrix. Moreover, those indexes would be useless for considering tIOU. Hopefully, we have a function to invert those indexes into video and segment indexes. However, we need to keep consistency in the list of videos and segments to make an apple 2 apple comparsion.\n",
    "\n",
    "Evaluation pseudo-code\n",
    "\n",
    "```\n",
    "inputs: list of queries; ground-truth, k\n",
    "\n",
    "recall_at_k = []\n",
    "miou = []\n",
    "for each query:\n",
    "    get vector\n",
    "    compute distance and return sorted list of indexes\n",
    "    # prediction would end here\n",
    "    # code for server could continue to provide more info.\n",
    "    # like red to miss and green for hit ;)\n",
    "    \n",
    "    # evaluation\n",
    "    for i in range(k)\n",
    "        if video_idx[i] == gt[query][video_idx]:\n",
    "            check if it's the segment we are looking for\n",
    "            miou.append()\n",
    "        else:\n",
    "            miou.append(0)\n",
    "\n",
    "sum(recall_at_k) / len(recall_at_k)\n",
    "sum(miou_at_k) / len(miou_at_k)\n",
    "```\n",
    "\n",
    "TODO: what is the average iou in this case? max/mean - average_iou\n",
    "\n",
    "DiDeMo evaluation\n",
    "\n",
    "```python\n",
    "average_ranks = []\n",
    "average_iou = []\n",
    "for s, d in zip(segments, data):\n",
    "  pred = s[0]\n",
    "  ious = [iou(pred, t) for t in d['times']]\n",
    "  average_iou.append(np.mean(np.sort(ious)[-3:]))\n",
    "  ranks = [rank(s, t) for t in d['times']]\n",
    "  average_ranks.append(np.mean(np.sort(ranks)[:3]))\n",
    "rank1 = np.sum(np.array(average_ranks) <= 1)/float(len(average_ranks))\n",
    "rank5 = np.sum(np.array(average_ranks) <= 5)/float(len(average_ranks))\n",
    "miou = np.mean(average_iou)\n",
    "```\n",
    "\n",
    "a. Task a\n",
    "- ~~data structure for ground-truth.~~\n",
    "- todo: consistency for indexing. avoid to \n",
    "\n",
    "b. evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'a man on a sled waves as he goes past the camera.',\n",
       " 'dl_link': 'https://www.flickr.com/video_download.gne?id=4273219306',\n",
       " 'num_segments': 6,\n",
       " 'segment_indices': array([ 3,  3,  2, 15]),\n",
       " 'times': [[3, 3], [3, 3], [2, 2], [2, 3]],\n",
       " 'video': '52614599@N00_4273219306_d2d89ed9f0.avi',\n",
       " 'video_index': 701}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GTQueries():\n",
    "    \"Pool of queries with ground-truth\"\n",
    "    \n",
    "    def __init__(self, filename, videos, segments):\n",
    "        self.data = OrderedDict()\n",
    "        self.filename = filename\n",
    "        self.diff_four = [0, 0]  # debugging\n",
    "        self._sanitize(videos, segments)\n",
    "        self._setup_from_file(filename)\n",
    "        \n",
    "    def _add_list_items(self, l):\n",
    "        for i in l:\n",
    "            query_id = i.pop('annotation_id')\n",
    "            i['video_index'] = self.lookup_video[i['video']]\n",
    "            i['segment_indices'] = self._get_segment_indexes(i['times'])\n",
    "            self.data[query_id] = i\n",
    "        self.query_ids = list(self.data.keys())\n",
    "        \n",
    "    def _get_segment_indexes(self, annotations):\n",
    "        idxs = np.array([self.lookup_segment[tuple(i)] for i in annotations])\n",
    "        # debugging\n",
    "        if len(annotations) > 4:\n",
    "            self.diff_four[0] += 1\n",
    "        elif len(annotations) < 4:\n",
    "            self.diff_four[1] += 1\n",
    "        return idxs\n",
    "        \n",
    "    def _sanitize(self, videos, segments):\n",
    "        if videos is None or segments is None:\n",
    "            raise ValueError\n",
    "        assert len(set(videos)) == len(videos)\n",
    "        assert len(set(segments)) == len(segments)\n",
    "        self.lookup_video = dict(zip(videos, range(len(videos))))\n",
    "        self.lookup_segment = dict(zip(segments, range(len(segments))))\n",
    "        \n",
    "    def _setup_from_file(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            self._add_list_items(json.load(f))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "segments = list(map(tuple, val_corpus.segments.tolist()))\n",
    "val_gt = GTQueries(file_annotations,\n",
    "                   val_corpus.videos.tolist(),\n",
    "                   segments)\n",
    "val_gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(segments1, segments2):\n",
    "    \"\"\"Compute pairwise intersection length between segments.\n",
    "    \n",
    "    Args:\n",
    "        segments1 (numpy array): shape [N, 2] holding N segments\n",
    "        segments2 (numpy array): shape [M, 2] holding M segments\n",
    "    Returns:\n",
    "        a numpy array with shape [N, M] representing pairwise intersection length\n",
    "    \"\"\"\n",
    "    [t_min1, t_max1] = np.split(segments1, 2, axis=1)\n",
    "    [t_min2, t_max2] = np.split(segments2, 2, axis=1)\n",
    "\n",
    "    all_pairs_min_tmax = np.minimum(t_max1, np.transpose(t_max2))\n",
    "    all_pairs_max_tmin = np.maximum(t_min1, np.transpose(t_min2))\n",
    "    intersect_length = np.maximum(\n",
    "        np.zeros(all_pairs_max_tmin.shape),\n",
    "        all_pairs_min_tmax - all_pairs_max_tmin)\n",
    "    return intersect_length\n",
    "\n",
    "\n",
    "def length(segments):\n",
    "    \"\"\"Computes length of segments.\n",
    "    \n",
    "    Args:\n",
    "        segments (numpy array): shape [N, 2] holding N segments\n",
    "    Returns:\n",
    "        a numpy array with shape [N] representing segment length\n",
    "    Note:\n",
    "        it works with time, it would be off if using frames.\n",
    "    \"\"\"\n",
    "    return segments[:, 1] - segments[:, 0]\n",
    "\n",
    "\n",
    "def iou(segments1, segments2):\n",
    "    \"\"\"Computes pairwise intersection-over-union between box collections.\n",
    "\n",
    "    Args:\n",
    "        segments1 (numpy array): shape [N, 2] holding N segments\n",
    "        segments2 (numpy array): shape [M, 4] holding N boxes.\n",
    "    Returns:\n",
    "        a numpy array with shape [N, M] representing pairwise iou scores.\n",
    "    \"\"\"\n",
    "    intersect = intersection(segments1, segments2)\n",
    "    length1 = length(segments1)\n",
    "    length2 = length(segments2)\n",
    "    union = np.expand_dims(length1, axis=1) + np.expand_dims(\n",
    "        length1, axis=0) - intersect\n",
    "    return intersect / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@(1, 5, 10)=[0.004545454545454545, 0.014114832535885167, 0.027033492822966507];\n",
      "r@(1, 5, 10),0.75=[0.004545454545454545, 0.014114832535885167, 0.027033492822966507];\n",
      "mRank=1657.46\n",
      "mIOU=0.0043;\n",
      "Elapsed time: 23.846515893936157\n"
     ]
    }
   ],
   "source": [
    "class RetrievalEvaluation():\n",
    "    \"TODO: count search for a given query_id\"\n",
    "    \n",
    "    def __init__(self, corpus_h5, groundtruth_json,\n",
    "                 k=(1,), iou_threshold=0.75):\n",
    "        self.corpus = Corpus(corpus_h5)\n",
    "        videos = self.corpus.videos.tolist()\n",
    "        segments = list(map(tuple, self.corpus.segments.tolist()))\n",
    "        self._precompute_iou()\n",
    "        self.gt_queries = GTQueries(groundtruth_json,\n",
    "                                    videos=videos, segments=segments)\n",
    "        self.k = k\n",
    "        self.iou_threshold = 0.75\n",
    "        self.reset()\n",
    "        self._k = np.array(self.k)\n",
    "        \n",
    "    def eval(self):\n",
    "        recall_at_k = [sum(i) / len(i) for i in self.hit_k]\n",
    "        recall_k_iou = [sum(i) / len(i) for i in self.hit_k_iou]\n",
    "        miou = sum(self.miou) / len(self.miou)\n",
    "        mrank = np.mean(self.rank)\n",
    "        return recall_at_k, recall_k_iou, miou, mrank\n",
    "        \n",
    "    def eval_single_query(self, query, query_id):\n",
    "        # todo encode language vector\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def eval_single_vector(self, vector, query_id):\n",
    "        if query_id not in self.gt_queries.data:\n",
    "            # log-this\n",
    "            pass\n",
    "        (pred_video_indices, pred_segment_indices,\n",
    "         sorted_dist) = self.corpus.search(vector)\n",
    "        true_video = self.gt_queries[query_id]['video_index']\n",
    "        true_segments = self.gt_queries[query_id]['segment_indices']\n",
    "        \n",
    "        # Q&D -> tp_fp_segments\n",
    "        # Note: I keep the entire corpus to compute rank\n",
    "        # Note: np.in1d won't generalize for other criteria\n",
    "        # TODO-p: check if bottleneck. In theory, using a loop\n",
    "        #         and break for fp from other videos sounds efficient.\n",
    "        tp_fp_videos = pred_video_indices == true_video\n",
    "        tp_fp_segments = np.in1d(pred_segment_indices, true_segments)\n",
    "        tp_fp_labels = np.logical_and(tp_fp_videos, tp_fp_segments)\n",
    "        for i, k in enumerate(self.k):\n",
    "            self.hit_k[i].append(tp_fp_labels[:k].sum(dtype=bool))\n",
    "        self.rank.append(np.where(tp_fp_labels)[0].min())\n",
    "        \n",
    "        # Q&D -> mIOU\n",
    "        self.miou.append(0)\n",
    "        if tp_fp_labels[0]:\n",
    "            ious = self.iou_matrix[true_segments, pred_segment_indices[0]]\n",
    "            self.miou[-1] = np.mean(np.sort(ious)[-3:])\n",
    "            \n",
    "        # Q&D -> R@k,tIOU\n",
    "        max_k = max(self.k)\n",
    "        topk_tp_fp_videos = tp_fp_videos[:max_k]\n",
    "        topk_pred_segment_indices = pred_segment_indices[:max_k]\n",
    "        i, j = np.meshgrid(true_segments, topk_pred_segment_indices)\n",
    "        iou = np.max(topk_tp_fp_videos[:, None] * self.iou_matrix[i, j],\n",
    "                     axis=1)\n",
    "        topk_tp_fp_labels = iou >= self.iou_threshold\n",
    "        cum_tp_fp_labels = topk_tp_fp_labels.cumsum()\n",
    "        for i, k in enumerate(self.k):\n",
    "            self.hit_k_iou[i].append(cum_tp_fp_labels[k - 1] > 0)\n",
    "            if self.hit_k[i][-1] != self.hit_k_iou[i][-1]:\n",
    "                print(i, k)\n",
    "                print(self.hit_k[i][-1], self.hit_k_iou[i][-1])\n",
    "                print(tp_fp_labels[:k])\n",
    "                print(topk_tp_fp_labels[:k])\n",
    "                print(cum_tp_fp_labels[:k])\n",
    "                print(cum_tp_fp_labels[k - 1] > 0)\n",
    "                raise\n",
    "    \n",
    "    def reset(self):\n",
    "        self.hit_k = [[] for i in self.k]\n",
    "        self.hit_k_iou = [[] for i in self.k]\n",
    "        self.miou = []\n",
    "        self.rank = []\n",
    "        \n",
    "    def _precompute_iou(self):\n",
    "        segments = self.corpus.segments * 5\n",
    "        segments[:, 1] += 5\n",
    "        self.iou_matrix = iou(segments, segments)\n",
    "    \n",
    "\n",
    "val_judge = RetrievalEvaluation(file_corpus, file_annotations, (1, 5, 10))\n",
    "file_queries = 'data/interim/mcn/features/queries_val.hdf5'\n",
    "\n",
    "with h5py.File(file_queries, 'r') as fid:\n",
    "    # sample_key = list(fid.keys())[0]\n",
    "    # sample_value = fid[sample_key][:]\n",
    "    # query_id = int(sample_key)\n",
    "    # val_judge.eval_single_vector(sample_value, query_id)\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for sample_key, h5ds in fid.items():\n",
    "        query_id = int(sample_key)\n",
    "        query_vector = h5ds[:]\n",
    "        val_judge.eval_single_vector(query_vector, query_id)\n",
    "    performace = val_judge.eval()\n",
    "    print('r@{0:}={2:};\\nr@{0:},{1:}={3:};\\nmRank={5:.2f}\\nmIOU={4:.4f};'\n",
    "          .format(val_judge.k, val_judge.iou_threshold,\n",
    "                  *performace))\n",
    "    print('Elapsed time:', time.time() - start)    \n",
    "# to run the evaluation again\n",
    "# val_judge.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
