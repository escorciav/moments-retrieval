{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Towards feature engineering\n",
    "\n",
    "## 1.1 Take a look at public metadata\n",
    "\n",
    "### 1.1.a Grab videos from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_annotations(subset='train'):\n",
    "    with open(f'../data/raw/{subset}_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def read_hash(filename='/home/escorciav/datasets/didemo/annotations/yfcc100m_hash.txt'):\n",
    "    \"Parse text-file with hash\"\n",
    "    lines = open(filename).readlines()\n",
    "    yfcc100m_hash = {}\n",
    "    for line_count, line in enumerate(lines):\n",
    "        line = line.strip().split('\\t')\n",
    "        yfcc100m_hash[line[0]] = line[1]\n",
    "    return yfcc100m_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_annotations('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there missing videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "video_dir = Path('/home/escorciav/datasets/didemo/videos')\n",
    "video_dir = Path('/home/escorciav/Downloads/didemo/missing_videos/')\n",
    "\n",
    "num_videos = 0\n",
    "for i in data:\n",
    "#     print(i.keys())\n",
    "#     print(i['video'])\n",
    "    video_file = video_dir / (i['video'] + '.mp4')\n",
    "    if video_file.is_file():\n",
    "        num_videos += 1\n",
    "    else:\n",
    "        video_file = \n",
    "        \n",
    "print(f'[{num_videos}/{len(data)}] {len(data)-num_videos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b Check additional videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9161595@N03_6019640821_67dcbb6797.mov\n",
      "29812295@N07_6093883918_0e9fb3d764.avi\n",
      "58773213@N00_3727553755_cbb12bf1ae.mov\n",
      "71701932@N00_3326901843_c126d74aac.mov\n",
      "85154009@N00_3183361585_1c1c1143aa.avi\n",
      "37996615073@N01_3336195519_579ea4136c.3gp\n",
      "44124421772@N01_2867159874_e39e716b7e.mpg\n",
      "7 7\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "extra_a = Path('/home/escorciav/Downloads/didemo/missing_videos/')\n",
    "extra_b = Path('/home/escorciav/Downloads/didemo/missing_videos_AWS/')\n",
    "repeated, indeed = 0, 0\n",
    "for i in extra_b.iterdir():\n",
    "    j = extra_a / i.name\n",
    "    if j.is_file():\n",
    "        repeated += 1\n",
    "        cmd = ['diff', i, j]\n",
    "        output = subprocess.run(cmd, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "        if output.returncode == 0:\n",
    "            indeed += 1\n",
    "print(f'Potential repeteated files: {repeated}. Confirmed {indeed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set with all videos of this dataset\n",
    "\n",
    "We will duplicate '.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number clips missing [0-0/0/0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folders = [\n",
    "    Path('/home/escorciav/datasets/didemo/missing/missing_videos/'),\n",
    "    Path('/home/escorciav/datasets/didemo/missing/missing_videos_AWS/'),\n",
    "    Path('/home/escorciav/datasets/didemo/videos/')\n",
    "]\n",
    "\n",
    "videos = set()\n",
    "for i in folders:\n",
    "    for j in i.iterdir():\n",
    "        videos.add(j.name)\n",
    "        if j.suffix == '.mp4':\n",
    "            videos.add(j.stem)\n",
    "\n",
    "num_missing = []\n",
    "for i in ['train', 'val', 'test']:\n",
    "    num_missing.append(0)\n",
    "    for j in read_annotations(i):\n",
    "        if j['video'] not in videos:\n",
    "            num_missing[-1] += 1\n",
    "            print(j['video'])\n",
    "print(f'Number clips missing [{sum(num_missing)}-'\n",
    "      f'{num_missing[0]}/{num_missing[1]}/{num_missing[2]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workaround for videos that where not in AWS or Lisas's website.\n",
    "\n",
    "I turned out that I only needed two videos, thus I didn't automate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "missing_videos = [\n",
    "    '33234611@N00_3567949618_0bee41c6f7.mp4',\n",
    "    '15389242@N00_4268126780_be074c803c.mov',\n",
    "    '15389242@N00_4268126780_be074c803c.mov',\n",
    "    '33234611@N00_3567949618_0bee41c6f7.mp4',\n",
    "    '15389242@N00_4268126780_be074c803c.mov',\n",
    "    '15389242@N00_4268126780_be074c803c.mov',\n",
    "    '15389242@N00_4268126780_be074c803c.mov',\n",
    "]\n",
    "missing_videos = set(missing_videos)\n",
    "\n",
    "def get_aws_link(h):\n",
    "    return f'https://multimedia-commons.s3-us-west-2.amazonaws.com/data/videos/mp4/{h[:3]}/{h[3:6]}/{h}.mp4'\n",
    "\n",
    "PATH = Path('/home/escorciav/datasets/didemo/missing/missing_trial/')\n",
    "VIDEO_DIR = Path('/home/escorciav/datasets/didemo/videos/')\n",
    "\n",
    "id2hash = read_hash()\n",
    "for video_name in missing_videos:\n",
    "    video_id = video_name.split('_')[1]\n",
    "    video_hash = id2hash[video_id]\n",
    "\n",
    "    filename_hash = PATH / (video_hash + '.mp4')\n",
    "    if filename_hash.is_file():\n",
    "        target = VIDEO_DIR / (video_name + '.mp4')\n",
    "        filename_hash.rename(target)\n",
    "    else:\n",
    "        print('Manual download, sorry :p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge missing videos into big folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number clips missing [48-48/0/0]\n",
      "Moving data from extra folders to main_folders\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "main_folder = [\n",
    "    Path('/home/escorciav/datasets/didemo/videos/')\n",
    "]\n",
    "\n",
    "extra_folders = [\n",
    "    Path('/home/escorciav/datasets/didemo/missing/missing_videos_AWS/'),\n",
    "    Path('/home/escorciav/datasets/didemo/missing/missing_videos/'),\n",
    "]\n",
    "\n",
    "videos = set()\n",
    "for i in main_folder:\n",
    "    for j in i.iterdir():\n",
    "        videos.add(j.name)\n",
    "        if j.suffix == '.mp4':\n",
    "            videos.add(j.stem)\n",
    "\n",
    "num_missing = []\n",
    "missing_set = set()\n",
    "for i in ['train', 'val', 'test']:\n",
    "    num_missing.append(0)\n",
    "    for j in read_annotations(i):\n",
    "        if j['video'] not in videos:\n",
    "            num_missing[-1] += 1\n",
    "            missing_set.add(j['video'])\n",
    "print(f'Number clips missing [{sum(num_missing)}-'\n",
    "      f'{num_missing[0]}/{num_missing[1]}/{num_missing[2]}]')\n",
    "\n",
    "print('Moving data from extra folders to main_folders')\n",
    "assert len(main_folder) == 1\n",
    "main_folder = main_folder[0]\n",
    "for i in missing_set:\n",
    "    for j in extra_folders:\n",
    "        full_i = (j / i)\n",
    "        if full_i.is_file():\n",
    "            target = main_folder / (i + '.mp4')\n",
    "            full_i.rename(target)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Getting metadata from videos\n",
    "\n",
    "### 1.2.a Export video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "videos_file = '../data/raw/videos.csv'\n",
    "video_lists = []\n",
    "with open(videos_file, 'w') as fw:\n",
    "    fw.write('video_id,subset\\n')\n",
    "    for subset in ['train', 'val', 'test']:\n",
    "        dataset_filename = '../data/raw/{}_data.json'.format(subset)\n",
    "\n",
    "        with open(dataset_filename) as fr:\n",
    "            dataset = json.load(fr)\n",
    "    \n",
    "        video_lists.append(set())\n",
    "        for i in dataset:\n",
    "            video_lists[-1].add(i['video'])\n",
    "            # sanity check videos to make sure videos are not mixed\n",
    "            if len(video_lists) > 1:\n",
    "                assert all([i['video'] not in j for j in video_lists[0:-1]])\n",
    "    \n",
    "        for i in video_lists[-1]:\n",
    "            fw.write(f'{i},{subset}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check videos and update CSV-file accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filename = '../data/raw/videos.csv'\n",
    "dirname = Path('/home/escorciav/datasets/didemo/videos/')\n",
    "df = pd.read_csv(filename)\n",
    "for i, row in df.iterrows():\n",
    "    filename = dirname / (row['video_id'] + '.mp4')\n",
    "    if not filename.exists():\n",
    "        assert False\n",
    "    df.loc[i, 'video_id'] = row['video_id'] + '.mp4'\n",
    "df.to_csv('../data/raw/cev.csv', index=False)\n",
    "# I had to do this because I was sleeping and somthing was working well in that monda\n",
    "!mv ../data/raw/cev.csv ../data/raw/videos.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.b Export metadata with video-utils\n",
    "\n",
    "```\n",
    "python tools/video_info.py \\\n",
    "  -i ~/projects/moments-retrieval/data/raw/videos.txt \\\n",
    "  -o ~/projects/moments-retrieval/data/raw/videos_info.csv \\\n",
    "  -r ~/datasets/didemo/videos/ \\\n",
    "  -n 48 --verbose 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.c Merge CSV files\n",
    "\n",
    "checking corrupted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num corrupted [3/10642]\n",
      "0         12235053@N05_5129768098_d725892466.mov.mp4\n",
      "10058    12090392@N02_13482799053_87ef417396.mov.mp4\n",
      "10641     14934133@N00_3736977321_6091381cef.avi.mp4\n",
      "Name: video_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/raw/videos_info.csv')\n",
    "corrupted = np.zeros(len(df), dtype=bool)\n",
    "for i in ['duration', 'frame_rate', 'num_frames', 'width', 'height']:\n",
    "    corrupted = np.logical_or(corrupted, pd.isna(df[i]))\n",
    "print(f'Num corrupted [{corrupted.sum()}/{len(corrupted)}]')\n",
    "print(df.loc[corrupted, 'video_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: merge CSV\n",
    "\n",
    "[check this](https://stackoverflow.com/questions/39862654/pandas-concat-of-multiple-data-frames-using-only-common-columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration [max, min, mean]: [428.962963, 0.0, 45.707006271266096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/raw/videos_info.csv')\n",
    "print(\n",
    "    'Duration [max, min, mean]: ['\n",
    "    f'{df.loc[:, \"duration\"].max()}, '\n",
    "    f'{df.loc[:, \"duration\"].min()}, '\n",
    "    f'{df.loc[:, \"duration\"].mean()}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1,0.001] = 716\n",
      "[0.001,5] = 0\n",
      "[5,10] = 0\n",
      "[10,15] = 2\n",
      "[15,20] = 1\n",
      "[20,25] = 1038\n",
      "[25,30] = 1416\n",
      "[30,35] = 1368\n",
      "[35,428.962963] = 6098\n"
     ]
    }
   ],
   "source": [
    "video_durations = df.loc[:, 'duration'].dropna()\n",
    "duration_edges = [-1, 0 + 1e-3] + list(range(5, 40, 5)) + [df.loc[:, \"duration\"].max()]\n",
    "videos_per_duration, _ = np.histogram(video_durations, bins=duration_edges)\n",
    "assert len(video_durations) == videos_per_duration.sum()\n",
    "for i, v in enumerate(videos_per_duration):\n",
    "    print(f'[{duration_edges[i]},{duration_edges[i+1]}] = {v}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
