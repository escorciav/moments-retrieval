{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Charades-STA\n",
    "\n",
    "TODO\n",
    "\n",
    "- Check duration stats\n",
    "- Generate a JSON\n",
    "    - list of dicts\n",
    "    - each dict represents as instance\n",
    "    - _Fields_: check code for `didemo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_charades_sta(filename):\n",
    "    \"\"\"Parser raw charades-STA annotations\n",
    "    \n",
    "    Args:\n",
    "        filename (str)\n",
    "    Returns:\n",
    "        instances (list of dicts)\n",
    "    TODO:\n",
    "        update dict by class\n",
    "        \n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    with open(filename, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()\n",
    "            video_info, description = line.split('##')\n",
    "            video_id, t_start, t_end = video_info.split()\n",
    "            t_start = float(t_start)\n",
    "            t_end = float(t_end)\n",
    "            \n",
    "            instances.append(\n",
    "                {'video': video_id,\n",
    "                 'times': [[t_start, t_end]],\n",
    "                 'description': description}\n",
    "            )\n",
    "            # print(video, t_start, t_end, description)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Moments duration analysis in the dataset\n",
    "\n",
    "Why? to extend SMCN and all its variants, we need to do the dirty work done by DiDeMo setup.\n",
    "\n",
    "> DiDeMo makes it easy by defining the search space up front.\n",
    "\n",
    "What? We need to set a couple of parameters: (i) _minimum_ moment length, (ii) _maximum_ moment length, (iii) _type of range_, how to explore minimum -> maximum, and (iv) _striding_ to reduce the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Q = 95\n",
    "QUANTILES = np.arange(25, 101, 2.5)\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "for i, subset in enumerate(['train', 'test']):\n",
    "    filename = f'../data/raw/charades/charades_sta_{subset}.txt'\n",
    "    data = parse_charades_sta(filename)\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data]\n",
    "    all_duration += duration\n",
    "    \n",
    "    duration = np.array(duration)\n",
    "    print('Negative durations: ', sum(duration < 0))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[i].set_xlabel('Duration')\n",
    "    axs[i].set_ylabel('Quantile')\n",
    "    axs[i].set_title(f'Duration stats {subset}\\n'\n",
    "                     f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                     f'Median: {np.median(duration):.2f}, '\n",
    "                     f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                     f'Max: {np.max(duration):.2f}')\n",
    "duration = np.array(all_duration)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1])\n",
    "axs[-1].set_xlabel('Duration')\n",
    "axs[-1].set_ylabel('Quantile')\n",
    "_ = axs[-1].set_title('Duration stats (train+test)\\n'\n",
    "                      f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                      f'Median: {np.median(duration):.2f}, '\n",
    "                      f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                      f'Max: {np.max(duration):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum length of a moment or chunk size.\n",
    "\n",
    "  The initial batch of experiments will be done with 3s as there are psicological references that support that number and it's close to the minimum.\n",
    "\n",
    "  _considerations that will go to the Appendix_\n",
    "  \n",
    "  We can pick the minimum or probably something around\n",
    "  \n",
    "  a. $\\text{min} ( \\text{duration} ) (1 + \\text{tIOU}_\\text{ref})$\n",
    "  \n",
    "  b. $\\text{min} ( \\text{duration} ) (1 + \\text{tIOU}_\\text{ref}) \\pm \\sigma_{\\text{annotators concensus}}$\n",
    "  \n",
    "     $\\sigma_{\\text{annotators concensus}}$ could be taken from [Sigurdsson et. al ICCV-2017](https://arxiv.org/abs/1708.02696)\n",
    "\n",
    "  > DiDeMo makes it easy by defining the minimum length of the segments to 5s with variance 0.\n",
    "  \n",
    "- Maximum length of moment.\n",
    "  \n",
    "  We ended up doing a hard cut-off at the duration covering 95%.\n",
    "  \n",
    "  _considerations that will go to the Appendix_\n",
    "  \n",
    "  Picking the maximum duration is more trickier that the minimum. Although, this could become a chicken or egg problem. Anyways, the practical thoughs are:\n",
    "  \n",
    "  a. $\\text{max} ( \\text{duration} ) (1 - \\text{tIOU}_\\text{ref})$\n",
    "  \n",
    "  b. $\\text{max} ( \\text{duration} ) (1 - \\text{tIOU}_\\text{ref}) \\pm \\sigma_{\\text{annotators concensus}}$\n",
    "  \n",
    "     $\\sigma_{\\text{annotators concensus}}$ could be taken from [Sigurdsson et. al ICCV-2017](https://arxiv.org/abs/1708.02696)\n",
    "     \n",
    "  However, those assume that your data is unimorly distributed which is not the case based on the above plots. The distribution is heavily skewed towards smaller durations.\n",
    "  \n",
    "  c. If you aren't interested on working on the long tail, a hard cut-off is OK. Why? the [paper](https://arxiv.org/pdf/1705.02101.pdf) didn't disclose the UI to collect annotation, nor mechanism to enforce consensus. Those is fine to consider them outliers or out of the scope of this work if you wanna be politically correct.\n",
    "  \n",
    "  d. We must tackle the long tail! OK, go to study multiple annotators and come-up with scalable methods for tackling the long tail. This is still an open problem and the people that has done [it](https://cs.stanford.edu/people/ranjaykrishna/densevid/) are unsure how to setup and scale the study. Once, you understand the nature designing the methods will be more interesting like throwing dards to the actual target rather than the wall.\n",
    "\n",
    "  > DiDeMo makes it easy by defining the maximum length of the segments to 30s and setting the length of the video also to 30s.\n",
    "\n",
    "- Linear or non-linear scale from minimum to maximum moment length.\n",
    "\n",
    "    For simplicity we will consider a linear scale.\n",
    "    \n",
    "    _considerations that will go to the Appendix_\n",
    "    \n",
    "    - We haven't still made our mind around this. Sounds like a nice assigment in a computer vision class. Thus, we won't put it in the appendix.\n",
    "    \n",
    "- Length of the stride.\n",
    "\n",
    "    Once the above are set, we study the amount of stride and the size of the search space in the next cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Recall analysis for a given search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from np_segments_ops import iou as segment_iou\n",
    "\n",
    "FPS = 5\n",
    "IOU_THRESHOLDS = [0.5, 0.7]\n",
    "\n",
    "def generate_windows(length, scale, linear=True):\n",
    "    \"create multi-scale (duration) right aligned windows\"\n",
    "    if not linear:\n",
    "        raise NotImplementedError('WIP')\n",
    "    windows = np.zeros((scale, 2))\n",
    "    windows[:, 1] += np.arange(1, scale + 1) * length\n",
    "    return windows\n",
    "\n",
    "def make_annotations_df(instances, file_h5):\n",
    "    \"Create data-frames to play easily with data\"\n",
    "    instances_df = pd.DataFrame([{**i, **{'t_start': i['times'][0][0],\n",
    "                                          't_end': i['times'][0][1]}}\n",
    "                                 for i in instances])\n",
    "    videos_in_charades_sta = {i for i in instances_df['video'].unique()}\n",
    "    instances_gbv = instances_df.groupby('video')\n",
    "    with h5py.File(file_h5, 'r') as f:\n",
    "        videos_info = []\n",
    "        for video_id, dataset in f.items():\n",
    "            if video_id not in videos_in_charades_sta:\n",
    "                continue\n",
    "            videos_info.append(\n",
    "                {'video': video_id,\n",
    "                 'num_frames': dataset.shape[0],\n",
    "                 'num_instances': instances_gbv.get_group(\n",
    "                     video_id).shape[0],\n",
    "                }\n",
    "            )\n",
    "    videos_df = pd.DataFrame(videos_info)\n",
    "    return videos_df, instances_df\n",
    "\n",
    "def sliding_window(length, scale, stride, t_end,\n",
    "                   t_start=0, linear=True):\n",
    "    \"Sliding windows for a given time interval\"\n",
    "    list_of_np_windows = []\n",
    "    canonical_windows = generate_windows(length, scale, linear)\n",
    "    for t in np.arange(t_start, t_end, stride):\n",
    "        window_t = canonical_windows * 1\n",
    "        # shift windows\n",
    "        window_t += t\n",
    "        list_of_np_windows.append(window_t)\n",
    "    windows = np.vstack(list_of_np_windows)\n",
    "    return windows\n",
    "\n",
    "def recall_bound_and_search_space(videos_df, instances_df,\n",
    "                                  stride, length, scale, linear=True,\n",
    "                                  iou_thresholds=IOU_THRESHOLDS, fps=FPS):\n",
    "    \"\"\"Compute recall and search-space for a given stride\n",
    "    \n",
    "    Args:\n",
    "        videos_df : DataFrame with video info, required `num_frames`.\n",
    "        instances_df : DataFrame with instance info, required `t_start`\n",
    "                       and `t_end'.\n",
    "    \n",
    "    Note: this takes ~5s\n",
    "    \"\"\"\n",
    "    num_videos = len(videos_df)\n",
    "    videos_gbv = videos_df.groupby('video')\n",
    "    instances_gbv = instances_df.groupby('video')\n",
    "    \n",
    "    recall_videos_ious = np.empty((num_videos, len(iou_thresholds)))\n",
    "    search_space_card_per_video = np.empty(num_videos)\n",
    "    for i, (video_id, gt_instances) in enumerate(instances_gbv):\n",
    "        # Get ground-truth segments\n",
    "        instances_start = gt_instances.loc[:, 't_start'].values[:, None]\n",
    "        instances_end = gt_instances.loc[:, 't_end'].values[:, None]\n",
    "        gt_segments = np.hstack([instances_start, instances_end])\n",
    "        \n",
    "        # Estimate video duration\n",
    "        num_frames = videos_gbv.get_group(\n",
    "            video_id)['num_frames'].values[0]\n",
    "        t_end = num_frames / FPS\n",
    "        \n",
    "        # Generate search space\n",
    "        windows = sliding_window(length, scale, stride, t_end,\n",
    "                                 linear=linear)\n",
    "        # TODO: clamp segments at the end?\n",
    "        search_space_card_per_video[i] = len(windows)\n",
    "        \n",
    "        # IOU between windows and gt_segments\n",
    "        iou_pred_vs_gt = segment_iou(windows, gt_segments)\n",
    "        # Computing upper-bound of recall, given that we don't have\n",
    "        # more info to do assigments\n",
    "        iou_per_gt_i = iou_pred_vs_gt.max(axis=0)\n",
    "        \n",
    "        # Compute recall for over multiple thresholds\n",
    "        num_pos_i = len(gt_segments)\n",
    "        for j, iou_thr in enumerate(iou_thresholds):\n",
    "            num_gt_retrieved = sum(iou_per_gt_i >= iou_thr)\n",
    "            recall_videos_ious[i, j] = num_gt_retrieved / num_pos_i\n",
    "        \n",
    "    recall_ious = recall_videos_ious.mean(axis=0)\n",
    "    search_space_median_std = np.array(\n",
    "        [np.median(search_space_card_per_video),\n",
    "         np.std(search_space_card_per_video)]\n",
    "    )\n",
    "    return recall_ious, search_space_median_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training the results look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to edit\n",
    "min_length = 3\n",
    "max_length = 80\n",
    "num_scales = 5\n",
    "max_stride = min(min_length * num_scales, max_length)\n",
    "strides = range(0, max_stride)\n",
    "strides = [2**i for i in range(0, max_stride) if 2**i < max_stride]\n",
    "strides = [1, 2, 3, 4, 5, 6, 8, 12]\n",
    "\n",
    "annotation_file = '../data/raw/charades/charades_sta_train.txt'\n",
    "features_file = '/home/escorciav/datasets/charades/features/resnet101-openimages_5fps_320x240.h5'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales)\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Avg recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the results look similar in testing a.k.a. there is _NOT_ distribution (training v.s. testing) miss-match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '../data/raw/charades/charades_sta_test.txt'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales)\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Avg recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous analysis setting a _stride_ of 3 or 4 sounds like a good point to kick-off experiments.\n",
    "\n",
    "# 2. Alternatives\n",
    "\n",
    "The previous analysis studies how to recover as many moments as possible only focus on their duration. This perspective assumes that the search space will consider many candidates. The candicates could come from a proposals method or a sliding window approach.\n",
    "\n",
    "Another perspective is to define a canonical video length. To scale this perspective to videos of multiple length, we could consider the following strategies:\n",
    "\n",
    "(i) sliding the model over time (convolution kinda).\n",
    "\n",
    "(ii) rescaling the temporal dimension (temporal pyramids) similar to how we re-scale images.\n",
    "\n",
    "  > sounds a bit like a variable chunk-size.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- define pertinence.\n",
    "\n",
    "- define methodology for (i); (ii) and/or (i-ii).\n",
    "\n",
    "- estimate workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dump data for dataset\n",
    "\n",
    "File to dump\n",
    "```json\n",
    "{\n",
    "     'moments': [moment_dict, ...],\n",
    "     'videos': {video_id: video_dict, ...},\n",
    "     'time_unit': 3\n",
    " }\n",
    "```\n",
    "\n",
    "video_dict\n",
    "```json\n",
    "{\n",
    "    'duration': float,\n",
    "    'num_clips': int\n",
    "}\n",
    "```\n",
    "duration := upper-bound of video duration. TODO: get real duration?\n",
    "\n",
    "num_clips := number of chunks with of time_unit length in the video\n",
    "\n",
    "moment_dict\n",
    "```json\n",
    "{\n",
    "    'description': str,\n",
    "    'time': [float, float],\n",
    "    'annotation_id': int,\n",
    "    'video': str,\n",
    "    'times': [[float, float]]\n",
    "}\n",
    "```\n",
    "\n",
    "__Note__: It requires to run 1st cell with `function:parse_charades_sta `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSETS = ['train', 'test']\n",
    "TIME_UNIT = 3\n",
    "MODE = 'x'\n",
    "FPS = 5\n",
    "H5_FILE = '/home/escorciav/datasets/charades/features/resnet152_max.h5'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == ['train', 'test']\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "def extend_metadata(list_of_moments, filename, offset=0):\n",
    "    \"\"\"Augment moments' metadata (in-place) and create video metadata\n",
    "    \n",
    "    Args:\n",
    "        list_of_moments (list of dicts) : the output of\n",
    "            function::parse_charades_sta.\n",
    "        filename (str) : path to HDF5 with all features of the dataset.\n",
    "        offset (int, optional) : ensure annotation-id accross moments is unique\n",
    "        \n",
    "    Adds `annotation_id` and `time` to each moment\n",
    "    \"\"\"\n",
    "    with h5py.File(filename) as fid:\n",
    "        videos = {}\n",
    "        for i, moment in enumerate(list_of_moments):\n",
    "            assert len(moment['times']) == 1\n",
    "            moment['time'] = moment['times'][0]\n",
    "            # we use the row index of the CSV ans unique identifier\n",
    "            # for the moment. Of course, 0-indexed.\n",
    "            moment['annotation_id'] = i + offset\n",
    "\n",
    "            video_id = moment['video']\n",
    "            if video_id not in videos:\n",
    "                num_clips = fid[video_id].shape[0]\n",
    "                duration =  num_clips * TIME_UNIT\n",
    "                videos[video_id] = {'duration': duration,\n",
    "                                    'num_clips': num_clips,\n",
    "                                    'num_moments': 0}\n",
    "            videos[video_id]['num_moments'] += 1\n",
    "    return videos    \n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    FILENAME = Path(f'../data/raw/charades/charades_sta_{subset}.txt')\n",
    "    OUTPUT_FILE = Path(f'../data/interim/charades-sta/{subset}.json')\n",
    "    \n",
    "    instances = parse_charades_sta(FILENAME)\n",
    "    videos = extend_metadata(instances, H5_FILE, offset=offset)\n",
    "    offset += len(instances)\n",
    "    \n",
    "    if not OUTPUT_FILE.parent.is_dir():\n",
    "        dirname = OUTPUT_FILE.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "    \n",
    "    with open(OUTPUT_FILE, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': instances,\n",
    "                   'time_unit': TIME_UNIT\n",
    "                  },\n",
    "                  fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline for single video retrieval\n",
    "\n",
    "| Model            | R@1,IoU=0.5 | R@1,IoU=0.7 | R@5,IoU=0.5 | R@5,IoU=0.7 |\n",
    "| :--------------- | ----------: | ----------: | ----------: | ----------: | \n",
    "| CTRL (aln)       |   17.69     |    5.91     |    55.54    |     23.79   |\n",
    "| CTRL (reg-p)     |   19.22     |    6.64     |    57.98    |     25.22   |\n",
    "| CTRL (reg-np)    |   21.42     |    7.15     |    59.11    |     26.91   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
