{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Charades-STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def parse_charades_sta(filename):\n",
    "    \"\"\"Parser raw charades-STA annotations\n",
    "    \n",
    "    Args:\n",
    "        filename (str)\n",
    "    Returns:\n",
    "        instances (list of dicts)\n",
    "    TODO:\n",
    "        update dict by class\n",
    "        \n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    with open(filename, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()\n",
    "            video_info, description = line.split('##')\n",
    "            video_id, t_start, t_end = video_info.split()\n",
    "            t_start = float(t_start)\n",
    "            t_end = float(t_end)\n",
    "            \n",
    "            instances.append(\n",
    "                {'video': video_id,\n",
    "                 'times': [[t_start, t_end]],\n",
    "                 'description': description}\n",
    "            )\n",
    "            # print(video, t_start, t_end, description)\n",
    "    return instances\n",
    "\n",
    "def make_annotations_df(instances, file_h5):\n",
    "    \"Create data-frames to play easily with data\"\n",
    "    instances_df = pd.DataFrame([{**i, **{'t_start': i['times'][0][0],\n",
    "                                          't_end': i['times'][0][1]}}\n",
    "                                 for i in instances])\n",
    "    videos_in_charades_sta = {i for i in instances_df['video'].unique()}\n",
    "    instances_gbv = instances_df.groupby('video')\n",
    "    with h5py.File(file_h5, 'r') as f:\n",
    "        videos_info = []\n",
    "        for video_id, dataset in f.items():\n",
    "            if video_id not in videos_in_charades_sta:\n",
    "                continue\n",
    "            videos_info.append(\n",
    "                {'video': video_id,\n",
    "                 'num_frames': dataset.shape[0],\n",
    "                 'num_instances': instances_gbv.get_group(\n",
    "                     video_id).shape[0],\n",
    "                }\n",
    "            )\n",
    "    videos_df = pd.DataFrame(videos_info)\n",
    "    return videos_df, instances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Moments duration analysis\n",
    "\n",
    "Why? to extend SMCN and all its variants, we need to do the dirty work done by DiDeMo setup.\n",
    "\n",
    "> DiDeMo makes it easy by defining the search space up front.\n",
    "\n",
    "What? We need to set a couple of parameters: (i) _minimum_ moment length, (ii) _maximum_ moment length, (iii) _type of range_, how to explore minimum -> maximum, and (iv) _striding_. Those parameters will define the search space, and will set the stage to define the size of the chunk/clip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Q = 95\n",
    "QUANTILES = np.arange(25, 101, 2.5)\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "for i, subset in enumerate(['train', 'test']):\n",
    "    filename = f'../data/raw/charades/charades_sta_{subset}.txt'\n",
    "    data = parse_charades_sta(filename)\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "#                 if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    \n",
    "    duration = np.array(duration)\n",
    "    print('Negative durations: ', sum(duration <= 0))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[i].set_xlabel('Duration')\n",
    "    axs[i].set_ylabel('Quantile')\n",
    "    axs[i].set_title(f'Duration stats {subset}\\n'\n",
    "                     f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                     f'Median: {np.median(duration):.2f}, '\n",
    "                     f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                     f'Max: {np.max(duration):.2f}')\n",
    "duration = np.array(all_duration)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1])\n",
    "axs[-1].set_xlabel('Duration')\n",
    "axs[-1].set_ylabel('Quantile')\n",
    "_ = axs[-1].set_title('Duration stats (train+test)\\n'\n",
    "                      f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                      f'Median: {np.median(duration):.2f}, '\n",
    "                      f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                      f'Max: {np.max(duration):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum length of a moment.\n",
    "\n",
    "  The initial batch of experiments will be done with 3s as it's close to the minimum and there are psicological references that support that number.\n",
    "\n",
    "  > DiDeMo makes it easy by defining the minimum length of the segments to 5s with variance 0.\n",
    "  \n",
    "- Maximum length of moment.\n",
    "  \n",
    "  We ended up taking 24s as it's close to the maximum moment duration in the testing set.\n",
    "  \n",
    "    > DiDeMo makes it easy by defining the maximum length of the segments to 30s and setting the length of the video also to 30s.\n",
    "\n",
    "- Explore range from minimum to maximum moment length.\n",
    "\n",
    "  TODO: describe\n",
    "    \n",
    "- Stride\n",
    "\n",
    "  TODO: describe\n",
    "  \n",
    "Based on the parameters mentioned above, we study multiple configurations of proposals in terms of its size and its recall upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from proposals import SlidingWindowMSRSS\n",
    "from nb_utils import parse_moments\n",
    "from nb_utils import recall_bound_and_search_space\n",
    "\n",
    "filename = '../data/processed/charades-sta/test-01.json'\n",
    "clip_length = 3\n",
    "proposals_prm = dict(\n",
    "    length=clip_length,\n",
    "    scales=list(range(2, 9, 1)),\n",
    "    stride=0.3\n",
    ")\n",
    "\n",
    "dataset = parse_moments(filename)\n",
    "proposals_fn = SlidingWindowMSRSS(**proposals_prm)\n",
    "train_results = recall_bound_and_search_space(\n",
    "    filename, proposals_fn)\n",
    "recall_ious, search_space, durations = train_results\n",
    "num_clips = np.ceil(durations / clip_length).sum()\n",
    "search_space[-1] /= num_clips\n",
    "print(recall_ious)\n",
    "print(search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update graphs\n",
    "\n",
    "In training the results look as follows like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to edit\n",
    "min_length = 3\n",
    "max_length = 80\n",
    "num_scales = 8\n",
    "strides = [1, 2, 3, 4, 5]\n",
    "\n",
    "annotation_file = '../data/raw/charades/charades_sta_train.txt'\n",
    "features_file = '/home/escorciav/datasets/charades/features/resnet101-openimages_5fps_320x240.h5'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales,\n",
    "        slidding_window_fn=sliding_window,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the results look similar in testing a.k.a. there is _NOT_ distribution (training v.s. testing) miss-match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '../data/raw/charades/charades_sta_test.txt'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales,\n",
    "        slidding_window_fn=sliding_window,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusions__\n",
    "\n",
    "TODO: update this cell\n",
    "\n",
    "- With a 1s stride, there is small gap due to _not_ regressing the exact boundaries for IOU=0.7.\n",
    "\n",
    "- However with such a low stride, the median size of the search space is an order of magnitud greater than DiDeMo. Note that the median duration of the videos is about the same as in DiDeMo (see section [1a.1](#1a.1-Video-duration)).\n",
    "\n",
    "- In case we wanna reduce the search space, good values to consider are:\n",
    "  - _min moment duration_: 3 seconds\n",
    "  - _max moment duration_: 24 seconds\n",
    "  - _scale from min to max_: linear, with unit slope, from min to max moment duration.\n",
    "  - _stride_: 3 seconds\n",
    "  - _clip/chunk size_: 3 seconds\n",
    "  \n",
    "- What would it happen if min/max moment duration or stride are not a multiple of clip/chunk size?\n",
    "\n",
    "  We need to do rounding or interpolation of features.\n",
    "  \n",
    "In case you are interested on a relationship between the size of the search space, $|\\mathcal{S}|$, in terms of the duration of the video $d$, you can use this formula:\n",
    "\n",
    "$|\\mathcal{S}| = \\sum_{w_l \\in \\mathcal{W}} (\\frac{d}{s} + 1) = |\\mathcal{W}| (\\frac{d}{s} + 1) $\n",
    "\n",
    "i.e. $|\\mathcal{S}| \\propto |\\mathcal{W}| \\frac{d}{s} $\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathcal{W}$ is the set of all possible durations. \n",
    "\n",
    "- $s$ stride of the window. We use the same stride for all the different $w_l$.\n",
    "\n",
    "_TLDR_ More details. Skip if you are in a rush time.\n",
    "\n",
    "The above formula comes from the generic:\n",
    "\n",
    "$|S| \\propto \\sum_{w_l \\in \\mathcal{W}} (\\frac{d - w_l + 2p}{s} + 1)  $\n",
    "\n",
    "where $2p$ is the amount padding. In our case, $2p = w_l$ because we kept windows ending at a time longer than $d$.\n",
    "\n",
    "We can get a better upper-bound by clamping ending time to $d$. However, that's tricky as some windows may have length which is not inside $\\mathcal{W}$.\n",
    "\n",
    "### Video duration\n",
    "\n",
    "The previous analysis only consider the duration of the moments. What about the video duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Q = 95\n",
    "FPS = 5\n",
    "QUANTILES = np.arange(25, 101, 2.5)\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "H5_FILE_FEAT_PER_FRAME = '/home/escorciav/datasets/charades/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "for i, subset in enumerate(['train', 'test']):\n",
    "    filename = f'../data/raw/charades/charades_sta_{subset}.txt'\n",
    "    data = parse_charades_sta(filename)\n",
    "    videos_df, _ = make_annotations_df(data, H5_FILE_FEAT_PER_FRAME)\n",
    "    duration = videos_df['num_frames'] / FPS\n",
    "    all_duration.append(duration)\n",
    "    \n",
    "    print('Negative durations: ', sum(duration <= 0))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[i].set_xlabel('Duration')\n",
    "    axs[i].set_ylabel('Quantile')\n",
    "    axs[i].set_title(f'Duration stats {subset}\\n'\n",
    "                     f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                     f'Median: {np.median(duration):.2f}, '\n",
    "                     f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                     f'Max: {np.max(duration):.2f}')\n",
    "duration = pd.concat(all_duration, axis=0, ignore_index=True)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1])\n",
    "axs[-1].set_xlabel('Duration')\n",
    "axs[-1].set_ylabel('Quantile')\n",
    "_ = axs[-1].set_title('Duration stats (train+test)\\n'\n",
    "                      f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                      f'Median: {np.median(duration):.2f}, '\n",
    "                      f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                      f'Max: {np.max(duration):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Conclusion_: comparing the two windows arragement [1a](#1a.-Search-space-with-sliding-windows) vs [1b.1](#1b.1-\"DiDeMofying\"-untrimmed-videos), we can conclude:\n",
    "\n",
    "- For 1s stride, 1b generates more windows than 1a.\n",
    "\n",
    "  In case we are open to explore the search space in a different way, it seems [1a](#1a.-Search-space-with-sliding-windows) reduces the amount of moments to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dump data for training and evaluation\n",
    "\n",
    "### 2.a.1 JSON files\n",
    "\n",
    "File to dump\n",
    "```json\n",
    "{\n",
    "     'moments': [moment_dict, ...],\n",
    "     'videos': {video_id: video_dict, ...},\n",
    "     'date': str,\n",
    "     'responsible': str,\n",
    " }\n",
    "```\n",
    "\n",
    "video_dict\n",
    "```json\n",
    "{\n",
    "    'duration': float,\n",
    "}\n",
    "```\n",
    "duration := approximate video durations\n",
    "\n",
    "moment_dict\n",
    "```json\n",
    "{\n",
    "    'description': str,\n",
    "    'annotation_id': int,\n",
    "    'video': str,\n",
    "    'time': [float, float],\n",
    "    'times': [[float, float]]\n",
    "}\n",
    "```\n",
    "\n",
    "description := description provided by the annotators\n",
    "\n",
    "times := list with all segments associated with a given description. Why a list? it is inherited from DiDeMo where you have multiple segments for a given description.\n",
    "\n",
    "time := first item in `times`. Given that moments in Charades-STA only have a single segment, it's easier to create a new attribute with it. We keep both to not break our dashboards.\n",
    "\n",
    "video := unique video-id to refer to the video. Make sure that this match the HDF5 with the features.\n",
    "\n",
    "__Note__: It requires to run 1st cell with `function:parse_charades_sta ` and `function:make_annotations_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSETS = ['train', 'test']\n",
    "MODE = 'w'\n",
    "FPS = 5\n",
    "CREATOR = 'EscorciaSSGR'\n",
    "H5_FILE = '/home/escorciav/datasets/charades/features/resnet152_max.h5'\n",
    "H5_FILE_FEAT_PER_FRAME = '/home/escorciav/datasets/charades/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == ['train', 'test']\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "def extend_metadata(list_of_moments, videos_gbv, filename, offset=0, fps=FPS):\n",
    "    \"\"\"Augment moments' (in-place) metadata and create video metadata\n",
    "    \n",
    "    Args:\n",
    "        list_of_moments (list of dicts) : the output of\n",
    "            function::parse_charades_sta.\n",
    "        videos_gbv (DataFrame groupedby) : DataFrame grouped by `video_id`. It\n",
    "            is mandatory that the DataFrame has a column `num_frames` such that\n",
    "            we can estimate the duration of the video.\n",
    "        filename (str) : path to HDF5 with all features of the dataset.\n",
    "        offset (int, optional) : ensure annotation-id accross moments is unique\n",
    "        \n",
    "    Adds `annotation_id` and `time` to each moment\n",
    "    \"\"\"\n",
    "    with h5py.File(filename) as fid:\n",
    "        videos = {}\n",
    "        keep = []\n",
    "        for i, moment in enumerate(list_of_moments):\n",
    "            assert len(moment['times']) == 1\n",
    "            video_id = moment['video']\n",
    "            # Get estimated video duration\n",
    "            num_frames = videos_gbv.get_group(\n",
    "                video_id)['num_frames'].values[0]\n",
    "            video_duration = num_frames / fps\n",
    "            \n",
    "            # TODO: sanitize by trimming moments up to video duration <= 0\n",
    "            # Sanitize\n",
    "            # i) clamp moments inside video\n",
    "            moment['times'][0][0] = min(moment['times'][0][0], video_duration)\n",
    "            moment['times'][0][1] = min(moment['times'][0][1], video_duration)\n",
    "            # ii) remove moments with duration <= 0\n",
    "            if moment['times'][0][1] <= moment['times'][0][0]:\n",
    "                continue\n",
    "            \n",
    "            keep.append(i)\n",
    "            moment['time'] = moment['times'][0]\n",
    "            # we use the row index of the original CSV as unique identifier\n",
    "            # for the moment. Of course, 0-indexed.\n",
    "            moment['annotation_id'] = i + offset\n",
    "\n",
    "            # Update dict with video info\n",
    "            if video_id not in videos:\n",
    "                num_clips = fid[video_id].shape[0]\n",
    "                videos[video_id] = {'duration': video_duration,\n",
    "                                    'num_clips': num_clips,\n",
    "                                    'num_moments': 0}\n",
    "            videos[video_id]['num_moments'] += 1\n",
    "    \n",
    "    clean_list_of_moments = []\n",
    "    for i in keep:\n",
    "        clean_list_of_moments.append(list_of_moments[i])\n",
    "    return videos, clean_list_of_moments \n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    FILENAME = Path(f'../data/raw/charades/charades_sta_{subset}.txt')\n",
    "    OUTPUT_FILE = Path(f'../data/interim/charades-sta/{subset}.json')\n",
    "    \n",
    "    instances = parse_charades_sta(FILENAME)\n",
    "    videos_df, _ = make_annotations_df(instances, H5_FILE_FEAT_PER_FRAME)\n",
    "    videos_gbv = videos_df.groupby('video')\n",
    "    videos, cleaned_instances = extend_metadata(\n",
    "        instances, videos_gbv, H5_FILE, offset=offset)\n",
    "    offset += len(instances)\n",
    "    \n",
    "    if not OUTPUT_FILE.parent.is_dir():\n",
    "        dirname = OUTPUT_FILE.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "    \n",
    "    with open(OUTPUT_FILE, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': cleaned_instances,\n",
    "                   'date': datetime.now().isoformat(),\n",
    "                   'git_hash': get_git_revision_hash(),\n",
    "                   'responsible': CREATOR,\n",
    "                  },\n",
    "                  fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a.2 Untied JSON and HDF5 inputs\n",
    "\n",
    "TLDR; reference: minor-detail. Safe to skip unless you have problems loading data for dispatching training.\n",
    "\n",
    "At some point, there was a undesired tied btw the JSON and HDF5 files (inputs) required by our implementation. \n",
    "\n",
    "- root `time_unit`. This is a property of the features, as such it should reside in the HDF5 a not in the JSON.\n",
    "\n",
    "- `videos/ith-video/num_clips`. This is a property of the ith-video, as such we should grab it from the HDF5 instead of placed it in the JSON.\n",
    "\n",
    "The following script was use to update the `*.json` files with metadata for training and evaluation.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "subsets = ['train-01', 'test-01', 'train-02_01', 'val-02_01']\n",
    "\n",
    "for subset in subsets:\n",
    "    filename = f'../data/processed/charades-sta/{subset}.json'\n",
    "    with open(filename, 'r') as fr:\n",
    "        data = json.load(fr)\n",
    "    del data['time_unit']\n",
    "    for video_id in data['videos']:\n",
    "        del data['videos'][video_id]['num_clips']\n",
    "    data['date'] = datetime.now().isoformat()\n",
    "    data['git_hash'] = get_git_revision_hash()\n",
    "    with open(filename, 'w') as fw:\n",
    "        json.dump(data, fw)\n",
    "```\n",
    "\n",
    "We also update the HDF5 such that it contains `metadata` [Group/Folder](http://docs.h5py.org/en/latest/high/group.html).\n",
    "\n",
    "```bash\n",
    "!h5ls /home/escorciav/datasets/charades-sta/features/resnet152_max_cs-5.h5 | grep metadata\n",
    "```\n",
    "\n",
    "In case the following line doesn't return anything, it means that you are using an old version of the data.\n",
    "If you know the `FPS`, `CLIP_LENGTH` and `POOL`ing operation used to get those features, the following snippet will add the metadata required for the most recent version of our code.\n",
    "\n",
    "```python\n",
    "FPS = 5\n",
    "CLIP_LENGTH = 3  # seconds\n",
    "POOL = 'max'  # pooling operation over time\n",
    "# verbose\n",
    "COMMENTS = (f'ResNet152 trained on Imagenet-ILSVRC12, Pytorch model. '\n",
    "            f'Extracted at {FPS} FPS with an image resolution of 320x240, '\n",
    "            f'and {POOL} pooled over time every {CLIP_LENGTH} seconds.')\n",
    "CREATOR = 'EscorciaSSGR'  # please add your name here to sign the file i.e. assign yourself as resposible\n",
    "filename = f'/home/escorciav/datasets/charades/features/resnet152_rgb_{POOL}_cl-{CLIP_LENGTH}.h5'\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "\n",
    "assert CLIP_LENGTH * FPS >= 1\n",
    "with h5py.File(filename, 'a') as fw:\n",
    "    grp = fw.create_group('metadata')\n",
    "    grp.create_dataset('time_unit', data=CLIP_LENGTH)\n",
    "    grp.create_dataset('date', data=datetime.now().isoformat(),\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "    grp.create_dataset('responsible', data=CREATOR,\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "    grp.create_dataset('comments', data=COMMENTS,\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Chunked features\n",
    "\n",
    "Go to notebook `4-feature-extraction.ipynb` section `#Varied-length-videos` (remove the # if you use your browser string matching).\n",
    "\n",
    "_TODO_ add procedure here to avoid jumping over the place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Train-val split\n",
    "\n",
    "_TODO_\n",
    "\n",
    "Motivation: create two disjoint partitions of the train set.\n",
    "\n",
    "1. get action categories for each video in Charades-STA from the [annotations](http://ai2-website.s3.amazonaws.com/data/Charades.zip)([source](https://allenai.org/plato/charades/)).\n",
    "\n",
    "2. The partition must be randomly generated, ideally with a [pseudo-random number generator](https://docs.python.org/3.6/library/random.html#random.seed), and by videos. That means that a given video $v_i$ cannot appear in both subsets.\n",
    "\n",
    "  Requirements:\n",
    "  \n",
    "  - In one of the subsets the percentage of examples for a given action category  must be ~$p$% of the total number of videos associated with that category.\n",
    "\n",
    "  - (try) make that all the categories in the $p$% subset have the same number of samples.\n",
    "  \n",
    "  Outcome:\n",
    "  \n",
    "  - Bar plot with the number of Charades-STA videos associate with each action category on both subsets.\n",
    "  \n",
    "  - Add the median number of videos per category in the title of the plots.\n",
    "\n",
    "3. Dump splits with the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "trial = '01'\n",
    "\n",
    "with open('data/processed/charades-sta/train.json', 'r') as fid:\n",
    "    data_train = json.load(fid)\n",
    "    id2ind = {\n",
    "        v['annotation_id']: k\n",
    "        for k, v in enumerate(data_train['moments'])\n",
    "    }\n",
    "\n",
    "for subset, subset_ in [\n",
    "    ('train', 'training'),\n",
    "    ('val', 'validation')\n",
    "    ]:\n",
    "    filename = f'data/interim/charades-sta/{subset_}_set_split_75-25_threshold_8.json'\n",
    "    with open(filename, 'r') as fid:\n",
    "        indices = json.load(fid)\n",
    "    data_subset = deepcopy(data_train)\n",
    "    moments = []\n",
    "    for i in indices:\n",
    "        if i in id2ind:\n",
    "            moment_i = data_train['moments'][id2ind[i]]\n",
    "            moments.append(moment_i)\n",
    "    data_subset['moments'] = moments\n",
    "    data_subset['videos'] = {}\n",
    "    for i in moments:\n",
    "        video_i = i['video']\n",
    "        if video_i not in data_subset['videos']:\n",
    "            data_subset['videos'][video_i] = data_train['videos'][video_i]\n",
    "    \n",
    "    with open(f'data/processed/charades-sta/{subset}-{trial}.json', 'x') as fid:\n",
    "        json.dump(data_subset, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Update pooled flow features\n",
    "\n",
    "We update the pooled flow features such that they span the same duration of our RGB features. In this way, we simplify the late fusion of different modalities.\n",
    "\n",
    "Code used to pad flow features according to RGB length.\n",
    "\n",
    "```python\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "CLIP_LENGTH = 3\n",
    "file_flow = f'inceptionbn-imagenet-ucf101.1_max_cl-{CLIP_LENGTH}.h5'\n",
    "file_rgb = f'resnet152_rgb_max_cl-{CLIP_LENGTH}.h5'\n",
    "subsets = ['train', 'test']\n",
    "weirds, edited = [], []\n",
    "counter, toremove, toclamp = 0, 0, 0\n",
    "\n",
    "videos = {}\n",
    "moments = []\n",
    "for subset in subsets:\n",
    "    with open(f'data/processed/charades-sta/{subset}-01.json', 'r') as fr:\n",
    "        data = json.load(fr)\n",
    "    videos.update(data['videos'])\n",
    "    moments += data['moments']\n",
    "for i, moment_i in enumerate(moments):\n",
    "    video_i = moment_i['video']\n",
    "    if 'indices' not in videos[video_i]:\n",
    "        videos[video_i]['indices'] = set()\n",
    "    videos[video_i]['indices'].add(i)\n",
    "\n",
    "fid = h5py.File(file_flow, 'r')\n",
    "fid.close()\n",
    "with h5py.File(file_flow, 'a') as fr_flow, h5py.File(file_rgb, 'r') as fr_rgb:\n",
    "    for video_id, metadata in videos.items():\n",
    "        assert video_id in fr_flow\n",
    "        assert video_id in fr_rgb\n",
    "        num_clips = fr_flow[video_id].shape[0]\n",
    "\n",
    "        if abs((metadata['duration'] // CLIP_LENGTH) - num_clips) > 1:\n",
    "            weirds.append(video_id)\n",
    "            print(subset, video_id,\n",
    "                  'Num clips: ', num_clips,\n",
    "                  'Expected clips: ', metadata['duration'] // CLIP_LENGTH,\n",
    "                  'Duration (s): ', metadata['duration'])\n",
    "\n",
    "        duration = metadata['duration']\n",
    "        estimated_duration = CLIP_LENGTH * num_clips\n",
    "        if estimated_duration < duration:\n",
    "            counter += 1\n",
    "        for ind in metadata['indices']:\n",
    "            times = np.array(moments[ind]['times'])\n",
    "            toremove += (times[:, 0] >= estimated_duration).sum()\n",
    "            toclamp += (times[:, 1] >= estimated_duration).sum()\n",
    "            \n",
    "        num_clips_rgb = fr_rgb[video_id].shape[0]\n",
    "        if num_clips != num_clips_rgb:\n",
    "            assert num_clips < num_clips_rgb\n",
    "            features = fr_flow[video_id][:]\n",
    "            width = num_clips_rgb - num_clips\n",
    "            edited.append((video_id, width))\n",
    "            padded_features = np.pad(features, ((0, width), (0, 0)), 'edge')\n",
    "            assert padded_features.shape[0] == num_clips_rgb\n",
    "            del fr_flow[video_id]\n",
    "            fr_flow.create_dataset(video_id, data=padded_features, chunks=True)\n",
    "print('duration-flow < duration-rgb: ', counter, f'{len(videos)}')\n",
    "print('Num moments with t_sart >= duration:', toremove)\n",
    "print('Num moments with t_end >= duration:', toclamp)\n",
    "\n",
    "with open(f'padded_inceptionbn-imagenet-ucf101.1_max_cl-{CLIP_LENGTH}.txt', 'w') as fid:\n",
    "    for video_id in edited:\n",
    "        fid.write('{},{}\\n'.format(*video_id))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines single video retrieval\n",
    "\n",
    "### 3.1 CTRL\n",
    "\n",
    "| Model            | R@1,IoU=0.5 | R@1,IoU=0.7 | R@5,IoU=0.5 | R@5,IoU=0.7 |\n",
    "| :--------------- | ----------: | ----------: | ----------: | ----------: | \n",
    "| CTRL (aln)       |   17.69     |    5.91     |    55.54    |     23.79   |\n",
    "| CTRL (reg-p)     |   19.22     |    6.64     |    57.98    |     25.22   |\n",
    "| CTRL (reg-np)    |   21.42     |    7.15     |    59.11    |     26.91   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Moment Frequency Prior\n",
    "\n",
    "Results in a train-val split form train set for our search space (sliding windows between length 3s (seconds) and max length 24s with steps of 3s, stride 3s) and with `NMS = 0.6`. Please don't fool yourself and update the baseline according to your search strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = 'r@1,0.5', 'r@5,0.5', 'r@1,0.7', 'r@5,0.7'\n",
    "bins = [5, 10, 15, 30, 20, 50, 100, 75, 1000, 500]\n",
    "results = [\n",
    "    [0.0678, 0.5051, 0.0245, 0.2522],\n",
    "    [0.0682, 0.5191, 0.0248, 0.2978],\n",
    "    [0.1729, 0.5815, 0.0841, 0.3755],\n",
    "    [0.1758, 0.5879, 0.0834, 0.3834], \n",
    "    [0.1019, 0.5971, 0.0449, 0.3920],\n",
    "    [0.1825, 0.6019, 0.0904, 0.4013],\n",
    "    [0.2051, 0.5939, 0.1057, 0.3758],\n",
    "    [0.1825, 0.6032, 0.0933, 0.3831],\n",
    "    [0.1866, 0.6061, 0.0946, 0.3739],\n",
    "    [0.2111, 0.6048, 0.1025, 0.3857]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose 75 bins as it's a good compromise for all the four metrics. The rationale is similar to the [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "For a given number of bins, we proceed to compute the prior using the entire training set, and evaluating of the entire testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
