{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Charades-STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "def parse_charades_sta(filename):\n",
    "    \"\"\"Parser raw charades-STA annotations\n",
    "    \n",
    "    Args:\n",
    "        filename (str)\n",
    "    Returns:\n",
    "        instances (list of dicts)\n",
    "    TODO:\n",
    "        update dict by class\n",
    "        \n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    with open(filename, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()\n",
    "            video_info, description = line.split('##')\n",
    "            video_id, t_start, t_end = video_info.split()\n",
    "            t_start = float(t_start)\n",
    "            t_end = float(t_end)\n",
    "            \n",
    "            instances.append(\n",
    "                {'video': video_id,\n",
    "                 'times': [[t_start, t_end]],\n",
    "                 'description': description}\n",
    "            )\n",
    "            # print(video, t_start, t_end, description)\n",
    "    return instances\n",
    "\n",
    "def make_annotations_df(instances, file_h5):\n",
    "    \"Create data-frames to play easily with data\"\n",
    "    instances_df = pd.DataFrame([{**i, **{'t_start': i['times'][0][0],\n",
    "                                          't_end': i['times'][0][1]}}\n",
    "                                 for i in instances])\n",
    "    videos_in_charades_sta = {i for i in instances_df['video'].unique()}\n",
    "    instances_gbv = instances_df.groupby('video')\n",
    "    with h5py.File(file_h5, 'r') as f:\n",
    "        videos_info = []\n",
    "        for video_id, dataset in f.items():\n",
    "            if video_id not in videos_in_charades_sta:\n",
    "                continue\n",
    "            videos_info.append(\n",
    "                {'video': video_id,\n",
    "                 'num_frames': dataset.shape[0],\n",
    "                 'num_instances': instances_gbv.get_group(\n",
    "                     video_id).shape[0],\n",
    "                }\n",
    "            )\n",
    "    videos_df = pd.DataFrame(videos_info)\n",
    "    return videos_df, instances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Moments duration analysis\n",
    "\n",
    "Why? to extend SMCN and all its variants, we need to do the dirty work done by DiDeMo setup.\n",
    "\n",
    "> DiDeMo makes it easy by defining the search space up front.\n",
    "\n",
    "What? We need to set a couple of parameters: (i) _minimum_ moment length, (ii) _maximum_ moment length, (iii) _type of range_, how to explore minimum -> maximum, and (iv) _striding_. Those parameters will define the search space, and will set the stage to define the size of the chunk/clip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Q = 95\n",
    "QUANTILES = np.arange(25, 101, 2.5)\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "for i, subset in enumerate(['train', 'test']):\n",
    "    filename = f'../data/raw/charades/charades_sta_{subset}.txt'\n",
    "    data = parse_charades_sta(filename)\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "#                 if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    \n",
    "    duration = np.array(duration)\n",
    "    print('Negative durations: ', sum(duration <= 0))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[i].set_xlabel('Duration')\n",
    "    axs[i].set_ylabel('Quantile')\n",
    "    axs[i].set_title(f'Duration stats {subset}\\n'\n",
    "                     f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                     f'Median: {np.median(duration):.2f}, '\n",
    "                     f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                     f'Max: {np.max(duration):.2f}')\n",
    "duration = np.array(all_duration)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1])\n",
    "axs[-1].set_xlabel('Duration')\n",
    "axs[-1].set_ylabel('Quantile')\n",
    "_ = axs[-1].set_title('Duration stats (train+test)\\n'\n",
    "                      f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                      f'Median: {np.median(duration):.2f}, '\n",
    "                      f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                      f'Max: {np.max(duration):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum length of a moment.\n",
    "\n",
    "  The initial batch of experiments will be done with 3s as it's close to the minimum and there are psicological references that support that number.\n",
    "\n",
    "  > DiDeMo makes it easy by defining the minimum length of the segments to 5s with variance 0.\n",
    "  \n",
    "- Maximum length of moment.\n",
    "  \n",
    "  We ended up taking 24s as it's close to the maximum moment duration in the testing set.\n",
    "  \n",
    "    > DiDeMo makes it easy by defining the maximum length of the segments to 30s and setting the length of the video also to 30s.\n",
    "\n",
    "- Explore range from minimum to maximum moment length.\n",
    "\n",
    "  For simplicity we will consider a linear scale with unit slope.\n",
    "    \n",
    "- Amount of stride.\n",
    "\n",
    "  The above parameters define $\\mathcal{W}$, the set of all possible window durations. When do we place any given $w \\in \\mathcal{W}$? A potential approach is to place them uniformly every $s$ seconds. The distance between two possible durations would be the stride.\n",
    "\n",
    "Based on the parameters mentioned above, we will study the size of the search space and its recall upper bound.\n",
    "\n",
    "There are multiple ways to generate the search space, here we consider two:\n",
    "\n",
    "## 1a. Search space with sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from np_segments_ops import iou as segment_iou\n",
    "\n",
    "FPS = 5\n",
    "IOU_THRESHOLDS = [0.5, 0.7]\n",
    "\n",
    "def generate_windows(length, scale, linear=True):\n",
    "    \"create multi-scale (duration) right aligned windows\"\n",
    "    if not linear:\n",
    "        raise NotImplementedError('WIP')\n",
    "    windows = np.zeros((scale, 2))\n",
    "    windows[:, 1] += np.arange(1, scale + 1) * length\n",
    "    return windows\n",
    "\n",
    "def sliding_window(length, scale, stride, t_end,\n",
    "                   t_start=0, linear=True):\n",
    "    \"Sliding windows for a given time interval\"\n",
    "    list_of_np_windows = []\n",
    "    canonical_windows = generate_windows(length, scale, linear)\n",
    "    for t in np.arange(t_start, t_end, stride):\n",
    "        window_t = canonical_windows * 1\n",
    "        # shift windows\n",
    "        window_t += t\n",
    "        list_of_np_windows.append(window_t)\n",
    "    windows = np.vstack(list_of_np_windows)\n",
    "    # only keep valid windows inside video\n",
    "    # this way is clean but change numbers\n",
    "    # windows = windows[windows[:, 1] <= t_end, :]\n",
    "    \n",
    "    # on the other hand, this way is hacky and gives R@0.5=1\n",
    "    # hacky := we end up with windows length != alpha * length\n",
    "    # with alpha in Z+\n",
    "    # windows[windows[:, 1] > t_end, 1] = t_end\n",
    "    return windows\n",
    "\n",
    "def recall_bound_and_search_space(videos_df, instances_df,\n",
    "                                  stride, length, scale, linear=True,\n",
    "                                  slidding_window_fn=sliding_window,\n",
    "                                  iou_thresholds=IOU_THRESHOLDS, fps=FPS):\n",
    "    \"\"\"Compute recall and search-space for a given stride\n",
    "    \n",
    "    Args:\n",
    "        videos_df : DataFrame with video info, required `num_frames`.\n",
    "        instances_df : DataFrame with instance info, required `t_start`\n",
    "                       and `t_end'.\n",
    "    \n",
    "    Note: this takes ~5s\n",
    "    \"\"\"\n",
    "    num_videos = len(videos_df)\n",
    "    videos_gbv = videos_df.groupby('video')\n",
    "    instances_gbv = instances_df.groupby('video')\n",
    "    \n",
    "    matched_gt_per_iou = [[] for i in range(len(iou_thresholds))]\n",
    "    search_space_card_per_video = np.empty(num_videos)\n",
    "    for i, (video_id, gt_instances) in enumerate(instances_gbv):\n",
    "        # Get ground-truth segments\n",
    "        instances_start = gt_instances.loc[:, 't_start'].values[:, None]\n",
    "        instances_end = gt_instances.loc[:, 't_end'].values[:, None]\n",
    "        gt_segments = np.hstack([instances_start, instances_end])\n",
    "        \n",
    "        # Estimate video duration\n",
    "        num_frames = videos_gbv.get_group(\n",
    "            video_id)['num_frames'].values[0]\n",
    "        t_end = num_frames / FPS\n",
    "        \n",
    "        # sanitize\n",
    "        # i) clamp moments inside video\n",
    "        gt_segments[gt_segments[:, 0] > t_end, 0] = t_end\n",
    "        gt_segments[gt_segments[:, 1] > t_end, 1] = t_end\n",
    "        # ii) remove moments with duration <= 0\n",
    "        duration = gt_segments[:, 1] - gt_segments[:, 0]\n",
    "        ind = duration > 0\n",
    "        if ind.sum() == 0:\n",
    "            continue\n",
    "        gt_segments = gt_segments[ind, :]\n",
    "        \n",
    "        # Generate search space\n",
    "        windows = slidding_window_fn(length, scale, stride, t_end,\n",
    "                                     linear=linear)\n",
    "        search_space_card_per_video[i] = len(windows)\n",
    "        \n",
    "        # IOU between windows and gt_segments\n",
    "        iou_pred_vs_gt = segment_iou(windows, gt_segments)\n",
    "        # Computing upper-bound of recall, given that we don't have\n",
    "        # more info to do assignments\n",
    "        iou_per_gt_i = iou_pred_vs_gt.max(axis=0)\n",
    "        \n",
    "        # Compute matched_gt_per_iou for over multiple thresholds\n",
    "        for j, iou_thr in enumerate(iou_thresholds):\n",
    "            matched_gt_per_iou[j].append(iou_per_gt_i >= iou_thr)\n",
    "\n",
    "    recall_ious = np.empty(len(iou_thresholds))\n",
    "    for i, list_of_arrays in enumerate(matched_gt_per_iou):\n",
    "        matched_gt_i = np.concatenate(list_of_arrays)\n",
    "        recall_ious[i] = matched_gt_i.sum() / len(matched_gt_i)\n",
    "    search_space_median_std = np.array(\n",
    "        [np.median(search_space_card_per_video),\n",
    "         np.std(search_space_card_per_video)]\n",
    "    )\n",
    "    \n",
    "    return recall_ious, search_space_median_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training the results look as follows like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to edit\n",
    "min_length = 3\n",
    "max_length = 80\n",
    "num_scales = 8\n",
    "strides = [1, 2, 3, 4, 5]\n",
    "\n",
    "annotation_file = '../data/raw/charades/charades_sta_train.txt'\n",
    "features_file = '/home/escorciav/datasets/charades/features/resnet101-openimages_5fps_320x240.h5'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales,\n",
    "        slidding_window_fn=sliding_window,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the results look similar in testing a.k.a. there is _NOT_ distribution (training v.s. testing) miss-match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = '../data/raw/charades/charades_sta_test.txt'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales,\n",
    "        slidding_window_fn=sliding_window,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusions__\n",
    "\n",
    "- With a 1s stride, there is small gap due to _not_ regressing the exact boundaries for IOU=0.7.\n",
    "\n",
    "- However with such a low stride, the median size of the search space is an order of magnitud greater than DiDeMo. Note that the median duration of the videos is about the same as in DiDeMo (see section [1a.1](#1a.1-Video-duration)).\n",
    "\n",
    "- In case we wanna reduce the search space, good values to consider are:\n",
    "  - _min moment duration_: 3 seconds\n",
    "  - _max moment duration_: 24 seconds\n",
    "  - _scale from min to max_: linear, with unit slope, from min to max moment duration.\n",
    "  - _stride_: 3 seconds\n",
    "  - _clip/chunk size_: 3 seconds\n",
    "  \n",
    "- What would it happen if min/max moment duration or stride are not a multiple of clip/chunk size?\n",
    "\n",
    "  We need to do rounding or interpolation of features.\n",
    "  \n",
    "In case you are interested on a relationship between the size of the search space, $|\\mathcal{S}|$, in terms of the duration of the video $d$, you can use this formula:\n",
    "\n",
    "$|\\mathcal{S}| = \\sum_{w_l \\in \\mathcal{W}} (\\frac{d}{s} + 1) = |\\mathcal{W}| (\\frac{d}{s} + 1) $\n",
    "\n",
    "i.e. $|\\mathcal{S}| \\propto |\\mathcal{W}| \\frac{d}{s} $\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathcal{W}$ is the set of all possible durations. \n",
    "\n",
    "- $s$ stride of the window. We use the same stride for all the different $w_l$.\n",
    "\n",
    "_TLDR_ More details. Skip if you are in a rush time.\n",
    "\n",
    "The above formula comes from the generic:\n",
    "\n",
    "$|S| \\propto \\sum_{w_l \\in \\mathcal{W}} (\\frac{d - w_l + 2p}{s} + 1)  $\n",
    "\n",
    "where $2p$ is the amount padding. In our case, $2p = w_l$ because we kept windows ending at a time longer than $d$.\n",
    "\n",
    "We can get a better upper-bound by clamping ending time to $d$. However, that's tricky as some windows may have length which is not inside $\\mathcal{W}$.\n",
    "\n",
    "### Video duration\n",
    "\n",
    "The previous analysis only consider the duration of the moments. What about the video duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "Q = 95\n",
    "FPS = 5\n",
    "QUANTILES = np.arange(25, 101, 2.5)\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "H5_FILE_FEAT_PER_FRAME = '/home/escorciav/datasets/charades/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "for i, subset in enumerate(['train', 'test']):\n",
    "    filename = f'../data/raw/charades/charades_sta_{subset}.txt'\n",
    "    data = parse_charades_sta(filename)\n",
    "    videos_df, _ = make_annotations_df(data, H5_FILE_FEAT_PER_FRAME)\n",
    "    duration = videos_df['num_frames'] / FPS\n",
    "    all_duration.append(duration)\n",
    "    \n",
    "    print('Negative durations: ', sum(duration <= 0))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i])\n",
    "    axs[i].set_xlabel('Duration')\n",
    "    axs[i].set_ylabel('Quantile')\n",
    "    axs[i].set_title(f'Duration stats {subset}\\n'\n",
    "                     f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                     f'Median: {np.median(duration):.2f}, '\n",
    "                     f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                     f'Max: {np.max(duration):.2f}')\n",
    "duration = pd.concat(all_duration, axis=0, ignore_index=True)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1])\n",
    "axs[-1].set_xlabel('Duration')\n",
    "axs[-1].set_ylabel('Quantile')\n",
    "_ = axs[-1].set_title('Duration stats (train+test)\\n'\n",
    "                      f'Min: {np.min(duration[duration > 0]):.2f}, '\n",
    "                      f'Median: {np.median(duration):.2f}, '\n",
    "                      f'{Q}Q: {percentiles[QUANTILES == Q][0]:.2f} '\n",
    "                      f'Max: {np.max(duration):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Alternatives\n",
    "\n",
    "__SKIP THIS__\n",
    "\n",
    "_TLDR_ Skip this in the interest of time. The thoughts below came to our mind when we were implementing the idea above.\n",
    "\n",
    "\n",
    "## 1b.1 Search space with DiDeMo style segments\n",
    "\n",
    "In this section, we consider the setup of generating segments as in DiDeMo. This setup only change the arrange of the segments over a window of longer duration ($w_{\\text{max}}$). In particular, the configuration would look similar to this:\n",
    "\n",
    "TODO: missing figure\n",
    "\n",
    "Given that videos may be longer, or shorter, than $w_{\\text{max}}$, we must stride this arragement to cover the entire video.\n",
    "\n",
    "In this case, the search space is computed as follow:\n",
    "\n",
    "$|S| \\propto \\frac{d}{w_{\\text{max}}} (\\frac{w_{\\text{max}}}{c} + \\sum_{i=1}^{w_{\\text{max}}/w_{\\text{min}}} (\\frac{w_{\\text{max}} - i \\; w_{\\text{min}}}{c} + 1)) $\n",
    "\n",
    "- $w_{\\text{max}}$ is the maximum moment duration.\n",
    "\n",
    "- $w_{\\text{max}}$ is the maximum moment duration.\n",
    "\n",
    "- $c$ is the length of the clip or chunk.\n",
    "\n",
    "- Note that this assume as stride equal to $w_{\\text{max}}$, which is reflected in the denominator of the first fraction.\n",
    "\n",
    "__Note__: the formula may break if $w_{\\text{max}}$ is not multiple of $w_{\\text{min}}$ and $w_{\\text{min}}$ is not divisible by $c$.\n",
    "\n",
    "For fair comparison with [1a](), we also clamped the segments longer than the video duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate windows with DiDeMo arragement\n",
    "\n",
    "def didemo_arragement(w_min, w_max, c=None):\n",
    "    \"canonical DiDeMo-style arrangement of windows\"\n",
    "    if c is None:\n",
    "        c = w_min\n",
    "    assert c == w_min\n",
    "    list_of_np_windows = []\n",
    "    for w in np.arange(w_min, w_max + 1e-6, c):\n",
    "        t_start = np.arange(0, w_max - w + 1e-6, c)\n",
    "        windows_w = np.empty((len(t_start), 2))\n",
    "        windows_w[:, 0] = t_start\n",
    "        windows_w[:, 1] = t_start + w\n",
    "        list_of_np_windows.append(windows_w)\n",
    "    windows = np.vstack(list_of_np_windows)\n",
    "    return windows\n",
    "\n",
    "def sliding_window3(w_min, w_max, stride, t_end,\n",
    "                    t_start=0, linear=True):\n",
    "    \"Sliding windows for a given time interval\"\n",
    "    list_of_np_windows = []\n",
    "    canonical_windows = didemo_arragement(w_min, w_max)\n",
    "    for t in np.arange(t_start, t_end, stride):\n",
    "        window_t = canonical_windows * 1\n",
    "        # shift windows\n",
    "        window_t += t\n",
    "        list_of_np_windows.append(window_t)\n",
    "    windows = np.vstack(list_of_np_windows)\n",
    "    # TBD\n",
    "    # windows = windows[windows[:, 1] <= t_end, :]\n",
    "    \n",
    "    # TBD\n",
    "    # windows[windows[:, 1] > t_end, 1] = t_end\n",
    "    # Extra because these arragement does not guarantee\n",
    "    # that previous clamp will give correct segments\n",
    "    # windows = windows[windows[:, 1] > windows[:, 0], :]\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running analysis only in testing set because we already know that we can reach Recall of 1 there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to edit\n",
    "min_length = 3\n",
    "max_length = 24\n",
    "strides = [1, 3, 12, 24]\n",
    "\n",
    "annotation_file = '../data/raw/charades/charades_sta_test.txt'\n",
    "features_file = '/home/escorciav/datasets/charades/features/resnet101-openimages_5fps_320x240.h5'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "font_size = 12\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = parse_charades_sta(annotation_file)\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    # this was a hack to evaluate didemo-style arragement of windows\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=max_length,\n",
    "        slidding_window_fn=sliding_window3,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r')\n",
    "for tick in (ax1.xaxis.get_major_ticks() + \n",
    "             ax1.yaxis.get_major_ticks() +\n",
    "             ax2.yaxis.get_major_ticks()):\n",
    "    tick.label.set_fontsize(font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Conclusion_: comparing the two windows arragement [1a](#1a.-Search-space-with-sliding-windows) vs [1b.1](#1b.1-\"DiDeMofying\"-untrimmed-videos), we can conclude:\n",
    "\n",
    "- For 1s stride, 1b generates more windows than 1a.\n",
    "\n",
    "  In case we are open to explore the search space in a different way, it seems [1a](#1a.-Search-space-with-sliding-windows) reduces the amount of moments to explore.\n",
    "\n",
    "### 1b.2 Other consideration\n",
    "\n",
    "Thoughts regarding the selection of the parameters mentioned at the begining of section 1. \n",
    "\n",
    "- Minimum length of moment.\n",
    "\n",
    "  We can pick the minimum or probably something around\n",
    "  \n",
    "  a. $\\text{min} ( \\text{duration} ) (1 + \\text{tIOU}_\\text{ref})$\n",
    "  \n",
    "  b. $\\text{min} ( \\text{duration} ) (1 + \\text{tIOU}_\\text{ref}) \\pm \\sigma_{\\text{annotators concensus}}$\n",
    "  \n",
    "     $\\sigma_{\\text{annotators concensus}}$ could be taken from [Sigurdsson et. al ICCV-2017](https://arxiv.org/abs/1708.02696)\n",
    "\n",
    "- Maximum length of moment.\n",
    "\n",
    "  Picking the maximum duration is trickier that the minimum. Indeed, this could become a chicken or egg problem. Anyways, the practical thoughs are:\n",
    "  \n",
    "  a. $\\text{max} ( \\text{duration} ) (1 - \\text{tIOU}_\\text{ref})$\n",
    "  \n",
    "  b. $\\text{max} ( \\text{duration} ) (1 - \\text{tIOU}_\\text{ref}) \\pm \\sigma_{\\text{annotators concensus}}$\n",
    "  \n",
    "     $\\sigma_{\\text{annotators concensus}}$ could be taken from [Sigurdsson et. al ICCV-2017](https://arxiv.org/abs/1708.02696)\n",
    "     \n",
    "  Those assume that your data is unimorly distributed which is not the case based on the above plots. The distribution is skewed towards smaller durations.\n",
    "  \n",
    "  c. If you aren't interested on working on the long tail, a hard cut-off is OK. Why? the [paper](https://arxiv.org/pdf/1705.02101.pdf) didn't disclose the UI to collect annotation, nor mechanism to enforce consensus. thus, it is fine to consider them outliers or out of the scope of this work if you wanna be politically correct.\n",
    "  \n",
    "  d. We must tackle the long tail! OK, the first step is to study multiple annotators, later we can come-up with scalable methods for tackling the long tail. This is still an open problem, people that has done [it](https://cs.stanford.edu/people/ranjaykrishna/densevid/) are unsure how to setup and scale the study. Once, we understand the nature of the long tail, designing the methods will be more interesting. It's like throwing dards to the actual target rather than the wall.\n",
    "\n",
    "- Explore range from minimum to maximum moment length.\n",
    "\n",
    "  - We haven't still made our mind around this. Sounds like a nice assigment in a computer vision class. Thus, we won't put it in the appendix.\n",
    "\n",
    "### 1b.3 Other thoughts\n",
    "\n",
    "_Note_ this was written before implementing section 1b.\n",
    "\n",
    "The previous analysis studies how to recover as many moments as possible only focus on their duration. This perspective assumes that the search space will consider many candidates. The candicates could come from a proposals method or a sliding window approach.\n",
    "\n",
    "Another perspective is to define a canonical video length. To scale this perspective to videos of multiple length, we could consider the following strategies:\n",
    "\n",
    "(i) sliding the model over time (convolution kinda).\n",
    "\n",
    "(ii) rescaling the temporal dimension (temporal pyramids) similar to how we re-scale images.\n",
    "\n",
    "  > sounds a bit like a variable clip/chunk-size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dump data for training and evaluation\n",
    "\n",
    "### 2a. JSON files\n",
    "\n",
    "File to dump\n",
    "```json\n",
    "{\n",
    "     'moments': [moment_dict, ...],\n",
    "     'videos': {video_id: video_dict, ...},\n",
    "     'time_unit': 3,\n",
    "     'date': str,\n",
    "     'responsible': str,\n",
    " }\n",
    "```\n",
    "\n",
    "time_unit := size of chunk/clip in seconds.\n",
    "\n",
    "video_dict\n",
    "```json\n",
    "{\n",
    "    'duration': float,\n",
    "    'num_clips': int\n",
    "}\n",
    "```\n",
    "duration := approximate video durations\n",
    "\n",
    "num_clips := number of chunks with of time_unit length in the video\n",
    "\n",
    "moment_dict\n",
    "```json\n",
    "{\n",
    "    'description': str,\n",
    "    'annotation_id': int,\n",
    "    'video': str,\n",
    "    'time': [float, float],\n",
    "    'times': [[float, float]]\n",
    "}\n",
    "```\n",
    "\n",
    "description := description provided by the annotators\n",
    "\n",
    "times := list with all segments associated with a given description. Why a list? it is inherited from DiDeMo where you have multiple segments for a given description.\n",
    "\n",
    "time := first item in `times`. Given that moments in Charades-STA only have a single segment, it's easier to create a new attribute with it. We keep both to not break our dashboards.\n",
    "\n",
    "video := unique video-id to refer to the video. Make sure that this match the HDF5 with the features.\n",
    "\n",
    "__Note__: It requires to run 1st cell with `function:parse_charades_sta ` and `function:make_annotations_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSETS = ['train', 'test']\n",
    "TIME_UNIT = 3\n",
    "MODE = 'w'\n",
    "FPS = 5\n",
    "CREATOR = 'EscorciaSSGR'\n",
    "H5_FILE = '/home/escorciav/datasets/charades/features/resnet152_max.h5'\n",
    "H5_FILE_FEAT_PER_FRAME = '/home/escorciav/datasets/charades/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == ['train', 'test']\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "def extend_metadata(list_of_moments, videos_gbv, filename, offset=0, fps=FPS):\n",
    "    \"\"\"Augment moments' (in-place) metadata and create video metadata\n",
    "    \n",
    "    Args:\n",
    "        list_of_moments (list of dicts) : the output of\n",
    "            function::parse_charades_sta.\n",
    "        videos_gbv (DataFrame groupedby) : DataFrame grouped by `video_id`. It\n",
    "            is mandatory that the DataFrame has a column `num_frames` such that\n",
    "            we can estimate the duration of the video.\n",
    "        filename (str) : path to HDF5 with all features of the dataset.\n",
    "        offset (int, optional) : ensure annotation-id accross moments is unique\n",
    "        \n",
    "    Adds `annotation_id` and `time` to each moment\n",
    "    \"\"\"\n",
    "    with h5py.File(filename) as fid:\n",
    "        videos = {}\n",
    "        keep = []\n",
    "        for i, moment in enumerate(list_of_moments):\n",
    "            assert len(moment['times']) == 1\n",
    "            video_id = moment['video']\n",
    "            # Get estimated video duration\n",
    "            num_frames = videos_gbv.get_group(\n",
    "                video_id)['num_frames'].values[0]\n",
    "            video_duration = num_frames / fps\n",
    "            \n",
    "            # TODO: sanitize by trimming moments up to video duration <= 0\n",
    "            # Sanitize\n",
    "            # i) clamp moments inside video\n",
    "            moment['times'][0][0] = min(moment['times'][0][0], video_duration)\n",
    "            moment['times'][0][1] = min(moment['times'][0][1], video_duration)\n",
    "            # ii) remove moments with duration <= 0\n",
    "            if moment['times'][0][1] <= moment['times'][0][0]:\n",
    "                continue\n",
    "            \n",
    "            keep.append(i)\n",
    "            moment['time'] = moment['times'][0]\n",
    "            # we use the row index of the original CSV as unique identifier\n",
    "            # for the moment. Of course, 0-indexed.\n",
    "            moment['annotation_id'] = i + offset\n",
    "\n",
    "            # Update dict with video info\n",
    "            if video_id not in videos:\n",
    "                num_clips = fid[video_id].shape[0]\n",
    "                videos[video_id] = {'duration': video_duration,\n",
    "                                    'num_clips': num_clips,\n",
    "                                    'num_moments': 0}\n",
    "            videos[video_id]['num_moments'] += 1\n",
    "    \n",
    "    clean_list_of_moments = []\n",
    "    for i in keep:\n",
    "        clean_list_of_moments.append(list_of_moments[i])\n",
    "    return videos, clean_list_of_moments \n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    FILENAME = Path(f'../data/raw/charades/charades_sta_{subset}.txt')\n",
    "    OUTPUT_FILE = Path(f'../data/interim/charades-sta/{subset}.json')\n",
    "    \n",
    "    instances = parse_charades_sta(FILENAME)\n",
    "    videos_df, _ = make_annotations_df(instances, H5_FILE_FEAT_PER_FRAME)\n",
    "    videos_gbv = videos_df.groupby('video')\n",
    "    videos, cleaned_instances = extend_metadata(\n",
    "        instances, videos_gbv, H5_FILE, offset=offset)\n",
    "    offset += len(instances)\n",
    "    \n",
    "    if not OUTPUT_FILE.parent.is_dir():\n",
    "        dirname = OUTPUT_FILE.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "    \n",
    "    with open(OUTPUT_FILE, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': cleaned_instances,\n",
    "                   'time_unit': TIME_UNIT,\n",
    "                   'date': datetime.now().isoformat(),\n",
    "                   'git_hash': get_git_revision_hash(),\n",
    "                   'responsible': CREATOR,\n",
    "                  },\n",
    "                  fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Chunked features\n",
    "\n",
    "Go to notebook `4-feature-extraction.ipynb` section `#Varied-length-videos` (remove the # if you use your browser string matching).\n",
    "\n",
    "_TODO_ add procedure here to avoid jumping over the place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Train-val split\n",
    "\n",
    "_TODO_\n",
    "\n",
    "Motivation: create two disjoint partitions of the train set.\n",
    "\n",
    "1. get action categories for each video in Charades-STA from the [annotations](http://ai2-website.s3.amazonaws.com/data/Charades.zip)([source](https://allenai.org/plato/charades/)).\n",
    "\n",
    "2. The partition must be randomly generated, ideally with a [pseudo-random number generator](https://docs.python.org/3.6/library/random.html#random.seed), and by videos. That means that a given video $v_i$ cannot appear in both subsets.\n",
    "\n",
    "  Requirements:\n",
    "  \n",
    "  - In one of the subsets the percentage of examples for a given action category  must be ~$p$% of the total number of videos associated with that category.\n",
    "\n",
    "  - (try) make that all the categories in the $p$% subset have the same number of samples.\n",
    "  \n",
    "  Outcome:\n",
    "  \n",
    "  - Bar plot with the number of Charades-STA videos associate with each action category on both subsets.\n",
    "  \n",
    "  - Add the median number of videos per category in the title of the plots.\n",
    "\n",
    "3. Dump splits with the format described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "trial = '01'\n",
    "\n",
    "with open('data/processed/charades-sta/train.json', 'r') as fid:\n",
    "    data_train = json.load(fid)\n",
    "    id2ind = {\n",
    "        v['annotation_id']: k\n",
    "        for k, v in enumerate(data_train['moments'])\n",
    "    }\n",
    "\n",
    "for subset, subset_ in [\n",
    "    ('train', 'training'),\n",
    "    ('val', 'validation')\n",
    "    ]:\n",
    "    filename = f'data/interim/charades-sta/{subset_}_set_split_75-25_threshold_8.json'\n",
    "    with open(filename, 'r') as fid:\n",
    "        indices = json.load(fid)\n",
    "    data_subset = deepcopy(data_train)\n",
    "    moments = []\n",
    "    for i in indices:\n",
    "        if i in id2ind:\n",
    "            moment_i = data_train['moments'][id2ind[i]]\n",
    "            moments.append(moment_i)\n",
    "    data_subset['moments'] = moments\n",
    "    data_subset['videos'] = {}\n",
    "    for i in moments:\n",
    "        video_i = i['video']\n",
    "        if video_i not in data_subset['videos']:\n",
    "            data_subset['videos'][video_i] = data_train['videos'][video_i]\n",
    "    \n",
    "    with open(f'data/processed/charades-sta/{subset}-{trial}.json', 'x') as fid:\n",
    "        json.dump(data_subset, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines single video retrieval\n",
    "\n",
    "### 3.1 CTRL\n",
    "\n",
    "| Model            | R@1,IoU=0.5 | R@1,IoU=0.7 | R@5,IoU=0.5 | R@5,IoU=0.7 |\n",
    "| :--------------- | ----------: | ----------: | ----------: | ----------: | \n",
    "| CTRL (aln)       |   17.69     |    5.91     |    55.54    |     23.79   |\n",
    "| CTRL (reg-p)     |   19.22     |    6.64     |    57.98    |     25.22   |\n",
    "| CTRL (reg-np)    |   21.42     |    7.15     |    59.11    |     26.91   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Moment Frequency Prior\n",
    "\n",
    "Results in a train-val split form train set for our search space (sliding windows between length 3s (seconds) and max length 24s with steps of 3s, stride 3s) and with `NMS = 0.6`. Please don't fool yourself and update the baseline according to your search strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = 'r@1,0.5', 'r@5,0.5', 'r@1,0.7', 'r@5,0.7'\n",
    "bins = [5, 10, 15, 30, 20, 50, 100, 75, 1000, 500]\n",
    "results = [\n",
    "    [0.0678, 0.5051, 0.0245, 0.2522],\n",
    "    [0.0682, 0.5191, 0.0248, 0.2978],\n",
    "    [0.1729, 0.5815, 0.0841, 0.3755],\n",
    "    [0.1758, 0.5879, 0.0834, 0.3834], \n",
    "    [0.1019, 0.5971, 0.0449, 0.3920],\n",
    "    [0.1825, 0.6019, 0.0904, 0.4013],\n",
    "    [0.2051, 0.5939, 0.1057, 0.3758],\n",
    "    [0.1825, 0.6032, 0.0933, 0.3831],\n",
    "    [0.1866, 0.6061, 0.0946, 0.3739],\n",
    "    [0.2111, 0.6048, 0.1025, 0.3857]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose 75 bins as it's a good compromise for all the four metrics. The rationale is similar to the [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "For a given number of bins, we proceed to compute the prior using the entire training set, and evaluating of the entire testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
