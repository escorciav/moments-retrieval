{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring ActivityNet-Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nb_utils import make_annotations_df\n",
    "from nb_utils import recall_bound_and_search_space\n",
    "from nb_utils import sliding_window\n",
    "\n",
    "def parse_activitynet_captions(filename):\n",
    "    \"\"\"Parser raw ActivityNet Captions annotations\n",
    "    Args:\n",
    "        filename (str)\n",
    "    Returns:\n",
    "        instances (list of dicts)\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    with open(filename) as f:\n",
    "        dataset = json.load(f)\n",
    "        for video_id in dataset:\n",
    "            time_and_descriptions = zip(\n",
    "                dataset[video_id][\"timestamps\"],\n",
    "                dataset[video_id][\"sentences\"])\n",
    "            for interval, description in time_and_descriptions:\n",
    "                instances.append(\n",
    "                    {'video': video_id,\n",
    "                     'times': [interval],\n",
    "                     'description': description}\n",
    "                )\n",
    "                #print(video_id, interval, description)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Moments duration analysis\n",
    "\n",
    "Similar to notebook 11.\n",
    "\n",
    "We need to set a couple of parameters:\n",
    "\n",
    "(i) _minimum_ moment length\n",
    "\n",
    "(ii) _maximum_ moment length\n",
    "\n",
    "(iii) _type of range_, how to explore minimum -> maximum\n",
    "\n",
    "(iv) _stride_.\n",
    "\n",
    "_Note:_ following [Xu et. al arxiv-2018](https://arxiv.org/pdf/1804.05113.pdf), we fuse the two annotations in the validation set.\n",
    "\n",
    "The first step is to get an indea of the duration of the moments in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 95\n",
    "QUANTILES = np.arange(25, 101, 5)\n",
    "\n",
    "# plot stuff\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "fontsize = 14\n",
    "lw = 3 # linewidth\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "                # ignore negative duration\n",
    "                if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    duration = np.array(duration)\n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    print('Negative durations in {}: {}'.format(subset, sum(duration <= 0)))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i], lw=lw)\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i], lw=lw)\n",
    "    axs[i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[i].set_ylabel('Percentile', fontsize=fontsize)\n",
    "    axs[i].tick_params(labelsize=fontsize)\n",
    "    axs[i].set_title('Duration stats {}\\nMin: {:.2f}, Median: {:.2f}, {}Q: {:.2f} Max: {:.2f}'\n",
    "                     .format(subset, np.min(duration[duration > 0]), np.median(duration), Q,\n",
    "                             percentiles[QUANTILES == Q][0], np.max(duration)),\n",
    "                     fontsize=fontsize)\n",
    "\n",
    "duration = np.array(all_duration)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1], lw=lw)\n",
    "axs[-1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[-1].set_ylabel('Quantile', fontsize=fontsize)\n",
    "axs[-1].tick_params(labelsize=fontsize)\n",
    "_ = axs[-1].set_title('Duration stats (train+val1+val2)\\nMin: {:.2f}, Median: {:.2f}, {}Q: {:.2f} Max: {:.2f}'\n",
    "                      .format(np.min(duration[duration > 0]), np.median(duration), Q,\n",
    "                              percentiles[QUANTILES == Q][0], np.max(duration)),\n",
    "                     fontsize=fontsize)\n",
    "#fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_percentile-moment-duration.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the distribution is quite particular, we decided to analyze the PDF and CDF closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_step = 5 # seconds\n",
    "duration_edges = np.arange(0, 400 + duration_step - 0.1, duration_step)\n",
    "\n",
    "# plot stuff\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "fontsize = 14\n",
    "rwidth = 0.75\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(2, 3, figsize=(21, 14), sharex=True)\n",
    "cdf_val, edges_val = None, None\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "                # ignore negative duration\n",
    "                if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    duration = np.array(duration)\n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    print('Negative durations in {}: {}'.format(subset, sum(duration <= 0)))\n",
    "    axs[0, i].hist(duration, duration_edges, color=COLOR[i], density=True,\n",
    "                   rwidth=rwidth)\n",
    "    cdf, edges, *_ = axs[1, i].hist(duration, duration_edges, color=COLOR[i], density=True,\n",
    "                                    cumulative=True, rwidth=rwidth)\n",
    "    axs[1, i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[0, i].tick_params(labelsize=fontsize)\n",
    "    axs[1, i].tick_params(labelsize=fontsize)\n",
    "    axs[0, i].set_title(subset, fontsize=fontsize)\n",
    "\n",
    "duration = np.array(all_duration)\n",
    "axs[0, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, rwidth=rwidth)\n",
    "axs[1, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, cumulative=True, rwidth=rwidth)\n",
    "axs[0, 0].set_ylabel('Norm frequency', fontsize=fontsize)\n",
    "axs[1, 0].set_ylabel('Cum frequency', fontsize=fontsize)\n",
    "axs[1, -1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[0, -1].set_title('train+val', fontsize=fontsize)\n",
    "axs[0, -1].tick_params(labelsize=fontsize)\n",
    "axs[1, -1].tick_params(labelsize=fontsize)\n",
    "fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_cdf-pdf_moments-duration.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video duration\n",
    "\n",
    "Let's take a look at the duration of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H5_FILE_FEAT_PER_FRAME = '/home/escorciav/datasets/activitynet/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "FPS = 5\n",
    "\n",
    "duration_edges = np.arange(0, 211, 30)\n",
    "\n",
    "# plot stuff\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "fontsize = 14\n",
    "rwidth = 0.75\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(2, 3, figsize=(21, 14), sharex=True)\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "    \n",
    "    videos_df, _ = make_annotations_df(data, H5_FILE_FEAT_PER_FRAME)\n",
    "    duration = videos_df['num_frames'].values / FPS\n",
    "    all_duration.append(duration)\n",
    "    \n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    axs[0, i].hist(duration, duration_edges, color=COLOR[i], density=True, rwidth=rwidth)\n",
    "    axs[1, i].hist(duration, duration_edges, color=COLOR[i], density=True, cumulative=True, rwidth=rwidth)\n",
    "    axs[1, i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[0, i].tick_params(labelsize=fontsize)\n",
    "    axs[1, i].tick_params(labelsize=fontsize)\n",
    "    axs[0, i].set_title(subset, fontsize=fontsize)\n",
    "\n",
    "    \n",
    "duration = np.concatenate(all_duration)\n",
    "axs[0, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, rwidth=rwidth)\n",
    "axs[1, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, cumulative=True, rwidth=rwidth)\n",
    "axs[0, 0].set_ylabel('Norm frequency', fontsize=fontsize)\n",
    "axs[1, 0].set_ylabel('Cum frequency', fontsize=fontsize)\n",
    "axs[1, -1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[0, -1].set_title('train_val', fontsize=fontsize)\n",
    "axs[0, -1].tick_params(labelsize=fontsize)\n",
    "axs[1, -1].tick_params(labelsize=fontsize)\n",
    "# fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_cdf-pdf_video-duration_uniform-bins-30s.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Search space with sliding windows\n",
    "\n",
    "Durations choose between 10-120s, strides multiples of 10s and one sublinear strides such as 5s\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from proposals import SlidingWindowMSRSS\n",
    "from nb_utils import parse_moments\n",
    "from nb_utils import recall_bound_and_search_space\n",
    "\n",
    "filename = '../data/processed/activitynet-captions/val.json'\n",
    "clip_length = 5\n",
    "proposals_prm = dict(\n",
    "    length=clip_length,\n",
    "    scales=list(range(2, 27, 2)),\n",
    "    stride=0.3\n",
    ")\n",
    "\n",
    "dataset = parse_moments(filename)\n",
    "proposals_fn = SlidingWindowMSRSS(**proposals_prm)\n",
    "train_results = recall_bound_and_search_space(\n",
    "    filename, proposals_fn)\n",
    "recall_ious, search_space, durations = train_results\n",
    "num_clips = np.ceil(durations / clip_length).sum()\n",
    "search_space[-1] /= num_clips\n",
    "print(recall_ious)\n",
    "print(search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dump data for training and evaluation\n",
    "\n",
    "### 2a. Chunked features\n",
    "\n",
    "Go to notebook `4-feature-extraction.ipynb` section `#Varied-length-videos` (remove the # if you use your browser string matching).\n",
    "\n",
    "_TODO_ add procedure here to avoid jumping over the place.\n",
    "\n",
    "### 2b. JSON files\n",
    "\n",
    "The same as in notebook `11-charades-sta.ipynb` section 2a.\n",
    "\n",
    "Following [this paper](https://arxiv.org/abs/1804.05113), we merge the validations set into a single validation set.\n",
    "\n",
    "__Note__: It requires to run 1st cell with function `parse_activitynet_captions`.\n",
    "\n",
    "_minor details_\n",
    "\n",
    "To avoid spaghetti code, we copy the function `extend_metadata` into the module `nb_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SUBSETS = [['train'], ['val_1', 'val_2']]\n",
    "TIME_UNIT = 5\n",
    "MODE = 'x'\n",
    "FPS = 5\n",
    "CREATOR = 'EscorciaSSGR'\n",
    "H5_FILE = f'/home/escorciav/datasets/activitynet/features/rgb_resnet152_max_cs-{TIME_UNIT}.h5'\n",
    "H5_FILE_FEAT_PER_FRAME = f'/home/escorciav/datasets/activitynet/features/resnet152-imagenet_{FPS}fps_320x240.hdf5'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == [['train'], ['val_1', 'val_2']]\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "from nb_utils import extend_metadata\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash \n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    FILENAMES = [Path(f'../data/raw/activitynet/{i}.json') for i in subset]\n",
    "    subset = subset[0]\n",
    "    if subset.startswith('val'):\n",
    "        subset = 'val'\n",
    "    OUTPUT_FILE = Path(f'../data/interim/activitynet/{subset}.json')\n",
    "    \n",
    "    # trick to aggregate val_1 and val_2\n",
    "    instances = sum([parse_activitynet_captions(i) for i in FILENAMES],\n",
    "                    [])\n",
    "    videos_df, _ = make_annotations_df(instances, H5_FILE_FEAT_PER_FRAME)\n",
    "    videos_gbv = videos_df.groupby('video')\n",
    "    videos, cleaned_instances = extend_metadata(\n",
    "        instances, videos_gbv, H5_FILE, offset=offset)\n",
    "    offset += len(instances)\n",
    "    \n",
    "    if not OUTPUT_FILE.parent.is_dir():\n",
    "        dirname = OUTPUT_FILE.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "    \n",
    "    print('Subset:', subset)\n",
    "    print('\\tNum videos:', len(videos))\n",
    "    print('\\tNum instances:', len(instances))\n",
    "    print('\\tNum dumped instances:', len(cleaned_instances))\n",
    "    with open(OUTPUT_FILE, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': cleaned_instances,\n",
    "                   'time_unit': TIME_UNIT,\n",
    "                   'date': datetime.now().isoformat(),\n",
    "                   'git_hash': get_git_revision_hash(),\n",
    "                   'responsible': CREATOR,\n",
    "                  },\n",
    "                  fid)\n",
    "    print('\\tDumped file:', OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b.1 Untied JSON and HDF5 inputs\n",
    "\n",
    "TLDR; reference: minor-detail. Safe to skip unless you have problems loading data for dispatching training.\n",
    "\n",
    "At some point, there was a undesired tied btw the JSON and HDF5 files (inputs) required by our implementation. \n",
    "\n",
    "- root `time_unit`. This is a property of the features, as such it should reside in the HDF5 a not in the JSON.\n",
    "\n",
    "- `videos/ith-video/num_clips`. This is a property of the ith-video, as such we should grab it from the HDF5 instead of placed it in the JSON.\n",
    "\n",
    "The following script was use to update the `*.json` files with metadata for training and evaluation.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "subsets = ['train', 'val', 'train-01', 'val-01', 'train-02', 'val-02']\n",
    "\n",
    "for subset in subsets:\n",
    "    filename = f'../data/processed/activitynet-captions/{subset}.json'\n",
    "    with open(filename, 'r') as fr:\n",
    "        data = json.load(fr)\n",
    "    del data['time_unit']\n",
    "    for video_id in data['videos']:\n",
    "        del data['videos'][video_id]['num_clips']\n",
    "    data['date'] = datetime.now().isoformat()\n",
    "    data['git_hash'] = get_git_revision_hash()\n",
    "    with open(filename, 'w') as fw:\n",
    "        json.dump(data, fw)\n",
    "```\n",
    "\n",
    "We also update the HDF5 such that it contains `metadata` [Group/Folder](http://docs.h5py.org/en/latest/high/group.html).\n",
    "\n",
    "```bash\n",
    "!h5ls /home/escorciav/datasets/activitynet-captions/features/resnet152_max_cs-5.h5 | grep metadata\n",
    "```\n",
    "\n",
    "In case the following line doesn't return anything, it means that you are using an old version of the data.\n",
    "If you know the `FPS`, `CLIP_LENGTH` and `POOL`ing operation used to get those features, the following snippet will add the metadata required for the most recent version of our code.\n",
    "\n",
    "```python\n",
    "FPS = 5\n",
    "CLIP_LENGTH = 5  # seconds\n",
    "POOL = 'max'  # pooling operation over time\n",
    "# verbose\n",
    "COMMENTS = (f'ResNet152 trained on Imagenet-ILSVRC12, Pytorch model. '\n",
    "            f'Extracted at {FPS} FPS with an image resolution of 320x240, '\n",
    "            f'and {POOL} pooled over time every {CLIP_LENGTH} seconds.')\n",
    "CREATOR = 'EscorciaSSGR'  # please add your name here to sign the file i.e. assign yourself as resposible\n",
    "filename = f'/home/escorciav/datasets/activitynet/features/resnet152_rgb_{POOL}_cl-{CLIP_LENGTH}.h5'\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "\n",
    "assert CLIP_LENGTH * FPS >= 1\n",
    "with h5py.File(filename, 'a') as fw:\n",
    "    grp = fw.create_group('metadata')\n",
    "    grp.create_dataset('time_unit', data=CLIP_LENGTH)\n",
    "    grp.create_dataset('date', data=datetime.now().isoformat(),\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "    grp.create_dataset('responsible', data=CREATOR,\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "    grp.create_dataset('comments', data=COMMENTS,\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Random partition\n",
    "\n",
    "Train/val partition out of training set. In the interest of time, we didn't take into account the action level information as we did for Charades-STA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "trial = '01'\n",
    "seed = 1701\n",
    "filename = '../data/processed/activitynet-captions/train.json'\n",
    "\n",
    "def create_subset(x):\n",
    "    \"Copy data and \"\n",
    "    split = deepcopy(x)\n",
    "    split['videos'] = {}\n",
    "    split['moments'] = []\n",
    "    return split\n",
    "\n",
    "random.seed(seed)\n",
    "with open(filename, 'r') as fid:\n",
    "    data = json.load(fid)\n",
    "    video2moment_ind = {}\n",
    "    for i, moment in enumerate(data['moments']):\n",
    "        video_id = moment['video']\n",
    "        if video_id not in video2moment_ind:\n",
    "            video2moment_ind[video_id] = []\n",
    "        video2moment_ind[video_id].append(i)\n",
    "    \n",
    "train_split = create_subset(data)\n",
    "val_split = create_subset(data)\n",
    "\n",
    "videos = list(data['videos'].keys())\n",
    "cut = int(len(videos) * 0.75)\n",
    "random.shuffle(videos)\n",
    "\n",
    "repo = train_split\n",
    "for i, video_id in enumerate(videos):\n",
    "    if i > cut:\n",
    "        repo = val_split\n",
    "    repo['videos'][video_id] = data['videos'][video_id]\n",
    "    for j in video2moment_ind[video_id]:\n",
    "        repo['moments'].append(data['moments'][j])\n",
    "with open(f'../data/processed/activitynet-captions/train-{trial}.json', 'x') as fid:\n",
    "    json.dump(train_split, fid)\n",
    "with open(f'../data/processed/activitynet-captions/val-{trial}.json', 'x') as fid:\n",
    "    json.dump(val_split, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines \n",
    "\n",
    "### 3.1 Single video moment retrieval\n",
    "\n",
    "Concurrent [work](https://arxiv.org/abs/1804.05113)\n",
    "\n",
    "| Model            | R@1,IoU=0.5 | R@1,IoU=0.7 | R@5,IoU=0.5 | R@5,IoU=0.7 |\n",
    "| :--------------- | ----------: | ----------: | ----------: | ----------: | \n",
    "| Random           |   0.0025    |   0.008     |   0.113     |     0.04    |\n",
    "| Vector Embedding |   0.237     |    0.11     |    0.52     |    0.321    |\n",
    "| LSTM+QSPN+Cap    |   0.277     |   0.136     |   0.592     |    0.383    |\n",
    "\n",
    "The values above look nice, but these are easy to copy and paste ðŸ˜‰\n",
    "\n",
    "Random\n",
    "R@{1,5,10},0.5: 2.5  11.3  21.6\n",
    "R@{1,5,10},0.7: 0.8  4.0  8.1\n",
    "\n",
    "VE\n",
    "R@{1,5,10},0.5: 23.7  52.0  62.2\n",
    "R@{1,5,10},0.7: 11.0  32.1  42.1\n",
    "\n",
    "LSTM+QSPN+Cap\n",
    "R@{1,5,10},0.5: 27.7   59.2   69.3\n",
    "R@{1,5,10},0.7: 13.6   38.3   49.1\n",
    "\n",
    "### 3.2 Moment frequency prior\n",
    "\n",
    "Results in a train-val split form train set for our search space (sliding windows between length 10s (seconds) and max length 120 with steps of 10s, stride 10s) and with `NMS = 0.5`. Please don't fool yourself and update the baseline according to your search strategy.\n",
    "\n",
    "```bash\n",
    "for i in 10 50 75 100 250 500 1000 2500 5000; do\n",
    "  python moment_freq_prior.py \\\n",
    "    --train-list data/processed/activitynet-captions/train-01.json \\\n",
    "    --test-list data/processed/activitynet-captions/val-01.json \\\n",
    "    --bins $i \\\n",
    "    --proposal-interface SlidingWindowMSFS \\\n",
    "    --min-length 10 --num-scales 12 --stride 10 --nms-threshold 0.5 \\\n",
    "    --logfile data/processed/activitynet-captions/mfp-$i.log;\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "metric = ['r@1,0.5', 'r@5,0.5', 'r@1,0.7', 'r@5,0.7']\n",
    "files= !ls ../data/processed/activitynet-captions/mfp-*.log\n",
    "bins = []\n",
    "results = []\n",
    "for file in files:\n",
    "    bins.append(int(file.split('-')[-1].split('.')[0]))\n",
    "    with open(file, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()\n",
    "            if 'r@1,0.5' in line:\n",
    "                blocks= line.split('\\t')\n",
    "                metrics = []\n",
    "                for i, content in enumerate(blocks):\n",
    "                    metrics.append(\n",
    "                        float(content.split()[-1]))\n",
    "                results.append(metrics)\n",
    "results = [x for _, x in sorted(zip(bins, results))]\n",
    "bins.sort()\n",
    "bins = np.array(bins)\n",
    "results = np.array(results)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(results.shape[1]):\n",
    "    plt.plot(bins, results[:, i], label=metric[i], lw=4,\n",
    "             marker='o')\n",
    "plt.xlabel('Number of bins')\n",
    "plt.ylabel('R@k,IoU')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose 500 bins as it's a good compromise for all the four metrics. The rationale is similar to the [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "For a given number of bins, we proceed to compute the prior using the entire training set, and evaluating of the entire testing set.\n",
    "\n",
    "```bash\n",
    "python moment_freq_prior.py \\\n",
    "  --train-list data/processed/activitynet-captions/train.json \\\n",
    "  --test-list data/processed/activitynet-captions/val.json \\\n",
    "  --bins 500 \\\n",
    "  --proposal-interface SlidingWindowMSFS \\\n",
    "  --min-length 10 --num-scales 12 --stride 10 --nms-threshold 0.5 \\\n",
    "  --logfile data/processed/activitynet-captions/mfp.log\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
