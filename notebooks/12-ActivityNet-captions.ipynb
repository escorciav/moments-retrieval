{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring ActivityNet-Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nb_utils import make_annotations_df\n",
    "from nb_utils import recall_bound_and_search_space\n",
    "from nb_utils import sliding_window\n",
    "\n",
    "def parse_activitynet_captions(filename):\n",
    "    \"\"\"Parser raw ActivityNet Captions annotations\n",
    "    Args:\n",
    "        filename (str)\n",
    "    Returns:\n",
    "        instances (list of dicts)\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    with open(filename) as f:\n",
    "        dataset = json.load(f)\n",
    "        for video_id in dataset:\n",
    "            time_and_descriptions = zip(\n",
    "                dataset[video_id][\"timestamps\"],\n",
    "                dataset[video_id][\"sentences\"])\n",
    "            for interval, description in time_and_descriptions:\n",
    "                instances.append(\n",
    "                    {'video': video_id,\n",
    "                     'times': [interval],\n",
    "                     'description': description}\n",
    "                )\n",
    "                #print(video_id, interval, description)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Moments duration analysis\n",
    "\n",
    "Similar to notebook 11.\n",
    "\n",
    "We need to set a couple of parameters:\n",
    "\n",
    "(i) _minimum_ moment length\n",
    "\n",
    "(ii) _maximum_ moment length\n",
    "\n",
    "(iii) _type of range_, how to explore minimum -> maximum\n",
    "\n",
    "(iv) _striding_.\n",
    "\n",
    "Those parameters will define the search space, and will set the stage to define the size of the chunk/clip. \n",
    "\n",
    "_Note:_ following [Xu et. al arxiv-2018](https://arxiv.org/pdf/1804.05113.pdf), we fuse the two annotations in the validation set.\n",
    "\n",
    "The first step is to get an indea of the duration of the moments in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 95\n",
    "QUANTILES = np.arange(25, 101, 5)\n",
    "\n",
    "# plot stuff\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "fontsize = 14\n",
    "lw = 3 # linewidth\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(1, 3, figsize=(21, 7))\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "                # ignore negative duration\n",
    "                if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    duration = np.array(duration)\n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    print('Negative durations in {}: {}'.format(subset, sum(duration <= 0)))\n",
    "    percentiles = np.percentile(duration, QUANTILES)\n",
    "    axs[i].plot(percentiles, QUANTILES, color=COLOR[i], lw=lw)\n",
    "    axs[-1].plot(percentiles, QUANTILES, color=COLOR[i], lw=lw)\n",
    "    axs[i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[i].set_ylabel('Percentile', fontsize=fontsize)\n",
    "    axs[i].tick_params(labelsize=fontsize)\n",
    "    axs[i].set_title('Duration stats {}\\nMin: {:.2f}, Median: {:.2f}, {}Q: {:.2f} Max: {:.2f}'\n",
    "                     .format(subset, np.min(duration[duration > 0]), np.median(duration), Q,\n",
    "                             percentiles[QUANTILES == Q][0], np.max(duration)),\n",
    "                     fontsize=fontsize)\n",
    "\n",
    "duration = np.array(all_duration)\n",
    "percentiles = np.percentile(duration, QUANTILES)\n",
    "axs[-1].plot(percentiles, QUANTILES, ls='--', color=COLOR[-1], lw=lw)\n",
    "axs[-1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[-1].set_ylabel('Quantile', fontsize=fontsize)\n",
    "axs[-1].tick_params(labelsize=fontsize)\n",
    "_ = axs[-1].set_title('Duration stats (train+val1+val2)\\nMin: {:.2f}, Median: {:.2f}, {}Q: {:.2f} Max: {:.2f}'\n",
    "                      .format(np.min(duration[duration > 0]), np.median(duration), Q,\n",
    "                              percentiles[QUANTILES == Q][0], np.max(duration)),\n",
    "                     fontsize=fontsize)\n",
    "#fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_percentile-moment-duration.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the distribution is quite particular, we decided to analyze the PDF and CDF closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_step = 5 # seconds\n",
    "duration_edges = np.arange(0, 400 + duration_step - 0.1, duration_step)\n",
    "\n",
    "# plot stuff\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "fontsize = 14\n",
    "rwidth = 0.75\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(2, 3, figsize=(21, 14), sharex=True)\n",
    "cdf_val, edges_val = None, None\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "                # ignore negative duration\n",
    "                if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    duration = np.array(duration)\n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    print('Negative durations in {}: {}'.format(subset, sum(duration <= 0)))\n",
    "    axs[0, i].hist(duration, duration_edges, color=COLOR[i], density=True,\n",
    "                   rwidth=rwidth)\n",
    "    cdf, edges, *_ = axs[1, i].hist(duration, duration_edges, color=COLOR[i], density=True,\n",
    "                                    cumulative=True, rwidth=rwidth)\n",
    "    axs[1, i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[0, i].tick_params(labelsize=fontsize)\n",
    "    axs[1, i].tick_params(labelsize=fontsize)\n",
    "    axs[0, i].set_title(subset, fontsize=fontsize)\n",
    "\n",
    "duration = np.array(all_duration)\n",
    "axs[0, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, rwidth=rwidth)\n",
    "axs[1, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, cumulative=True, rwidth=rwidth)\n",
    "axs[0, 0].set_ylabel('Norm frequency', fontsize=fontsize)\n",
    "axs[1, 0].set_ylabel('Cum frequency', fontsize=fontsize)\n",
    "axs[1, -1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[0, -1].set_title('train+val', fontsize=fontsize)\n",
    "axs[0, -1].tick_params(labelsize=fontsize)\n",
    "axs[1, -1].tick_params(labelsize=fontsize)\n",
    "fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_cdf-pdf_moments-duration.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the search space is gonna be humongous doing a linear search over a minimum length of 5s. Maybe, 10s is a better choice.\n",
    "\n",
    "Why not doing a non-linear exploration?\n",
    "\n",
    "In particular a piece-wise linear (_irrelevant joke (ignore it)_  because who does not like PWL functions these days. Before you pull the trigger in our head, make sure that you did not use a conv+relu combo in the last 10 years. Otherwise, you should commit suicide ðŸ˜‰)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [0, 25, 55, 125]\n",
    "widths = [5, 10, 25]\n",
    "duration_edges = [np.arange(times[i], times[i+1], width)\n",
    "                  for i, width in enumerate(widths)]\n",
    "duration_edges = np.concatenate(duration_edges)\n",
    "# plot stuff\n",
    "fontsize = 14\n",
    "rwidth = 0.75\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(2, 3, figsize=(21, 14), sharex=True)\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "\n",
    "    duration = [i['times'][0][1] - i['times'][0][0]\n",
    "                for i in data\n",
    "                # ignore negative duration\n",
    "                if i['times'][0][1] > i['times'][0][0]\n",
    "               ]\n",
    "    all_duration += duration\n",
    "    duration = np.array(duration)\n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    print('Negative durations in {}: {}'.format(subset, sum(duration <= 0)))\n",
    "    axs[0, i].hist(duration, duration_edges, color=COLOR[i], density=True, rwidth=rwidth)\n",
    "    cdf, edges, *_ = axs[1, i].hist(duration, duration_edges, color=COLOR[i], density=True, cumulative=True, rwidth=rwidth)\n",
    "    axs[1, i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[0, i].tick_params(labelsize=fontsize)\n",
    "    axs[1, i].tick_params(labelsize=fontsize)\n",
    "    axs[0, i].set_title(subset, fontsize=fontsize)\n",
    "    \n",
    "duration = np.array(all_duration)\n",
    "axs[0, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, rwidth=rwidth)\n",
    "axs[1, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, cumulative=True, rwidth=rwidth)\n",
    "axs[0, 0].set_ylabel('Norm frequency', fontsize=fontsize)\n",
    "axs[1, 0].set_ylabel('Cum frequency', fontsize=fontsize)\n",
    "axs[1, -1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[0, -1].set_title('train+val', fontsize=fontsize)\n",
    "axs[0, -1].tick_params(labelsize=fontsize)\n",
    "axs[1, -1].tick_params(labelsize=fontsize)\n",
    "fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_cdf-pdf_moments-duration_non-uniform-bins.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot look sparse and piece-wise linear :), but how did we pick the values?\n",
    "\n",
    "1. The minimum lenght of 5s was picked based on the fact that it covers at least 10% of the annotations (check the cum freq plot). We liked multiples, thus the other windows are a scaled version of 5s ;).\n",
    "\n",
    "1. What about the 25s (in `times`)? it's close to the median duration, thus we explore the range 0-50% precisely.\n",
    "\n",
    "1. The value of 55s (in `times`) was conveniently chosen to cover around 75% of the annotations.\n",
    "\n",
    "1. The value of 125s (in `times`) was picked because matches with the 95% coverage of the annotations.\n",
    "\n",
    "1. There was no science picking the `widths` 10 and 25s. @escorcia wanted to move forward quickly.\n",
    "\n",
    "  In future work, we may repeat the reasoning of (2) (explore 10% to 50%) in those ranges to select them appropriately. A more interesting strategy is to make duration edges that give you a linear cdf i.e. you force your data to be uniformly distributed wrt to duration ðŸ˜‰. That approach should be constrained to produce few number of widths and take into account the IOU threshold during evaluation.\n",
    "\n",
    "### Video duration\n",
    "\n",
    "Let's take a look at the duration of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H5_FILE_FEAT_PER_FRAME = '/home/escorciav/datasets/activitynet/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "FPS = 5\n",
    "\n",
    "duration_edges = np.arange(0, 211, 30)\n",
    "\n",
    "# plot stuff\n",
    "COLOR = ['blue', 'orange', 'green']\n",
    "fontsize = 14\n",
    "rwidth = 0.75\n",
    "\n",
    "all_duration = []\n",
    "fig, axs = plt.subplots(2, 3, figsize=(21, 14), sharex=True)\n",
    "# we use a list of list to merge val_1 and val_2 ;)\n",
    "for i, subsets in enumerate([['train'], ['val_1', 'val_2']]):\n",
    "    data = []\n",
    "    for subset in subsets:\n",
    "        filename = '../data/raw/activitynet/{}.json'.format(subset)\n",
    "        data.append(parse_activitynet_captions(filename))\n",
    "    # this will merge the instances ;)\n",
    "    data = sum(data, [])\n",
    "    \n",
    "    videos_df, _ = make_annotations_df(data, H5_FILE_FEAT_PER_FRAME)\n",
    "    duration = videos_df['num_frames'].values / FPS\n",
    "    all_duration.append(duration)\n",
    "    \n",
    "    if subset.startswith('val'):\n",
    "        subset = subset[:-2]\n",
    "    axs[0, i].hist(duration, duration_edges, color=COLOR[i], density=True, rwidth=rwidth)\n",
    "    axs[1, i].hist(duration, duration_edges, color=COLOR[i], density=True, cumulative=True, rwidth=rwidth)\n",
    "    axs[1, i].set_xlabel('Duration', fontsize=fontsize)\n",
    "    axs[0, i].tick_params(labelsize=fontsize)\n",
    "    axs[1, i].tick_params(labelsize=fontsize)\n",
    "    axs[0, i].set_title(subset, fontsize=fontsize)\n",
    "\n",
    "    \n",
    "duration = np.concatenate(all_duration)\n",
    "axs[0, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, rwidth=rwidth)\n",
    "axs[1, -1].hist(duration, duration_edges, ls='--', color=COLOR[-1], density=True, cumulative=True, rwidth=rwidth)\n",
    "axs[0, 0].set_ylabel('Norm frequency', fontsize=fontsize)\n",
    "axs[1, 0].set_ylabel('Cum frequency', fontsize=fontsize)\n",
    "axs[1, -1].set_xlabel('Duration', fontsize=fontsize)\n",
    "axs[0, -1].set_title('train_val', fontsize=fontsize)\n",
    "axs[0, -1].tick_params(labelsize=fontsize)\n",
    "axs[1, -1].tick_params(labelsize=fontsize)\n",
    "# fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_cdf-pdf_video-duration_uniform-bins-30s.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Search space with sliding windows\n",
    "\n",
    "#### 1a.1 Linear\n",
    "\n",
    "Durations choose between 5-50s, strides multiples of 5s and sublinear strides such as 1 and 2.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to edit for analysis\n",
    "min_length = 5\n",
    "num_scales = 10\n",
    "strides = [1, 2.5, 5, 10]\n",
    "\n",
    "# Eval stuff\n",
    "IOU_THRESHOLDS = [0.5, 0.7]\n",
    "\n",
    "# Dataset stuff\n",
    "annotation_files = ['../data/raw/activitynet/val_1.json',\n",
    "                    '../data/raw/activitynet/val_2.json']\n",
    "features_file = '/home/escorciav/datasets/activitynet/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "\n",
    "# plot stuff\n",
    "figsize = (10, 7) # (21, 7)\n",
    "fontsize = 14\n",
    "lw = 3  # linewidth\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = sum(\n",
    "    [parse_activitynet_captions(i) for i in annotation_files],\n",
    "    [])\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales,\n",
    "        slidding_window_fn=sliding_window,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=figsize)\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls, lw=lw,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=fontsize)\n",
    "ax1.set_ylabel('Recall', fontsize=fontsize)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', lw=lw,\n",
    "         color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=fontsize)\n",
    "ax2.tick_params('y', colors='r', labelsize=fontsize)\n",
    "ax1.tick_params(labelsize=fontsize)\n",
    "ax1.legend(fontsize=fontsize)\n",
    "# fig.savefig('/home/escorciav/Downloads/adobe-prj/anet_recall-and-ss-vs-stride_sw-5-50-10.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'S3 {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a.2 Linear\n",
    "\n",
    "Durations choose between 10-120s, strides multiples of 10s and one sublinear strides such as 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to edit for analysis\n",
    "min_length = 10\n",
    "num_scales = 12\n",
    "strides = [5, 10, 20, 30]\n",
    "\n",
    "# Eval stuff\n",
    "IOU_THRESHOLDS = [0.5, 0.7]\n",
    "\n",
    "# Dataset stuff\n",
    "annotation_files = ['../data/raw/activitynet/val_1.json',\n",
    "                    '../data/raw/activitynet/val_2.json']\n",
    "features_file = '/home/escorciav/datasets/activitynet/features/resnet152-imagenet_5fps_320x240.hdf5'\n",
    "\n",
    "# plot stuff\n",
    "font_size = 14\n",
    "COLOR_2ND_AXIS = 'red' \n",
    "IOU_COLORS = ['blue', 'orange', 'green']\n",
    "iou_thresholds = IOU_THRESHOLDS\n",
    "assert len(IOU_COLORS) - 1 == len(iou_thresholds)\n",
    "\n",
    "instances = sum(\n",
    "    [parse_activitynet_captions(i) for i in annotation_files],\n",
    "    [])\n",
    "videos_df, instances_df = make_annotations_df(instances, features_file)\n",
    "\n",
    "recalls = []\n",
    "search_space = []\n",
    "for stride in tqdm(strides):\n",
    "    recall_iou, search_space_stats = recall_bound_and_search_space(\n",
    "        videos_df, instances_df, stride,\n",
    "        length=min_length, scale=num_scales,\n",
    "        slidding_window_fn=sliding_window,\n",
    "    )\n",
    "    recalls.append(recall_iou)\n",
    "    search_space.append(search_space_stats)\n",
    "search_space = np.vstack(search_space)\n",
    "recalls = np.vstack(recalls)\n",
    "recalls = np.column_stack([recalls, recalls.mean(axis=1)])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(21, 7))\n",
    "for i, iou in enumerate(iou_thresholds + [None]):\n",
    "    ls, label, color = '-', f'tIOU={iou}', IOU_COLORS[i]\n",
    "    if i == len(IOU_COLORS) - 1:\n",
    "        ls, label = '-.', 'avg tIOU'\n",
    "    ax1.plot(strides, recalls[:, i], ls=ls,\n",
    "             color=color, label=label)\n",
    "ax1.set_xlabel('stride', fontsize=font_size)\n",
    "ax1.set_ylabel('Recall', fontsize=font_size)\n",
    "ax1.tick_params('y')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(strides, search_space[:, 0], ls='--', color=COLOR_2ND_AXIS)\n",
    "ax2.set_ylabel('Median size of search space',\n",
    "               color=COLOR_2ND_AXIS, fontsize=font_size)\n",
    "ax2.tick_params('y', colors='r', labelsize=fontsize)\n",
    "ax1.tick_params(labelsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'Stride': np.array(strides)}\n",
    "for i in range(recalls.shape[1]):\n",
    "    if i > (len(iou_thresholds) - 1):\n",
    "        iou = f'Avg({iou_thresholds[0]}, {iou_thresholds[-1]})'\n",
    "    else:\n",
    "        iou = iou_thresholds[i]\n",
    "    info[f'R@{iou}'] = recalls[:, i]\n",
    "for i, label in enumerate(['(median)', '(std)']):\n",
    "    info[f'Search space size {label}'] = search_space[:, i]\n",
    "display(pd.DataFrame(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a.2 Piece-wise linear exploration\n",
    "\n",
    "_todo_ non-linear search space with piece-wise linear function:\n",
    "\n",
    "durations $\\in \\{5, 10, 25\\}$\n",
    "\n",
    "time range for those durations t = $\\{0, 25, 55, 125\\}$\n",
    "\n",
    "TODO: get all possible duration in a linear function over the corresponding pairs of $t$ e.g. from $t = [25, 55] \\rightarrow D_t = \\{25, 35, 45\\}$\n",
    "\n",
    "TODO: write down piece-wise function of $t$ that return all possible durations.\n",
    "\n",
    "strides TBD according to overlap threshold, around 0.6.\n",
    "\n",
    "Try this config. BTW, the last two lines can span $D$ as piece-wise linear function inside time range.\n",
    "\n",
    "```python\n",
    "times = [0, 5, 15, 25, 45, 100]\n",
    "widths = [2.5, 5, 10, 20, 50]\n",
    "duration_edges = [np.arange(times[i], times[i+1], width)\n",
    "                  for i, width in enumerate(widths)]\n",
    "duration_edges = np.concatenate(duration_edges)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dump data for training and evaluation\n",
    "\n",
    "### 2a. Chunked features\n",
    "\n",
    "Go to notebook `4-feature-extraction.ipynb` section `#Varied-length-videos` (remove the # if you use your browser string matching).\n",
    "\n",
    "_TODO_ add procedure here to avoid jumping over the place.\n",
    "\n",
    "### 2b. JSON files\n",
    "\n",
    "The same as in notebook `11-charades-sta.ipynb` section 2a.\n",
    "\n",
    "Following [this paper](https://arxiv.org/abs/1804.05113), we merge the validations set into a single validation set.\n",
    "\n",
    "__Note__: It requires to run 1st cell with function `parse_activitynet_captions`.\n",
    "\n",
    "_minor details_\n",
    "\n",
    "To avoid spaghetti code, we copy the function `extend_metadata` into the module `nb_utils.py`.\n",
    "\n",
    "_comment:_ in principle we could automate this. However, there is a well-know principle that says: \n",
    "\n",
    "1. do it manually.\n",
    "\n",
    "2. repeat 1 again.\n",
    "\n",
    "3. automate it.\n",
    "\n",
    "we are in the step 2 ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SUBSETS = [['train'], ['val_1', 'val_2']]\n",
    "TIME_UNIT = 5\n",
    "MODE = 'x'\n",
    "FPS = 5\n",
    "CREATOR = 'EscorciaSSGR'\n",
    "H5_FILE = f'/home/escorciav/datasets/activitynet/features/rgb_resnet152_max_cs-{TIME_UNIT}.h5'\n",
    "H5_FILE_FEAT_PER_FRAME = f'/home/escorciav/datasets/activitynet/features/resnet152-imagenet_{FPS}fps_320x240.hdf5'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == [['train'], ['val_1', 'val_2']]\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "from nb_utils import extend_metadata\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash \n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    FILENAMES = [Path(f'../data/raw/activitynet/{i}.json') for i in subset]\n",
    "    subset = subset[0]\n",
    "    if subset.startswith('val'):\n",
    "        subset = 'val'\n",
    "    OUTPUT_FILE = Path(f'../data/interim/activitynet/{subset}.json')\n",
    "    \n",
    "    # trick to aggregate val_1 and val_2\n",
    "    instances = sum([parse_activitynet_captions(i) for i in FILENAMES],\n",
    "                    [])\n",
    "    videos_df, _ = make_annotations_df(instances, H5_FILE_FEAT_PER_FRAME)\n",
    "    videos_gbv = videos_df.groupby('video')\n",
    "    videos, cleaned_instances = extend_metadata(\n",
    "        instances, videos_gbv, H5_FILE, offset=offset)\n",
    "    offset += len(instances)\n",
    "    \n",
    "    if not OUTPUT_FILE.parent.is_dir():\n",
    "        dirname = OUTPUT_FILE.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "    \n",
    "    print('Subset:', subset)\n",
    "    print('\\tNum videos:', len(videos))\n",
    "    print('\\tNum instances:', len(instances))\n",
    "    print('\\tNum dumped instances:', len(cleaned_instances))\n",
    "    with open(OUTPUT_FILE, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': cleaned_instances,\n",
    "                   'time_unit': TIME_UNIT,\n",
    "                   'date': datetime.now().isoformat(),\n",
    "                   'git_hash': get_git_revision_hash(),\n",
    "                   'responsible': CREATOR,\n",
    "                  },\n",
    "                  fid)\n",
    "    print('\\tDumped file:', OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Random partition\n",
    "\n",
    "Train/val partition out of training set. In the interest of time, we didn't take into account the action level information as we did for Charades-STA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "trial = '01'\n",
    "seed = 1701\n",
    "filename = '../data/processed/activitynet-captions/train.json'\n",
    "\n",
    "def create_subset(x):\n",
    "    \"Copy data and \"\n",
    "    split = deepcopy(x)\n",
    "    split['videos'] = {}\n",
    "    split['moments'] = []\n",
    "    return split\n",
    "\n",
    "random.seed(seed)\n",
    "with open(filename, 'r') as fid:\n",
    "    data = json.load(fid)\n",
    "    video2moment_ind = {}\n",
    "    for i, moment in enumerate(data['moments']):\n",
    "        video_id = moment['video']\n",
    "        if video_id not in video2moment_ind:\n",
    "            video2moment_ind[video_id] = []\n",
    "        video2moment_ind[video_id].append(i)\n",
    "    \n",
    "train_split = create_subset(data)\n",
    "val_split = create_subset(data)\n",
    "\n",
    "videos = list(data['videos'].keys())\n",
    "cut = int(len(videos) * 0.75)\n",
    "random.shuffle(videos)\n",
    "\n",
    "repo = train_split\n",
    "for i, video_id in enumerate(videos):\n",
    "    if i > cut:\n",
    "        repo = val_split\n",
    "    repo['videos'][video_id] = data['videos'][video_id]\n",
    "    for j in video2moment_ind[video_id]:\n",
    "        repo['moments'].append(data['moments'][j])\n",
    "with open(f'../data/processed/activitynet-captions/train-{trial}.json', 'x') as fid:\n",
    "    json.dump(train_split, fid)\n",
    "with open(f'../data/processed/activitynet-captions/val-{trial}.json', 'x') as fid:\n",
    "    json.dump(val_split, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baselines \n",
    "\n",
    "### 3.1 Single video moment retrieval\n",
    "\n",
    "Concurrent [work](https://arxiv.org/abs/1804.05113)\n",
    "\n",
    "| Model            | R@1,IoU=0.5 | R@1,IoU=0.7 | R@5,IoU=0.5 | R@5,IoU=0.7 |\n",
    "| :--------------- | ----------: | ----------: | ----------: | ----------: | \n",
    "| Random           |   0.0025    |   0.008     |   0.113     |     0.04    |\n",
    "| Vector Embedding |   0.237     |    0.11     |    0.52     |    0.321    |\n",
    "| LSTM+QSPN+Cap    |   0.277     |   0.136     |   0.592     |    0.383    |\n",
    "\n",
    "The values above look nice, but these are easy to copy and paste ðŸ˜‰\n",
    "\n",
    "Random\n",
    "R@{1,5,10},0.5: 2.5  11.3  21.6\n",
    "R@{1,5,10},0.7: 0.8  4.0  8.1\n",
    "\n",
    "VE\n",
    "R@{1,5,10},0.5: 23.7  52.0  62.2\n",
    "R@{1,5,10},0.7: 11.0  32.1  42.1\n",
    "\n",
    "LSTM+QSPN+Cap\n",
    "R@{1,5,10},0.5: 27.7   59.2   69.3\n",
    "R@{1,5,10},0.7: 13.6   38.3   49.1\n",
    "\n",
    "### 3.2 Moment frequency prior\n",
    "\n",
    "Results in a train-val split form train set for our search space (sliding windows between length 10s (seconds) and max length 120 with steps of 10s, stride 10s) and with `NMS = 0.5`. Please don't fool yourself and update the baseline according to your search strategy.\n",
    "\n",
    "```bash\n",
    "for i in 10 50 75 100 250 500 1000 2500 5000; do\n",
    "  python moment_freq_prior.py \\\n",
    "    --train-list data/processed/activitynet-captions/train-01.json \\\n",
    "    --test-list data/processed/activitynet-captions/val-01.json \\\n",
    "    --bins $i \\\n",
    "    --proposal-interface SlidingWindowMSFS \\\n",
    "    --min-length 10 --num-scales 12 --stride 10 --nms-threshold 0.5 \\\n",
    "    --logfile data/processed/activitynet-captions/mfp-$i.log;\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "metric = ['r@1,0.5', 'r@5,0.5', 'r@1,0.7', 'r@5,0.7']\n",
    "files= !ls ../data/processed/activitynet-captions/mfp-*.log\n",
    "bins = []\n",
    "results = []\n",
    "for file in files:\n",
    "    bins.append(int(file.split('-')[-1].split('.')[0]))\n",
    "    with open(file, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()\n",
    "            if 'r@1,0.5' in line:\n",
    "                blocks= line.split('\\t')\n",
    "                metrics = []\n",
    "                for i, content in enumerate(blocks):\n",
    "                    metrics.append(\n",
    "                        float(content.split()[-1]))\n",
    "                results.append(metrics)\n",
    "results = [x for _, x in sorted(zip(bins, results))]\n",
    "bins.sort()\n",
    "bins = np.array(bins)\n",
    "results = np.array(results)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(results.shape[1]):\n",
    "    plt.plot(bins, results[:, i], label=metric[i], lw=4,\n",
    "             marker='o')\n",
    "plt.xlabel('Number of bins')\n",
    "plt.ylabel('R@k,IoU')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose 500 bins as it's a good compromise for all the four metrics. The rationale is similar to the [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "For a given number of bins, we proceed to compute the prior using the entire training set, and evaluating of the entire testing set.\n",
    "\n",
    "```bash\n",
    "python moment_freq_prior.py \\\n",
    "  --train-list data/processed/activitynet-captions/train.json \\\n",
    "  --test-list data/processed/activitynet-captions/val.json \\\n",
    "  --bins 500 \\\n",
    "  --proposal-interface SlidingWindowMSFS \\\n",
    "  --min-length 10 --num-scales 12 --stride 10 --nms-threshold 0.5 \\\n",
    "  --logfile data/processed/activitynet-captions/mfp.log\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
