{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dump data for training and evaluation\n",
    "\n",
    "### 1a. Chunked features\n",
    "\n",
    "In case, you haven't dumped the features. Go to notebook [old feature extraction](4-feature-extraction.ipynb) section `#Varied-length-videos` (remove the # if you use your browser string matching).\n",
    "\n",
    "_TODO_ add procedure here to avoid jumping over the repo.\n",
    "\n",
    "### 1b. JSON files\n",
    "\n",
    "The format is the same as in notebook [charades notebook](11-charades-sta.ipynb).\n",
    "\n",
    "We added the field:\n",
    "    - `annotation_id_didemo` given that didemo provides an annotation id, but is only unique inside a subset.\n",
    "    \n",
    "_Implementation details and considerations:_\n",
    "\n",
    "Given the continuous nature of untrimmed videos, it is a bit trickier to have a 1-to-1 equivalence between this format and the original discrete data of DiDeMo. However, we try our best for replicating the insights from the [MCN paper](https://arxiv.org/pdf/1708.01641.pdf). In particular:\n",
    "\n",
    "- The video `duration` is set to 30s to approximate the TEF features proposed by [MCN](https://github.com/LisaAnne/LocalizingMoments). Note that even making `duration == 30`, the continous TEF features are different to those of the discrete setup e.g. [5, 10] / 30 != [1, 1] / 6.\n",
    "\n",
    "- Global features will be computed only for the existing clips of the video. Thus, `num_clips != duration != num_frames`.\n",
    "\n",
    "- [DiDeMo dataset](https://github.com/LisaAnne/LocalizingMoments/tree/9b453d2af2255c9c7b2a3e5f3d345d2f06c2ec20)\n",
    "\n",
    "    It corresponds to an older version of the code, but hopefully is the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "# This time unit (seconds) must match the one in the original DiDeMo\n",
    "# annotations, link in the description above.\n",
    "DIDEMO_TIME_UNIT = 5 \n",
    "MAX_TIME = 30\n",
    "\n",
    "def update_instances_make_videos_dict(moments, offset=0):\n",
    "    \"\"\"Update (in-place) metadata from instances\n",
    "    \n",
    "    1. Transform annotations from index to time\n",
    "    2. Backup annotation-id and create a new-one\n",
    "    4. Remove unneeded fields `num_segments`, `dl_link`. Note that we can go\n",
    "       back to them because we preserve the original `annotation_id`.\n",
    "    3. Add field `time` added 'cause we weren't planning to merge both\n",
    "       domains, untrimmed & trimmed videos.\n",
    "\n",
    "    Args:\n",
    "        moments (list of dict): raw data from DiDeMo\n",
    "        \n",
    "    Returns:\n",
    "        videos (dict) : map information about videos in the subset.\n",
    "    \"\"\"\n",
    "    videos = {}\n",
    "    for moment_i in moments:\n",
    "        time_stamps = np.array(moment_i['times'])\n",
    "        time_stamps *= DIDEMO_TIME_UNIT\n",
    "        time_stamps[:, 1] += DIDEMO_TIME_UNIT\n",
    "        moment_i['times'] = time_stamps.tolist()\n",
    "        # DIDEMO_TIME_UNIT * 6 == 30s, which is the time-span that annotators\n",
    "        # watched\n",
    "        assert (time_stamps <= DIDEMO_TIME_UNIT * 6).all()\n",
    "        \n",
    "        moment_i['annotation_id_original'] = moment_i['annotation_id']\n",
    "        moment_i['annotation_id'] = offset\n",
    "        \n",
    "        del moment_i['num_segments']\n",
    "        del moment_i['dl_link']\n",
    "        moment_i['time'] = None\n",
    "        offset += 1\n",
    "        \n",
    "        video_id = moment_i['video']\n",
    "        if video_id in videos:\n",
    "            videos[video_id]['num_instances'] += 1\n",
    "            continue\n",
    "\n",
    "        videos[video_id] = {\n",
    "            'num_instances': 1,\n",
    "            # This is incorrect, but we follow the ICCV17 recipe for fair\n",
    "            # comparison. Keep in mind, that we pool features accordingly.\n",
    "            'num_frames': MAX_TIME * DIDEMO_TIME_UNIT,\n",
    "            'duration': MAX_TIME\n",
    "        }\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SUBSETS = ['train', 'val', 'test']\n",
    "MODE = 'x'\n",
    "CREATOR = 'EscorciaSSGR'\n",
    "RAW_DATA_FMT = '../data/raw/{}_data.json'\n",
    "OUTPUT_FMT = '../data/interim/didemo/{}.json'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == ['train', 'val', 'test']\n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    filename = Path(RAW_DATA_FMT.format(subset))\n",
    "    output_file = Path(OUTPUT_FMT.format(subset))\n",
    "    with open(filename, 'r') as fid:\n",
    "        instances = json.load(fid)\n",
    "        videos = update_instances_make_videos_dict(\n",
    "            instances, offset)\n",
    "        offset += len(instances)\n",
    "\n",
    "    if not output_file.parent.is_dir():\n",
    "        dirname = output_file.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "\n",
    "    print('Subset:', subset)\n",
    "    print('\\tNum videos:', len(videos))\n",
    "    print('\\tNum instances:', len(instances))\n",
    "    with open(output_file, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': instances,\n",
    "                   'date': datetime.now().isoformat(),\n",
    "                   'git_hash': get_git_revision_hash(),\n",
    "                   'responsible': CREATOR,\n",
    "                  },\n",
    "                  fid)\n",
    "    print('\\tDumped file:', output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.1 Create partition for hyper-parameter search\n",
    "\n",
    "Script to sample $p\\%$ of the training set to avoid over-fitting during training. Although, it's passed to our implementation as \"validation\" data, don't get confused. The purposes of this subset are twofold:\n",
    "- Get an idea of performance in the training set to study over/under-fitting.\n",
    "\n",
    "- validate that the training scheme translates into good performance.\n",
    "\n",
    "_Why not doing directly in the training loop?_\n",
    "\n",
    "We are comparing two randomly sampled intervals of the video. While in inference, we are solving a retrieval over a single video. Given that the scheme is different, we did this hack to simplify the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from nb_utils import split_moments_dataset\n",
    "\n",
    "filename = '../data/processed/didemo/train-03.json'\n",
    "trial = '01'\n",
    "seed = 1701\n",
    "\n",
    "random.seed(seed)\n",
    "_, val = split_moments_dataset(filename, ratio=0.85)\n",
    "\n",
    "with open(f'../data/processed/didemo/train-03_{trial}.json', 'x') as fid:\n",
    "    json.dump(val, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.2 Untied JSON and HDF5 inputs\n",
    "\n",
    "TLDR; reference: minor-detail. Safe to skip unless you have problems loading data for dispatching training.\n",
    "\n",
    "At some point, there was a undesired tied btw the JSON and HDF5 files (inputs) required by our implementation. \n",
    "\n",
    "- root `time_unit`. This is a property of the features, as such it should reside in the HDF5 a not in the JSON.\n",
    "\n",
    "- `videos/ith-video/num_clips`. This is a property of the ith-video, as such we should grab it from the HDF5 instead of placed it in the JSON.\n",
    "\n",
    "The following script was use to create the `*-03.json` files with metadata for training and evaluation.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "subsets = ['train', 'val', 'test']\n",
    "\n",
    "for subset in subsets:\n",
    "    file_src = f'../data/processed/didemo/{subset}-02.json'\n",
    "    file_dst = f'../data/processed/didemo/{subset}-03.json'\n",
    "    with open(file_src, 'r') as fr:\n",
    "        data = json.load(fr)\n",
    "    del data['time_unit']\n",
    "    for video_id in data['videos']:\n",
    "        del data['videos'][video_id]['num_clips']\n",
    "    data['date'] = datetime.now().isoformat()\n",
    "    data['git_hash'] = get_git_revision_hash()\n",
    "    with open(file_dst, 'x') as fw:\n",
    "        json.dump(data, fw)\n",
    "```\n",
    "\n",
    "We also update the HDF5 such that it contains `metadata` [Group/Folder](http://docs.h5py.org/en/latest/high/group.html).\n",
    "\n",
    "```bash\n",
    "!h5ls /home/escorciav/datasets/didemo/features/resnet152_max_cs-5.h5 | grep metadata\n",
    "```\n",
    "\n",
    "In case the following line doesn't return anything, it means that you are using an old version of the data.\n",
    "If you know the `FPS`, `CLIP_LENGTH` and `POOL`ing operation used to get those features, the following snippet will add the metadata required for the most recent version of our code.\n",
    "\n",
    "```python\n",
    "FPS = 5\n",
    "CLIP_LENGTH = 5  # seconds\n",
    "POOL = 'max'  # pooling operation over time\n",
    "# verbose\n",
    "COMMENTS = (f'ResNet152 trained on Imagenet-ILSVRC12, Pytorch model. '\n",
    "            f'Extracted at {FPS} FPS with an image resolution of 320x240, '\n",
    "            f'and {POOL} pooled over time every {CLIP_LENGTH} seconds.')\n",
    "CREATOR = 'EscorciaSSGR'  # please add your name here to sign the file i.e. assign yourself as resposible\n",
    "filename = f'/home/escorciav/datasets/didemo/features/resnet152_{POOL}_cs-{CLIP_LENGTH}.h5'\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "\n",
    "assert CHUNK_SIZE * FPS >= 1\n",
    "with h5py.File(filename, 'a') as fw:\n",
    "    grp = fw.create_group('metadata')\n",
    "    grp.create_dataset('time_unit', data=CLIP_LENGTH)\n",
    "    grp.create_dataset('date', data=datetime.now().isoformat(),\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "    grp.create_dataset('responsible', data=CREATOR,\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "    grp.create_dataset('comments', data=COMMENTS,\n",
    "                       dtype=h5py.special_dtype(vlen=str))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Word vectors\n",
    "\n",
    "We extracted word vectors for ELMO and FastText. In the interest of time, we will document this step later.\n",
    "\n",
    "### 2.1 ELMo\n",
    "\n",
    "We take last output as word representation.\n",
    "\n",
    "_TODO_: study each layer independently as well as all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "for subset in ['train', 'val', 'test']:\n",
    "\n",
    "    h5_file = f'../data/interim/word-vectors/elmo/elmo_wordvec_didemo_{subset}.hdf5'\n",
    "    output_file = f'../data/processed/didemo/{subset}_elmo-001.h5'\n",
    "\n",
    "    txt_file = f'../data/interim/word-vectors/descriptions/didemo_descriptions_{subset}.txt'\n",
    "    json_file = f'../data/processed/didemo/{subset}.json'\n",
    "    # with open(json_file, 'r') as f_json, h5py.File(h5_file, 'r') as f_h5, h5py.File(txt_file, 'x') as fw:\n",
    "    with h5py.File(h5_file, 'r') as f_h5, h5py.File(output_file, 'w') as fw:\n",
    "        for i in range(len(f_h5) - 1):\n",
    "            item = f_h5[f'{i}'][:]\n",
    "            item = item.transpose(1, 0, 2)\n",
    "            n = item.shape[0]\n",
    "\n",
    "            # Take last layer\n",
    "            item = item[:, -1, :].reshape(n, -1)\n",
    "            fw.create_dataset(str(i), data=item, chunks=True)\n",
    "        # cross-check\n",
    "        # data = json.load(f_json)['moments']\n",
    "        # for i, line in enumerate(f_txt):\n",
    "            # line = line.strip()\n",
    "            # if data[i]['description'] != line:\n",
    "                # if '\\n' in data[i]['description']:\n",
    "                #     continue\n",
    "                # elif data[i]['description'][-1] == ' ':\n",
    "                #     continue\n",
    "\n",
    "    # cross-check\n",
    "    with open(json_file, 'r') as fid:\n",
    "        assert len(json.load(fid)['moments']) == (i + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
