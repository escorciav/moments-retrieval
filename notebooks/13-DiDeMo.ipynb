{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dump data for training and evaluation\n",
    "\n",
    "### 1a. Chunked features\n",
    "\n",
    "In case, you haven't dumped the features. Go to notebook [old feature extraction](4-feature-extraction.ipynb) section `#Varied-length-videos` (remove the # if you use your browser string matching).\n",
    "\n",
    "_TODO_ add procedure here to avoid jumping over the place.\n",
    "\n",
    "### 1b. JSON files\n",
    "\n",
    "The format is the same as in notebook [charades notebook](11-charades-sta.ipynb).\n",
    "\n",
    "We added the field:\n",
    "    - `annotation_id_didemo` given that didemo provides an annotation id, but is only unique inside a subset.\n",
    "    \n",
    "_Implementation details and considerations:_\n",
    "\n",
    "Given the continuous nature of untrimmed videos, it is a bit trickier to have a 1-to-1 equivalence between this format and the original discrete data of DiDeMo. However, we try our best for replicating the insights from the [MCN paper](https://arxiv.org/pdf/1708.01641.pdf). In particular:\n",
    "\n",
    "- The video `duration` is set to 30s to approximate the TEF features proposed by [MCN](https://github.com/LisaAnne/LocalizingMoments). Note that even making `duration == 30`, the continous TEF features are different to those of the discrete setup e.g. [5, 10] / 30 != [1, 1] / 6.\n",
    "\n",
    "- Global features will be computed only for the existing clips of the video. Thus, `num_clips != duration != num_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_git_revision_hash\n",
    "\n",
    "DIDEMO_TIME_UNIT = 5\n",
    "MAX_TIME = 30\n",
    "\n",
    "def update_instances_make_videos_dict(moments, filename, offset=0):\n",
    "    \"\"\"Update (in-place) metadata from instances\n",
    "    \n",
    "    1. Transform annotations from index to time\n",
    "    2. Backup annotation-id and create a new-one\n",
    "    4. Remove unneeded fields `num_segments`, `dl_link`. Note that we can go\n",
    "       back to them because we preserve the original `annotation_id`.\n",
    "    3. Add field `time` added 'cause we weren't planning to merge both\n",
    "       domains, untrimmed & trimmed videos.\n",
    "\n",
    "    Args:\n",
    "        moments (list of dict): raw data from DiDeMo\n",
    "        \n",
    "    Returns:\n",
    "        videos (dict) : map information about videos in the subset.\n",
    "    \"\"\"\n",
    "    videos = {}\n",
    "    for moment_i in moments:\n",
    "        time_stamps = np.array(moment_i['times'])\n",
    "        time_stamps *= DIDEMO_TIME_UNIT\n",
    "        time_stamps[:, 1] += DIDEMO_TIME_UNIT\n",
    "        moment_i['times'] = time_stamps.tolist()\n",
    "        # DIDEMO_TIME_UNIT * 6 == 30s, which is the time-span that annotators\n",
    "        # watched\n",
    "        assert (time_stamps <= DIDEMO_TIME_UNIT * 6).all()\n",
    "        \n",
    "        moment_i['annotation_id_original'] = moment_i['annotation_id']\n",
    "        moment_i['annotation_id'] = offset\n",
    "        \n",
    "        del moment_i['num_segments']\n",
    "        del moment_i['dl_link']\n",
    "        moment_i['time'] = None\n",
    "        offset += 1\n",
    "        \n",
    "        video_id = moment_i['video']\n",
    "        if video_id in videos:\n",
    "            videos[video_id]['num_instances'] += 1\n",
    "            continue\n",
    "        with h5py.File(filename, 'r') as fid:\n",
    "            features = fid[video_id][:]\n",
    "        num_clips, ind = 6, -1\n",
    "        while features[ind, :].sum() == 0:\n",
    "            num_clips -= 1\n",
    "            ind -= 1\n",
    "        videos[video_id] = {\n",
    "            'num_instances': 1,\n",
    "            'num_clips': num_clips,\n",
    "            # This is incorrect, but we follow the ICCV17 recipe for fair\n",
    "            # comparison. Note that we dumped features accordingly.\n",
    "            'num_frames': MAX_TIME * DIDEMO_TIME_UNIT,\n",
    "            'duration': MAX_TIME\n",
    "        }\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SUBSETS = ['train', 'val', 'test']\n",
    "MODE = 'x'\n",
    "CREATOR = 'EscorciaSSGR'\n",
    "RAW_DATA_FMT = '../data/raw/{}_data.json'\n",
    "OUTPUT_FMT = '../data/interim/didemo/{}.json'\n",
    "H5_FILE = '../data/processed/didemo/rgb_resnet152_max.h5'\n",
    "if MODE == 'w':\n",
    "    print('are you sure you wanna do this? comment these 3 lines!')\n",
    "    raise\n",
    "assert SUBSETS == ['train', 'val', 'test']\n",
    "\n",
    "offset = 0\n",
    "for subset in SUBSETS:\n",
    "    filename = Path(RAW_DATA_FMT.format(subset))\n",
    "    output_file = Path(OUTPUT_FMT.format(subset))\n",
    "    with open(filename, 'r') as fid:\n",
    "        instances = json.load(fid)\n",
    "        videos = update_instances_make_videos_dict(\n",
    "            instances, H5_FILE, offset)\n",
    "        offset += len(instances)\n",
    "\n",
    "    if not output_file.parent.is_dir():\n",
    "        dirname = output_file.parent\n",
    "        dirname.mkdir(parents=True)\n",
    "        print(f'Create dir: {dirname}')\n",
    "\n",
    "    print('Subset:', subset)\n",
    "    print('\\tNum videos:', len(videos))\n",
    "    print('\\tNum instances:', len(instances))\n",
    "    with open(output_file, MODE) as fid:\n",
    "        json.dump({'videos': videos,\n",
    "                   'moments': instances,\n",
    "                   'time_unit': DIDEMO_TIME_UNIT,\n",
    "                   'date': datetime.now().isoformat(),\n",
    "                   'git_hash': get_git_revision_hash(),\n",
    "                   'responsible': CREATOR,\n",
    "                  },\n",
    "                  fid)\n",
    "    print('\\tDumped file:', output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
