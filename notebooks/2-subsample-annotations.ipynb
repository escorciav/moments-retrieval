{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping new ground-truth\n",
    "\n",
    "Apparently, there is a notion of weak-aggreement that we can use to subsample \"outliers\" annotators.\n",
    "\n",
    "\n",
    "## 1. Weak agreement\n",
    "\n",
    "- 1st criterion: __3 or more__ annotators match either start or end.\n",
    "\n",
    "- 2nd criterion: The other endpoint is off at most by 1 \"clip\" (5 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: train\n",
      "Pctg unanimity 34.70\n",
      "Number of multimodal annotations 12\n",
      "Pctg of multimodal annotations 0.04\n",
      "Tied multimodal annotations 3\n",
      "\n",
      "Subset: val\n",
      "Pctg unanimity 34.14\n",
      "Number of multimodal annotations 1\n",
      "Pctg of multimodal annotations 0.02\n",
      "Tied multimodal annotations 0\n",
      "\n",
      "Subset: test\n",
      "Pctg unanimity 34.59\n",
      "Number of multimodal annotations 2\n",
      "Pctg of multimodal annotations 0.05\n",
      "Tied multimodal annotations 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "VALID_OFFSETS = [-1, 1]\n",
    "MIN_TIME, MAX_TIME = 0, 5\n",
    "NUM_ANNOTATORS_IN_WA = 3\n",
    "\n",
    "filename = '../data/raw/{}_data.json'\n",
    "new_gt = '../data/raw/{}_data_wwa.json'\n",
    "multimodal_agreement = {}\n",
    "unanimity = {}\n",
    "def validate_criterion2(endpoint_c1, endpoint, min_quota=3):\n",
    "    # pick most voted end-point\n",
    "    unique_endpoint_c1, votes_c1 = np.unique(endpoint_c1, return_counts=True)\n",
    "    ind_most_voted_endpoint_c1 = np.argmax(votes_c1)\n",
    "    ind_criterion1 = endpoint_c1 == unique_endpoint_c1[ind_most_voted_endpoint_c1]\n",
    "    picked_endpoint_c1 = unique_endpoint_c1[ind_most_voted_endpoint_c1]\n",
    "    \n",
    "    # check offset for the subset of annotators that satisfy first criterion\n",
    "    endpoint = np.sort(endpoint[ind_criterion1])\n",
    "    median_endpoint = endpoint[len(endpoint) // 2]\n",
    "    offset = np.abs(endpoint - median_endpoint)\n",
    "    ind_criterion2 = offset <= 1\n",
    "    if ind_criterion2.sum() >= min_quota:\n",
    "        return True, picked_endpoint_c1, endpoint[ind_criterion2]\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "def make_annotations(start_, end_):\n",
    "    if isinstance(start_, np.ndarray):\n",
    "        start_ = np.unique(start_)\n",
    "        end_ = np.repeat(end_, len(start_))\n",
    "    else:\n",
    "        end_ = np.unique(end_)\n",
    "        start_ = np.repeat(start_, len(end_))\n",
    "    annotations = np.empty((len(end_), 2), dtype=end_.dtype)\n",
    "    annotations[:, 0] = start_\n",
    "    annotations[:, -1] = end_\n",
    "    return annotations\n",
    "\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    with open(filename.format(subset), 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    new_data = []\n",
    "    unanimity[subset] = {'ids': []}\n",
    "    multimodal_agreement[subset] = {'ids': [], 'tied': []}\n",
    "    unanimity_ids = unanimity[subset]['ids']\n",
    "    multimodal_agreement_ids = multimodal_agreement[subset]['ids']\n",
    "    multimodal_agreement_tied = multimodal_agreement[subset]['tied']\n",
    "    print('Subset:', subset)\n",
    "    for query in data:\n",
    "        annotations = np.array(query['times'])\n",
    "        start, end = np.split(annotations, 2, axis=1)\n",
    "        unique_start_points, start_votes = np.unique(start, return_counts=True)\n",
    "        unique_end_points, end_votes = np.unique(end, return_counts=True)\n",
    "        start_matched = (start_votes >= NUM_ANNOTATORS_IN_WA).any()\n",
    "        end_matched = (end_votes >= NUM_ANNOTATORS_IN_WA).any()\n",
    "        one_endpoint_matched_wac1 = start_matched or end_matched\n",
    "        assert one_endpoint_matched_wac1\n",
    "        criterion2 = False\n",
    "        \n",
    "        # if all are equal, no purpose of testing offset\n",
    "        new_annotations, counts = np.unique(annotations, axis=0, return_counts=True)\n",
    "        if new_annotations.shape[0] == 1:\n",
    "            unanimity_ids.append(query['annotation_id'])\n",
    "            criterion2 = True\n",
    "            start_, end_ = None, None\n",
    "\n",
    "        # Check multiple modes\n",
    "        if start_matched and len(start_votes) > 1:\n",
    "            num_annot_in_wa = start_votes >= NUM_ANNOTATORS_IN_WA\n",
    "            possible_tied = np.where((start_votes - start_votes.max()) == 0)[0]\n",
    "            if num_annot_in_wa.sum() >= 2:\n",
    "                multimodal_agreement_ids.append(query['annotation_id'])\n",
    "            if len(possible_tied) > 1:\n",
    "                multimodal_agreement_tied.append(query['annotation_id'])\n",
    "        elif end_matched and len(end_votes) > 1:\n",
    "            num_annot_in_wa = end_votes >= NUM_ANNOTATORS_IN_WA\n",
    "            possible_tied = np.where((end_votes - end_votes.max()) == 0)[0]\n",
    "            if num_annot_in_wa.sum() >= 2:\n",
    "                multimodal_agreement_ids.append(query['annotation_id'])\n",
    "            if len(possible_tied) > 1:\n",
    "                multimodal_agreement_tied.append(query['annotation_id'])\n",
    "\n",
    "        # 2nd criterion\n",
    "        if start_matched and not criterion2:\n",
    "            criterion2, start_, end_ = validate_criterion2(start, end, NUM_ANNOTATORS_IN_WA)\n",
    "        if end_matched and not criterion2:\n",
    "            criterion2, end_, start_ = validate_criterion2(end, start, NUM_ANNOTATORS_IN_WA)\n",
    "\n",
    "        if criterion2 and start_ is not None:\n",
    "            new_annotations = make_annotations(start_, end_)\n",
    "        elif not criterion2:\n",
    "            raise\n",
    "            \n",
    "        new_data.append(dict(query))\n",
    "        new_data[-1]['times'] = new_annotations.tolist()\n",
    "\n",
    "    with open(new_gt.format(subset), 'w') as f:\n",
    "        json.dump(new_data, f)\n",
    "\n",
    "    print(f'Pctg unanimity {100 * len(unanimity_ids) / len(data):.2f}')\n",
    "    print('Number of multimodal annotations', len(multimodal_agreement_ids))\n",
    "    print(f'Pctg of multimodal annotations {100 * len(multimodal_agreement_ids) / len(data):.2f}')\n",
    "    print(f'Tied multimodal annotations', len(multimodal_agreement_tied))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matching criterion\n",
    "\n",
    "We opt for using IOU=1.0 and merging multiple annotations, if any, with the $\\max := \\text{OR}$ operation.\n",
    "\n",
    "In other words, a prediciton is valid iff it matches one of the annotations with IOU=1.0.\n",
    "\n",
    "Why IOU=1.0?\n",
    "1. the annotations are coarse, thus they are already relax.\n",
    "    \n",
    "2. we are dealing with ambiguity by considering multiple annotations with the weak-agreement enforced during the collection of the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Flow\n",
      "R@(1, 5, 10)=[1.0, 1.0, 1.0];\n",
      "R@(1, 5, 10),0.75=[1.0, 1.0, 1.0];\n",
      "R@(1, 5, 10),didemo=[0.6607809002735638, 0.7689629445411589, 0.8067644864461577];\n",
      "mIOU=0.9055;\n",
      "mRank=0.00;\n",
      "Elapsed time: 65.75612425804138\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from evaluation import RetrievalEvaluation\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file_corpus = '../data/interim/mcn/corpus_test_flow.hdf5'\n",
    "file_queries = '../data/interim/mcn/queries_test_flow.hdf5'\n",
    "file_annotations = '../data/raw/test_data_wwa.json'\n",
    "judge = RetrievalEvaluation(file_corpus, file_annotations, (1, 5, 10))\n",
    "\n",
    "with h5py.File(file_queries, 'r') as fid:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for sample_key, h5ds in fid.items():\n",
    "        query_id = int(sample_key)\n",
    "        # Copy vector\n",
    "        video_index = judge.gt_queries[query_id]['video_index']\n",
    "        segment_indices = judge.gt_queries[query_id]['segment_indices']\n",
    "        corpus_indices = judge.corpus.repo_to_ind(video_index, segment_indices)\n",
    "        for sampled_index in corpus_indices:\n",
    "            query_vector = judge.corpus.features[sampled_index, :]\n",
    "            judge.eval_single_vector(query_vector, query_id)\n",
    "            # assert and erase mess\n",
    "            for i, k in enumerate(judge.k):\n",
    "                assert judge.hit_k[i][-1]\n",
    "                judge.hit_k[i].pop()\n",
    "                judge.hit_k_iou[i].pop()\n",
    "            assert judge.rank[-1] == 0\n",
    "            judge.rank.pop()\n",
    "            judge.miou.pop()\n",
    "            judge.avg_rank.pop()\n",
    "            \n",
    "        np.random.shuffle(corpus_indices)\n",
    "        sampled_index = corpus_indices[0]\n",
    "        query_vector = judge.corpus.features[sampled_index, :]\n",
    "        judge.eval_single_vector(query_vector, query_id)\n",
    "    performace = judge.eval(full=True)\n",
    "    print('Test Flow')\n",
    "    print('R@{0:}={2:};\\nR@{0:},{1:}={3:};\\nR@{0:},didemo={4:};\\n'\n",
    "          'mIOU={5:.4f};\\nmRank={6:.2f};'\n",
    "          .format(judge.k, judge.iou_threshold,\n",
    "                  *performace))\n",
    "    print('Elapsed time:', time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplifying weak agreement\n",
    "Finding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: val\n",
      "730 0.17464114832535885\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "VALID_OFFSETS = [-1, 1]\n",
    "MIN_TIME, MAX_TIME = 0, 5\n",
    "NUM_ANNOTATORS_IN_WA = 3\n",
    "\n",
    "filename = '../data/raw/{}_data.json'\n",
    "multimodal_agreement = {}\n",
    "def validate_criterion2(endpoint_c1, endpoint, min_quota=3):\n",
    "    # pick most voted end-point\n",
    "    unique_endpoint_c1, votes_c1 = np.unique(endpoint_c1, return_counts=True)\n",
    "    ind_most_voted_endpoint_c1 = np.argmax(votes_c1)\n",
    "    ind_criterion1 = endpoint_c1 == unique_endpoint_c1[ind_most_voted_endpoint_c1]\n",
    "    picked_endpoint_c1 = unique_endpoint_c1[ind_most_voted_endpoint_c1]\n",
    "    \n",
    "    # check offset for the subset of annotators that satisfy first criterion\n",
    "    endpoint = np.sort(endpoint[ind_criterion1])\n",
    "    median_endpoint = endpoint[len(endpoint) // 2]\n",
    "    offset = np.abs(endpoint - median_endpoint)\n",
    "    ind_criterion2 = offset <= 1\n",
    "    if ind_criterion2.sum() >= min_quota:\n",
    "        return True, picked_endpoint_c1, endpoint[ind_criterion2]\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "def make_annotations(start_, end_):\n",
    "    if isinstance(start_, np.ndarray):\n",
    "        start_ = np.unique(start_)\n",
    "        end_ = np.repeat(end_, len(start_))\n",
    "    else:\n",
    "        end_ = np.unique(end_)\n",
    "        start_ = np.repeat(start_, len(end_))\n",
    "    annotations = np.empty((len(end_), 2), dtype=end_.dtype)\n",
    "    annotations[:, 0] = start_\n",
    "    annotations[:, -1] = end_\n",
    "    return annotations\n",
    "\n",
    "for subset in ['val']:\n",
    "    with open(filename.format(subset), 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # 1406 -> showcase trouble of choosing start-point first\n",
    "    # 71193 -> recover example of weak-agreement in slides\n",
    "    random.seed(1406)\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    trigger = False\n",
    "    multimodal_agreement[subset] = {'ids': [], 'tied': []}\n",
    "    multimodal_agreement_ids = multimodal_agreement[subset]['ids']\n",
    "    multimodal_agreement_tied = multimodal_agreement[subset]['tied']\n",
    "    print('Subset:', subset)\n",
    "    start_end_tied = 0\n",
    "    for query in data:\n",
    "        annotations = np.array(query['times'])\n",
    "        start, end = np.split(annotations, 2, axis=1)\n",
    "        unique_start_points, start_votes = np.unique(start, return_counts=True)\n",
    "        unique_end_points, end_votes = np.unique(end, return_counts=True)\n",
    "        start_matched = (start_votes >= NUM_ANNOTATORS_IN_WA).any()\n",
    "        end_matched = (end_votes >= NUM_ANNOTATORS_IN_WA).any()\n",
    "        one_endpoint_matched_wac1 = start_matched or end_matched\n",
    "        assert one_endpoint_matched_wac1\n",
    "        criterion2 = False\n",
    "        \n",
    "        # if all are equal, no purpose of testing offset\n",
    "        new_annotations, counts = np.unique(annotations, axis=0, return_counts=True)\n",
    "        if new_annotations.shape[0] == 1:\n",
    "            criterion2 = True\n",
    "            start_, end_ = None, None\n",
    "\n",
    "        # Check multiple modes\n",
    "        if start_matched and len(start_votes) > 1:\n",
    "            num_annot_in_wa = start_votes >= NUM_ANNOTATORS_IN_WA\n",
    "            possible_tied = np.where((start_votes - start_votes.max()) == 0)[0]\n",
    "            if num_annot_in_wa.sum() >= 2:\n",
    "                multimodal_agreement_ids.append(query['annotation_id'])\n",
    "            if len(possible_tied) > 1:\n",
    "                multimodal_agreement_tied.append(query['annotation_id'])\n",
    "        elif end_matched and len(end_votes) > 1:\n",
    "            num_annot_in_wa = end_votes >= NUM_ANNOTATORS_IN_WA\n",
    "            possible_tied = np.where((end_votes - end_votes.max()) == 0)[0]\n",
    "            if num_annot_in_wa.sum() >= 2:\n",
    "                multimodal_agreement_ids.append(query['annotation_id'])\n",
    "            if len(possible_tied) > 1:\n",
    "                multimodal_agreement_tied.append(query['annotation_id'])\n",
    "\n",
    "        # 2nd criterion\n",
    "        if (start_votes.max() == end_votes.max()) and not criterion2:\n",
    "            start_end_tied += 1\n",
    "        if start_matched and not criterion2:\n",
    "            criterion2, start_, end_ = validate_criterion2(start, end, NUM_ANNOTATORS_IN_WA)\n",
    "        if end_matched and not criterion2:\n",
    "            criterion2, end_, start_ = validate_criterion2(end, start, NUM_ANNOTATORS_IN_WA)\n",
    "\n",
    "        if criterion2 and start_ is not None:\n",
    "            new_annotations = make_annotations(start_, end_)\n",
    "            if len(new_annotations) > 1:\n",
    "                trigger = True\n",
    "        elif not criterion2:\n",
    "            raise\n",
    "        \n",
    "        if trigger:\n",
    "            print(query['video'])\n",
    "            print(query['description'])\n",
    "            print('Original annotations')\n",
    "            print(annotations)\n",
    "            print('New annotations')\n",
    "            print(new_annotations)\n",
    "            break\n",
    "#     print('Number of multimodal annotations', len(multimodal_agreement_ids))\n",
    "#     print(f'Pctg of multimodal annotations {100 * len(multimodal_agreement_ids) / len(data):.2f}')\n",
    "#     print(f'Tied multimodal annotations', len(multimodal_agreement_tied))\n",
    "    print(start_end_tied, start_end_tied/len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
