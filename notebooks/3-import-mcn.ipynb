{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from didemo import Didemo\n",
    "from model import MCN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class MCNDebug(MCN):\n",
    "    \"Debug MCN to add Caffe weights\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MCNDebug, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, padded_query, query_length, visual_pos,\n",
    "                visual_neg_intra=None, visual_neg_inter=None):\n",
    "        # Keep the same signature but does not use neg inputs\n",
    "        visual_pos, visual_neg_intra, visual_neg_inter = self._unpack_visual(\n",
    "            visual_pos, visual_neg_intra, visual_neg_inter)\n",
    "        v_embedding_neg_intra = None\n",
    "        v_embedding_neg_inter = None\n",
    "        B = len(padded_query)\n",
    "\n",
    "        v_embedding_pos = self.img_encoder(visual_pos)\n",
    "        if visual_neg_intra is not None:\n",
    "            v_embedding_neg_intra = self.img_encoder(visual_neg_intra)\n",
    "        if visual_neg_inter is not None:\n",
    "            v_embedding_neg_inter = self.img_encoder(visual_neg_inter)\n",
    "\n",
    "        packed_query = pack_padded_sequence(\n",
    "            padded_query, query_length, batch_first=True)\n",
    "        packed_output, _ = self.sentence_encoder(packed_query)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n",
    "                                        total_length=self.max_length)\n",
    "        # TODO: try max-pooling\n",
    "        last_output = output[range(B), query_length - 1, :]\n",
    "        l_embedding = self.lang_encoder(last_output)\n",
    "        return (l_embedding, v_embedding_pos, last_output, output)\n",
    "\n",
    "\n",
    "def load_mcn_weights_from_caffe(weights_h5, visual_size = 8194,\n",
    "                                text_dim = 300, max_length = 50):\n",
    "    with h5py.File(weights_h5) as f:\n",
    "        ported_weights = {}\n",
    "        mapping = {}\n",
    "        for k, v in f.items():\n",
    "            ported_weights[k] = v[:]\n",
    "            # print(k, v.shape)\n",
    "            if k == 'InnerProduct1_0':\n",
    "                mapping['img_encoder.0.weight'] = k\n",
    "            elif k == 'InnerProduct1_1':\n",
    "                mapping['img_encoder.0.bias'] = k\n",
    "            elif k == 'InnerProduct2_0':\n",
    "                mapping['img_encoder.2.weight'] = k\n",
    "            elif k == 'InnerProduct2_1':\n",
    "                mapping['img_encoder.2.bias'] = k\n",
    "            elif k == 'LSTM1_0':\n",
    "                dim = v.shape[0] // 4\n",
    "                v = np.concatenate([v[:2*dim, ...], v[3*dim:4*dim, ...], v[2*dim:3*dim, ...]], axis=0)\n",
    "                mapping['sentence_encoder.weight_ih_l0'] = k\n",
    "            elif k == 'LSTM1_2':\n",
    "                dim = v.shape[0] // 4\n",
    "                v = np.concatenate([v[:2*dim, ...], v[3*dim:4*dim, ...], v[2*dim:3*dim, ...]], axis=0)\n",
    "                mapping['sentence_encoder.weight_hh_l0'] = k\n",
    "            elif k == 'LSTM1_1':\n",
    "                dim = v.shape[0] // 4\n",
    "                v = np.concatenate([v[:2*dim, ...], v[3*dim:4*dim, ...], v[2*dim:3*dim, ...]], axis=0)\n",
    "                mapping['sentence_encoder.bias_hh_l0'] = k\n",
    "            elif k == 'embedding_text_0':\n",
    "                mapping['lang_encoder.weight'] = k\n",
    "            elif k == 'embedding_text_1':\n",
    "                mapping['lang_encoder.bias'] = k\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    net = MCN(visual_size=visual_size,\n",
    "               lang_size=text_dim,\n",
    "               max_length=max_length)\n",
    "    for name, parameter in net.named_parameters():\n",
    "        # print(name, parameter.shape)\n",
    "        if name == 'sentence_encoder.bias_hh_l0':\n",
    "            parameter.data.zero_()\n",
    "            continue\n",
    "        parameter.data = torch.from_numpy(ported_weights[mapping[name]])\n",
    "\n",
    "    return net\n",
    "\n",
    "def load_mcn_weights_from_caffe_debug(\n",
    "    weights_h5, visual_size = 8194, text_dim = 300, max_length = 50):\n",
    "    with h5py.File(weights_h5) as f:\n",
    "        ported_weights = {}\n",
    "        mapping = {}\n",
    "        for k, v in f.items():\n",
    "            # print(k, v.shape)\n",
    "            if k == 'InnerProduct1_0':\n",
    "                mapping['img_encoder.0.weight'] = k\n",
    "            elif k == 'InnerProduct1_1':\n",
    "                mapping['img_encoder.0.bias'] = k\n",
    "            elif k == 'InnerProduct2_0':\n",
    "                mapping['img_encoder.2.weight'] = k\n",
    "            elif k == 'InnerProduct2_1':\n",
    "                mapping['img_encoder.2.bias'] = k\n",
    "            elif k == 'LSTM1_0':\n",
    "                dim = v.shape[0] // 4\n",
    "                v = np.concatenate([v[:2*dim, ...], v[3*dim:4*dim, ...], v[2*dim:3*dim, ...]], axis=0)\n",
    "                mapping['sentence_encoder.weight_ih_l0'] = k\n",
    "            elif k == 'LSTM1_2':\n",
    "                dim = v.shape[0] // 4\n",
    "                v = np.concatenate([v[:2*dim, ...], v[3*dim:4*dim, ...], v[2*dim:3*dim, ...]], axis=0)\n",
    "                mapping['sentence_encoder.weight_hh_l0'] = k\n",
    "            elif k == 'LSTM1_1':\n",
    "                dim = v.shape[0] // 4\n",
    "                v = np.concatenate([v[:2*dim, ...], v[3*dim:4*dim, ...], v[2*dim:3*dim, ...]], axis=0)\n",
    "                # sentence_encoder.bias_hh_l0\n",
    "                # sentence_encoder.bias_ih_l0\n",
    "                mapping['sentence_encoder.bias_hh_l0'] = k\n",
    "            elif k == 'embedding_text_0':\n",
    "                mapping['lang_encoder.weight'] = k\n",
    "            elif k == 'embedding_text_1':\n",
    "                mapping['lang_encoder.bias'] = k\n",
    "            else:\n",
    "                raise\n",
    "            ported_weights[k] = v[:]\n",
    "\n",
    "    net = MCNDebug(visual_size=visual_size,\n",
    "               lang_size=text_dim,\n",
    "               max_length=max_length)\n",
    "    for name, parameter in net.named_parameters():\n",
    "        # print(name, parameter.shape)\n",
    "        if name == 'sentence_encoder.bias_ih_l0':\n",
    "            parameter.data.zero_()\n",
    "            continue\n",
    "        \n",
    "        parameter.data = torch.from_numpy(ported_weights[mapping[name]])\n",
    "\n",
    "    return net\n",
    "\n",
    "RAW_PATH = Path('data/raw')\n",
    "EVAL_BATCH_SIZE = 1\n",
    "rgb_feat_path = RAW_PATH / 'average_fc7.h5'\n",
    "flow_feat_path = RAW_PATH / 'average_global_flow.h5'\n",
    "val_list_path = RAW_PATH / 'val_data.json'\n",
    "rgb_cue = {'rgb': {'file': rgb_feat_path}}\n",
    "flow_cue = {'flow': {'file': flow_feat_path}}\n",
    "rgb_data = '../localizing-moments/results/sample_blobs_rgb_iccv_release_feature_process_context_recurrent_embedding_lfTrue_dv0.3_dl0.0_nlv2_nlllstm_no_embed_edl1000-100_edv500-100_pmFalse_losstriplet_lwInter0.2_iter_30000.caffemodel_val.hdf5'\n",
    "flow_data = '../localizing-moments/results/sample_blobs_flow_iccv_release_feature_process_context_recurrent_embedding_lfTrue_dv0.3_dl0.0_nlv2_nlllstm_no_embed_edl1000-100_edv500-100_pmFalse_losstriplet_lwInter0.2_iter_30000.caffemodel_val.hdf5'\n",
    "rgb_weights = '../localizing-moments/rgb-weights.hdf5'\n",
    "flow_weights = '../localizing-moments/flow-weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rgb = Didemo(val_list_path, cues=rgb_cue, test=True)\n",
    "val_flow = Didemo(val_list_path, cues=flow_cue, test=True)\n",
    "# val_dataset.collate_test_data\n",
    "keys_rgb = {v['annotation_id']: i\n",
    "            for i, v in enumerate(val_rgb.metadata)}\n",
    "keys_flow = {v['annotation_id']: i\n",
    "             for i, v in enumerate(val_flow.metadata)}\n",
    "assert keys_rgb == keys_flow\n",
    "feat_rgb = val_rgb[0]\n",
    "feat_flow = val_flow[0]\n",
    "text_dim = feat_rgb[0].shape[1]\n",
    "if val_rgb.eval == True:\n",
    "    text_dim = feat_rgb[0].shape[2]\n",
    "rgb_dim = feat_rgb[2]['rgb'].shape[0]\n",
    "flow_dim = feat_flow[2]['flow'].shape[0]\n",
    "max_length = feat_rgb[0].shape[0]\n",
    "rgb_setup = dict(visual_size=rgb_dim, lang_size=text_dim,\n",
    "                 max_length=max_length)\n",
    "flow_setup = dict(visual_size=flow_dim, lang_size=text_dim,\n",
    "                 max_length=max_length)\n",
    "net_rgb = load_mcn_weights_from_caffe_debug(rgb_weights, visual_size=rgb_dim)\n",
    "net_flow = load_mcn_weights_from_caffe_debug(flow_weights, visual_size=flow_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed the same data for cross-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = {}\n",
    "with h5py.File(flow_data) as f:\n",
    "    for k, v in f.items():\n",
    "        k = int(k)\n",
    "        original_data[k] = {}\n",
    "        for k2, v2 in v.items():\n",
    "            original_data[k][k2] = v2[:]\n",
    "\n",
    "net_rgb.eval()\n",
    "net_flow.eval()\n",
    "for i in original_data.keys():\n",
    "    index = keys_rgb[i]\n",
    "    # pre-processing\n",
    "    rst = val_flow.collate_test_data([val_flow[index]])\n",
    "    #\n",
    "    input_visual = rst[2]['flow'].detach().numpy()\n",
    "    input_visual_ = original_data[i]['Concat1']\n",
    "    np.testing.assert_almost_equal(input_visual, input_visual_)\n",
    "    #\n",
    "    text_data = rst[0].detach().numpy()\n",
    "    text_data = text_data.transpose((1, 0, 2))\n",
    "    text_data_ = original_data[i]['text_data']\n",
    "    # np.testing.assert_almost_equal(text_data, text_data_)\n",
    "    seq_len = rst[1][0].item()\n",
    "    np.testing.assert_almost_equal(text_data[0:seq_len, :, :],\n",
    "                                   text_data_[50-seq_len:, :, :])\n",
    "    # visual embedding\n",
    "    embedding_text, embedding_visual, encoded_sentence, lstm_output  = net_flow(*rst)\n",
    "    embedding_visual = embedding_visual.detach().numpy()\n",
    "    embedding_visual_ = original_data[i]['embedding_visual']\n",
    "    np.testing.assert_almost_equal(embedding_visual, embedding_visual_)\n",
    "    # lstm output\n",
    "    lstm_output = lstm_output.detach().numpy()\n",
    "    lstm_output = lstm_output[:, 0:seq_len, :]\n",
    "    lstm_output_ = original_data[i]['LSTM1'].transpose((1, 0, 2))\n",
    "    lstm_output_ = lstm_output_[:, 50-seq_len:, :]\n",
    "    # NOTE: reduce tolerance to 6 decimals due to mismatch of 0.02%\n",
    "    np.testing.assert_almost_equal(lstm_output, lstm_output_, 6)\n",
    "    # sentence encoded\n",
    "    encoded_sentence = encoded_sentence.detach().numpy()\n",
    "    encoded_sentence_ = original_data[i]['Reshape1']\n",
    "    np.testing.assert_almost_equal(encoded_sentence, encoded_sentence_)\n",
    "    # language embedding\n",
    "    embedding_text = embedding_text.detach().numpy()\n",
    "    embedding_text_ = original_data[i]['embedding_text']\n",
    "    np.testing.assert_almost_equal(embedding_text, embedding_text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[debugging] force reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/mnt/ilcompf9d1/user/escorcia/moments-retrieval/model.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[debugging] Check blob names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data[i].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
