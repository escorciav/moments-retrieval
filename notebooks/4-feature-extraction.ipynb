{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Preparing inputs for [video-utils](https://git.corp.adobe.com/escorcia/video-utils) tools\n",
    "\n",
    "- Create file with all the video names\n",
    "\n",
    "- Divide and conquer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!split -d -n l/7 ../data/interim/didemo/frame_extraction/all_videos.txt ../data/interim/didemo/frame_extraction/videos-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Double check commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 1 ../data/interim/didemo/frame_extraction/videos-*\n",
    "!wc -l ../data/interim/didemo/frame_extraction/videos-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Double check that `adobe/extract_frames.[sh/condor]` are pointitng to the appropriate folders\n",
    "\n",
    "3) Launch job `condor_submit adobe/extract_frames.condor`\n",
    "\n",
    "4) Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "dirname = 'data/interim/didemo/frame_extraction/'\n",
    "log_wildcard = os.path.join(dirname, '*_300h.log')\n",
    "csv_wildcard = os.path.join(dirname, '*_300h.csv')\n",
    "num_jobs = len(list(glob.glob(log_wildcard)))\n",
    "num_summary_files = len(list(glob.glob(csv_wildcard)))\n",
    "print(f'Completed jobs [{num_summary_files}/{num_jobs}]')\n",
    "!tail -n 2 $log_wildcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting DiDeMo frames at 5FPS and 320x240 took roughly an hour on ten machines with multiple cores\n",
    "\n",
    "5) Check if all videos were extracted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "dirname = 'data/interim/didemo/frame_extraction/'\n",
    "csv_wildcard = os.path.join(dirname, '*_300h.csv')\n",
    "df = []\n",
    "for i in glob.glob(csv_wildcard):\n",
    "    df.append(pd.read_csv(i, header=None))\n",
    "df = pd.concat(df, axis=0, ignore_index=True)\n",
    "print('Number of buggy videos', (df.loc[:, 1] == False).sum())\n",
    "df.loc[df[1] == False, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Apparently the video appears twice in the dataset with different extension. We double check that the video correspond to the same content, thus we ignore this error.\n",
    "\n",
    "6) Merge all tar-files:\n",
    "\n",
    "I used the file `adobe/merge_frames.sh` which should do somth close to this:\n",
    "\n",
    "```bash\n",
    "output_dir=/mnt/ssd/tmp/didemo_prep\n",
    "prefix=frames_300h\n",
    "\n",
    "set -x\n",
    "output_dir=$1\n",
    "prefix=$2\n",
    "if [ -d $output_dir ]; then rm -rf $output_dir; fi\n",
    "mkdir -p $output_dir/all &&\n",
    "for f in $(find ~/ -maxdepth 1 -name $prefix\"*\"); do\n",
    "  tar -xf $f -C $output_dir;\n",
    "done  &&\n",
    "for f in $(find $output_dir -maxdepth 1 -name $prefix\"*\"); do\n",
    "  mv $f/* $output_dir/all/;\n",
    "done  &&\n",
    "cd $output_dir  &&\n",
    "for f in $(find . -maxdepth 1 -name $prefix\"*\"); do\n",
    "  echo rmdir $f;\n",
    "done  &&\n",
    "mv all frames &&\n",
    "tar -cf ~/didemo_$prefix\".tar\" frames/ && cd ~ && echo rm -rf $output_dir\n",
    "```\n",
    "\n",
    "This took less than TBD mins (once it took 20mins but I changed the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Sanity checks\n",
    "\n",
    "__Note__: In case, all the videos were not dummped, it is important that you generate a text-file with the list of video to process.\n",
    "\n",
    "- Making sure that frames are not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "dirname = Path('/mnt/ssd/tmp/didemo/frames/frames/')\n",
    "for i in dirname.iterdir():\n",
    "    assert i.is_dir\n",
    "    assert len(os.listdir(i)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[note] small parentheses. Feel free to ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filename = 'data/interim/didemo/frame_extraction/all_videos.txt'\n",
    "df = pd.read_csv(filename, header=None)\n",
    "print('Count videos taking care of extensions')\n",
    "print(len(df), len(df[0].unique()), len(df) - len(df[0].unique()))\n",
    "print('Count videos after removing extension')\n",
    "df2 = df[0].apply(lambda x: os.path.splitext(x)[0])\n",
    "print(len(df2), len(df2.unique()), len(df2) - len(df2.unique()))\n",
    "# Fuck the same video appears with different extensions :S\n",
    "# TODO: check if video length\n",
    "# TODO: check video content manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[conclusion] @escorcia decided to move with the mass because this is considered as \"an engineering practice\" by the community. It would be nice to go deeper in this, but we don't have the bandwidth. We decided to replicate lisa's strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 152/VGG16\n",
    "\n",
    "The protocol is the same if you use the code in [feature_extraction](https://github.com/escorciav/feature-extraction)\n",
    "\n",
    "1) Generate list of videos or images\n",
    "\n",
    "a. In case, you did not have trouble extracting frames for all videos, you can use the same list\n",
    "\n",
    "```bash\n",
    "ln -s $(pwd)/data/interim/didemo/frame_extraction/videos-* data/interim/didemo/resnet_extraction/\n",
    "```\n",
    "\n",
    "b. If you are running other jobs, you will find these commands handy\n",
    "\n",
    "_Note_: take care with the __rm__\n",
    "\n",
    "```bash\n",
    "rm ../data/interim/didemo/resnet_extraction/videos-0*.csv\n",
    "wc -l ../data/interim/didemo/resnet_extraction/videos-0*\n",
    "split -d -n l/2 ../data/interim/didemo/resnet_extraction/videos-all ../data/interim/didemo/resnet_extraction/videos-\n",
    "```\n",
    "\n",
    "TODO: sample test of code\n",
    "\n",
    "2) Double check that `adobe/extract_resnet.[sh/condor]` are pointitng to the appropriate folders\n",
    "\n",
    "3) Launch job `condor_submit adobe/extract_resnet.condor`\n",
    "\n",
    "4) Monitor\n",
    "\n",
    "- Check nodes\n",
    "\n",
    "!grep \"slot\" log/[prefix]\n",
    "\n",
    "- Check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 ../data/interim/didemo/vgg_extraction/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing number of frames processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = !grep \"images\" ../data/interim/didemo/vgg_extraction/*.log\n",
    "num_images = sum([int(i.split()[-2]) for i in a])\n",
    "print('Number of processed images:', num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Merging HDF5s\n",
    "\n",
    "- In case that you dumped with the default option in `pack_features.py`.\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Group)\n",
    "|    feature_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "filename = '/mnt/ssd/tmp/vgg16_320x240_224x224_5fps.h5'\n",
    "persistent_file = '../data/interim/didemo/vgg16/320x240_224x224_5fps.h5'\n",
    "wildcard = '/mnt/ilcompf9d1/user/escorcia/vgg16-*.h5'\n",
    "\n",
    "with h5py.File(filename, 'w') as fid:\n",
    "    for file_i in glob.glob(wildcard):\n",
    "        with h5py.File(file_i, 'r') as fr:\n",
    "            for _, source_group in fr.items():\n",
    "                fr.copy(source_group, fid)\n",
    "![ ! -d $(dirname $persistent_file) ] && mkdir -p $(dirname $persistent_file)\n",
    "!cp $filename $persistent_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case that you dumped with the original MCN format\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "filename = '/mnt/ssd/tmp/320x240_5fps.h5'\n",
    "persistent_file = '../data/interim/didemo/inceptionv4/320x240_5fps.h5'\n",
    "wildcard = '/mnt/ilcompf9d1/user/escorcia/inceptionv4-*.h5'\n",
    "\n",
    "with h5py.File(filename, 'w') as fid:\n",
    "    for file_i in glob.glob(wildcard):\n",
    "        with h5py.File(file_i, 'r') as fr:\n",
    "            for video_id, video_object in fr.items():\n",
    "                fr.copy(video_object, fid, name=video_id)\n",
    "![ ! -d $(dirname $persistent_file) ] && mkdir -p $(dirname $persistent_file)\n",
    "!cp $filename $persistent_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse pool per chunk/clip \n",
    "\n",
    "- In case that you keep with the default file structure in `pack_features.py`.\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Group)\n",
    "|    feature_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FPS = 5\n",
    "CHUNK_SIZE = 5  # seconds\n",
    "NUM_CHUNKS = 6\n",
    "filename = '/mnt/ssd/tmp/320x240_max.h5'\n",
    "dense_file = f'/mnt/ssd/tmp/320x240_{FPS}fps.h5.h5'\n",
    "persistent_file = '../data/interim/didemo/inceptionv4/320x240_max.h5'\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File(filename, 'w') as fw, h5py.File(dense_file, 'r') as fr:\n",
    "    for video, group_src in fr.items():\n",
    "        # ensure compatibility with MCN code\n",
    "        # MCN code was written for a hdf5 per feature type\n",
    "        # TODO: deprecate this\n",
    "        assert len(list(group_src.keys())) == 1\n",
    "        for name, v in group_src.items():            \n",
    "            feat = v[:]\n",
    "            pooled_feat = np.zeros((NUM_CHUNKS, feat.shape[1]), dtype=feat.dtype)\n",
    "            for i in range(NUM_CHUNKS):\n",
    "                start_ind = i * CHUNK_SIZE * FPS\n",
    "                end_ind = min(start_ind + CHUNK_SIZE * FPS, len(feat))\n",
    "                if start_ind >= len(feat):\n",
    "                    break\n",
    "                # pooled_feat[i, :] = feat[start_ind:end_ind, :].mean(axis=0)\n",
    "                # pooled_feat[i, :] = feat[start_ind:end_ind, :].max(axis=0)\n",
    "                # center = (start_ind + end_ind) // 2\n",
    "                # pooled_feat[i, :] = feat[center, :].max(axis=0)\n",
    "            fw.create_dataset(video, data=pooled_feat, chunks=True)\n",
    "!mv $filename $persistent_file\n",
    "# commented as it may be harmful\n",
    "# !rm $dense_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case that you dumped with the original MCN format\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FPS = 5\n",
    "CHUNK_SIZE = 5  # seconds\n",
    "NUM_CHUNKS = 6\n",
    "POOL = 'max'\n",
    "filename = f'/home/escorciav/datasets/didemo/features/resnet152_{POOL}.h5'\n",
    "dense_file = f'/home/escorciav/datasets/didemo/features/resnet_{FPS}fps_320x240.h5'\n",
    "persistent_file = None  # '../data/interim/didemo/inceptionv4/'\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File(filename, 'w') as fw, h5py.File(dense_file, 'r') as fr:\n",
    "    for video, v in fr.items():\n",
    "        # ensure compatibility with MCN code\n",
    "        # MCN code was written for a hdf5 per feature type\n",
    "        feat = v[:]\n",
    "        pooled_feat = np.zeros((NUM_CHUNKS, feat.shape[1]), dtype=feat.dtype)\n",
    "        for i in range(NUM_CHUNKS):\n",
    "            start_ind = i * CHUNK_SIZE * FPS\n",
    "            end_ind = min(start_ind + CHUNK_SIZE * FPS, len(feat))\n",
    "            if start_ind >= len(feat):\n",
    "                break\n",
    "            if POOL == 'mean':\n",
    "                pooled_feat[i, :] = feat[start_ind:end_ind, :].mean(axis=0)\n",
    "            elif POOL == 'max':\n",
    "                pooled_feat[i, :] = feat[start_ind:end_ind, :].max(axis=0)\n",
    "            elif POOL == 'center':\n",
    "                center = (start_ind + end_ind) // 2\n",
    "                pooled_feat[i, :] = feat[center, :].max(axis=0)\n",
    "            else:\n",
    "                raise\n",
    "        fw.create_dataset(video, data=pooled_feat, chunks=True)\n",
    "if persistent_file is not None:\n",
    "    !mv $filename $persistent_file\n",
    "# commented as it may be harmful\n",
    "# !rm $dense_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varied length videos\n",
    "\n",
    "In case that you dumped with the original MCN format __AND__ your are in an untrimmed video scenario where all videos don't have the same length.\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FPS = 5\n",
    "CHUNK_SIZE = 3  # seconds\n",
    "POOL = 'max'\n",
    "filename = f'/home/escorciav/datasets/charades/features/resnet152_{POOL}.h5'\n",
    "dense_file = f'/home/escorciav/datasets/charades/features/resnet152-imagenet_{FPS}fps_320x240.hdf5'\n",
    "persistent_file = None  # '../data/interim/didemo/inceptionv4/'\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# simple check to ensure that we review the logic when number of frames are float :)\n",
    "assert isinstance(FPS * CHUNK_SIZE, int)\n",
    "with h5py.File(filename, 'w') as fw, h5py.File(dense_file, 'r') as fr:\n",
    "    for video, v in fr.items():\n",
    "        # ensure compatibility with MCN code\n",
    "        # MCN code was written for a hdf5 per feature type\n",
    "        feat = v[:]\n",
    "        temporal_video_representation = []\n",
    "        num_frames = len(feat)\n",
    "        start_ind, end_ind, chunk_ind = 0, 1, 0\n",
    "        while end_ind < num_frames:\n",
    "            start_ind = chunk_ind * CHUNK_SIZE * FPS\n",
    "            end_ind = min(start_ind + CHUNK_SIZE * FPS, num_frames)\n",
    "\n",
    "            # compute representation of a chunk\n",
    "            # You could replace this with fancy arch spanning more\n",
    "            # frames like I3D, NLN, etc.\n",
    "            if POOL == 'mean':\n",
    "                chunk_feat = feat[start_ind:end_ind, :].mean(axis=0)\n",
    "            elif POOL == 'max':\n",
    "                chunk_feat = feat[start_ind:end_ind, :].max(axis=0)\n",
    "            elif POOL == 'center':\n",
    "                center = (start_ind + end_ind) // 2\n",
    "                chunk_feat = feat[center, :].max(axis=0)\n",
    "            else:\n",
    "                raise\n",
    "            temporal_video_representation.append(chunk_feat)\n",
    "            chunk_ind += 1\n",
    "        \n",
    "        fw.create_dataset(video,\n",
    "                          data=np.row_stack(temporal_video_representation),\n",
    "                          chunks=True)\n",
    "if persistent_file is not None:\n",
    "    !mv $filename $persistent_file\n",
    "# commented as it may be harmful\n",
    "# !rm $dense_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[debug] Making sure that copy is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file1 = '/mnt/ilcompf9d1/user/escorcia/resnet152-0.h5'\n",
    "with h5py.File(filename, 'r') as f1, h5py.File(file1, 'r') as f2:\n",
    "    f1_keys = sorted(list(f1.keys()))\n",
    "    f2_keys = sorted(list(f2.keys()))\n",
    "    assert f1_keys == f2_keys\n",
    "    for i in f1.keys():\n",
    "        f1_i_keys = sorted(list(f1[i].keys()))\n",
    "        f2_i_keys = sorted(list(f2[i].keys()))\n",
    "        assert f1_i_keys == f2_i_keys\n",
    "        for j, v1 in f1[i].items():\n",
    "            x1 = v1[:]\n",
    "            x2 = f2[i][j][:]\n",
    "            np.testing.assert_array_almost_equal(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = '/mnt/ilcompf9d1/user/escorcia/resnet152-0.h5'\n",
    "fid = h5py.File(filename, 'r')\n",
    "for k, v in fid.items():\n",
    "    print(k, v['resnet152'][:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [legacy] VGG feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging because features where different to those provided by MCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "file1 = '/mnt/ilcompf9d1/user/escorcia/localizing-moments/data/average_fc7.h5'\n",
    "careful = {}\n",
    "with h5py.File(file1, 'r') as f1:\n",
    "    for k, v in f1.items():\n",
    "        feat = v[:]\n",
    "        if feat.sum() != 0:\n",
    "            print(k)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing extracted features with public features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = '/mnt/ilcompf9d1/user/escorcia/tmp_didemo/fc7_subsample10_stock_44971549@N06_8077235126_bc346362b8.mov.h5'\n",
    "filename = '/mnt/ilcompf9d1/user/escorcia/tmp_didemo/fc7_subsample10_stock_10015567@N08_3655084291_d8b58466fa.mov.h5'\n",
    "!ls -la $filename\n",
    "fid = h5py.File(filename)\n",
    "feat = fid['features'][:]\n",
    "print(feat.shape)\n",
    "print(feat.max(), feat.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file1 = '/mnt/ilcompf9d1/user/escorcia/localizing-moments/data/average_fc7.h5'\n",
    "file2 = '/mnt/ilcompf9d1/user/escorcia/tmp_didemo/average_fc7.h5'\n",
    "\n",
    "with h5py.File(file1, 'r') as f1, h5py.File(file2, 'r') as f2:\n",
    "    video_id = list(f2.keys())[0]\n",
    "    feat2 = f2[video_id][:]\n",
    "    feat1 = f1[video_id][:]\n",
    "    np.testing.assert_array_almost_equal(feat1, feat2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
