{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Preparing inputs for [video-utils](https://git.corp.adobe.com/escorcia/video-utils) tools\n",
    "\n",
    "- Create file with all the video names\n",
    "\n",
    "- Divide and conquer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!split -d -n l/7 ../data/interim/didemo/frame_extraction/all_videos.txt ../data/interim/didemo/frame_extraction/videos-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Double check commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 1 ../data/interim/didemo/frame_extraction/videos-*\n",
    "!wc -l ../data/interim/didemo/frame_extraction/videos-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Double check that `adobe/extract_frames.[sh/condor]` are pointitng to the appropriate folders\n",
    "\n",
    "3) Launch job `condor_submit adobe/extract_frames.condor`\n",
    "\n",
    "4) Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "dirname = 'data/interim/didemo/frame_extraction/'\n",
    "log_wildcard = os.path.join(dirname, '*_300h.log')\n",
    "csv_wildcard = os.path.join(dirname, '*_300h.csv')\n",
    "num_jobs = len(list(glob.glob(log_wildcard)))\n",
    "num_summary_files = len(list(glob.glob(csv_wildcard)))\n",
    "print(f'Completed jobs [{num_summary_files}/{num_jobs}]')\n",
    "!tail -n 2 $log_wildcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting DiDeMo frames at 5FPS and 320x240 took roughly an hour on ten machines with multiple cores\n",
    "\n",
    "5) Check if all videos were extracted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "dirname = 'data/interim/didemo/frame_extraction/'\n",
    "csv_wildcard = os.path.join(dirname, '*_300h.csv')\n",
    "df = []\n",
    "for i in glob.glob(csv_wildcard):\n",
    "    df.append(pd.read_csv(i, header=None))\n",
    "df = pd.concat(df, axis=0, ignore_index=True)\n",
    "print('Number of buggy videos', (df.loc[:, 1] == False).sum())\n",
    "df.loc[df[1] == False, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Apparently the video appears twice in the dataset with different extension. We double check that the video correspond to the same content, thus we ignore this error.\n",
    "\n",
    "6) Merge all tar-files:\n",
    "\n",
    "I used the file `adobe/merge_frames.sh` which should do somth close to this:\n",
    "\n",
    "```bash\n",
    "output_dir=/mnt/ssd/tmp/didemo_prep\n",
    "prefix=frames_300h\n",
    "\n",
    "set -x\n",
    "output_dir=$1\n",
    "prefix=$2\n",
    "if [ -d $output_dir ]; then rm -rf $output_dir; fi\n",
    "mkdir -p $output_dir/all &&\n",
    "for f in $(find ~/ -maxdepth 1 -name $prefix\"*\"); do\n",
    "  tar -xf $f -C $output_dir;\n",
    "done  &&\n",
    "for f in $(find $output_dir -maxdepth 1 -name $prefix\"*\"); do\n",
    "  mv $f/* $output_dir/all/;\n",
    "done  &&\n",
    "cd $output_dir  &&\n",
    "for f in $(find . -maxdepth 1 -name $prefix\"*\"); do\n",
    "  echo rmdir $f;\n",
    "done  &&\n",
    "mv all frames &&\n",
    "tar -cf ~/didemo_$prefix\".tar\" frames/ && cd ~ && echo rm -rf $output_dir\n",
    "```\n",
    "\n",
    "This took less than TBD mins (once it took 20mins but I changed the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Sanity checks\n",
    "\n",
    "__Note__: In case, all the videos were not dummped, it is important that you generate a text-file with the list of video to process.\n",
    "\n",
    "- Making sure that frames are not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "dirname = Path('/mnt/ssd/tmp/didemo/frames/frames/')\n",
    "for i in dirname.iterdir():\n",
    "    assert i.is_dir\n",
    "    assert len(os.listdir(i)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[note] small parentheses. Feel free to ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "filename = 'data/interim/didemo/frame_extraction/all_videos.txt'\n",
    "df = pd.read_csv(filename, header=None)\n",
    "print('Count videos taking care of extensions')\n",
    "print(len(df), len(df[0].unique()), len(df) - len(df[0].unique()))\n",
    "print('Count videos after removing extension')\n",
    "df2 = df[0].apply(lambda x: os.path.splitext(x)[0])\n",
    "print(len(df2), len(df2.unique()), len(df2) - len(df2.unique()))\n",
    "# Fuck the same video appears with different extensions :S\n",
    "# TODO: check if video length\n",
    "# TODO: check video content manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[conclusion] @escorcia decided to move with the mass because this is considered as \"an engineering practice\" by the community. It would be nice to go deeper in this, but we don't have the bandwidth. We decided to replicate lisa's strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 152/VGG16\n",
    "\n",
    "The protocol is the same if you use the code in [feature_extraction](https://github.com/escorciav/feature-extraction)\n",
    "\n",
    "TODO: push to adobe-gitcorp and update link\n",
    "\n",
    "1) Generate list of videos or images\n",
    "\n",
    "a. In case, you did not have trouble extracting frames for all videos, you can use the same list\n",
    "\n",
    "```bash\n",
    "ln -s $(pwd)/data/interim/didemo/frame_extraction/videos-* data/interim/didemo/resnet_extraction/\n",
    "```\n",
    "\n",
    "b. If you are running other jobs, you will find these commands handy\n",
    "\n",
    "_Note_: take care with the __rm__\n",
    "\n",
    "```bash\n",
    "rm ../data/interim/didemo/resnet_extraction/videos-0*.csv\n",
    "wc -l ../data/interim/didemo/resnet_extraction/videos-0*\n",
    "split -d -n l/2 ../data/interim/didemo/resnet_extraction/videos-all ../data/interim/didemo/resnet_extraction/videos-\n",
    "```\n",
    "\n",
    "TODO: sample test of code\n",
    "\n",
    "2) Double check that `adobe/extract_resnet.[sh/condor]` are pointitng to the appropriate folders\n",
    "\n",
    "3) Launch job `condor_submit adobe/extract_resnet.condor`\n",
    "\n",
    "4) Monitor\n",
    "\n",
    "- Check nodes\n",
    "\n",
    "!grep \"slot\" log/[prefix]\n",
    "\n",
    "- Check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 ../data/interim/didemo/vgg_extraction/*.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing number of frames processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = !grep \"images\" ../data/interim/didemo/vgg_extraction/*.log\n",
    "num_images = sum([int(i.split()[-2]) for i in a])\n",
    "print('Number of processed images:', num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Merging HDF5s\n",
    "\n",
    "- In case that you dumped with the default option in `pack_features.py`.\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Group)\n",
    "|    feature_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot create regular file '../data/interim/didemo/vgg16/320x240_224x224_5fps.h5': No such file or directory\n",
      "CPU times: user 10.3 s, sys: 19.1 s, total: 29.3 s\n",
      "Wall time: 10min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "filename = '/mnt/ssd/tmp/vgg16_320x240_224x224_5fps.h5'\n",
    "persistent_file = '../data/interim/didemo/vgg16/320x240_224x224_5fps.h5'\n",
    "wildcard = '/mnt/ilcompf9d1/user/escorcia/vgg16-*.h5'\n",
    "\n",
    "with h5py.File(filename, 'w') as fid:\n",
    "    for file_i in glob.glob(wildcard):\n",
    "        with h5py.File(file_i, 'r') as fr:\n",
    "            for _, source_group in fr.items():\n",
    "                fr.copy(source_group, fid)\n",
    "![ ! -d $(dirname $persistent_file) ] && mkdir -p $(dirname $persistent_file)\n",
    "!cp $filename $persistent_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case that you dumped with the original MCN format\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 27.7 s, total: 39.8 s\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "filename = '/mnt/ssd/tmp/320x240_5fps.h5'\n",
    "persistent_file = '../data/interim/didemo/inceptionv4/320x240_5fps.h5'\n",
    "wildcard = '/mnt/ilcompf9d1/user/escorcia/inceptionv4-*.h5'\n",
    "\n",
    "with h5py.File(filename, 'w') as fid:\n",
    "    for file_i in glob.glob(wildcard):\n",
    "        with h5py.File(file_i, 'r') as fr:\n",
    "            for video_id, video_object in fr.items():\n",
    "                fr.copy(video_object, fid, name=video_id)\n",
    "![ ! -d $(dirname $persistent_file) ] && mkdir -p $(dirname $persistent_file)\n",
    "!cp $filename $persistent_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Coarse average pool per chunk\n",
    "\n",
    "- In case that you keep with the default file structure in `pack_features.py`.\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Group)\n",
    "|    feature_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 5s, sys: 12.4 s, total: 4min 17s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FPS = 5\n",
    "CHUNK_SIZE = 5  # seconds\n",
    "NUM_CHUNKS = 6\n",
    "filename = '/mnt/ssd/tmp/320x240_max.h5'\n",
    "dense_file = f'/mnt/ssd/tmp/320x240_{FPS}fps.h5.h5'\n",
    "persistent_file = '../data/interim/didemo/inceptionv4/320x240_max.h5'\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File(filename, 'w') as fw, h5py.File(dense_file, 'r') as fr:\n",
    "    for video, group_src in fr.items():\n",
    "        # ensure compatibility with MCN code\n",
    "        # MCN code was written for a hdf5 per feature type\n",
    "        # TODO: deprecate this\n",
    "        assert len(list(group_src.keys())) == 1\n",
    "        for name, v in group_src.items():            \n",
    "            feat = v[:]\n",
    "            pooled_feat = np.zeros((NUM_CHUNKS, feat.shape[1]), dtype=feat.dtype)\n",
    "            for i in range(NUM_CHUNKS):\n",
    "                start_ind = i * CHUNK_SIZE * FPS\n",
    "                end_ind = min(start_ind + CHUNK_SIZE * FPS, len(feat))\n",
    "                if start_ind >= len(feat):\n",
    "                    break\n",
    "                # pooled_feat[i, :] = feat[start_ind:end_ind, :].mean(axis=0)\n",
    "                # pooled_feat[i, :] = feat[start_ind:end_ind, :].max(axis=0)\n",
    "                # center = (start_ind + end_ind) // 2\n",
    "                # pooled_feat[i, :] = feat[center, :].max(axis=0)\n",
    "            fw.create_dataset(video, data=pooled_feat, chunks=True)\n",
    "!mv $filename $persistent_file\n",
    "# commented as it may be harmful\n",
    "# !rm $dense_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case that you dumped with the original MCN format\n",
    "\n",
    "```\n",
    "/\n",
    "|--video_id (Dataset)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 4.62 s, total: 1min 58s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "FPS = 5\n",
    "CHUNK_SIZE = 5  # seconds\n",
    "NUM_CHUNKS = 6\n",
    "filename = '/mnt/ssd/tmp/320x240_mean.h5'\n",
    "dense_file = f'/mnt/ssd/tmp/320x240_{FPS}fps.h5'\n",
    "persistent_file = '../data/interim/didemo/inceptionv4/'\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File(filename, 'w') as fw, h5py.File(dense_file, 'r') as fr:\n",
    "    for video, v in fr.items():\n",
    "        # ensure compatibility with MCN code\n",
    "        # MCN code was written for a hdf5 per feature type\n",
    "        feat = v[:]\n",
    "        pooled_feat = np.zeros((NUM_CHUNKS, feat.shape[1]), dtype=feat.dtype)\n",
    "        for i in range(NUM_CHUNKS):\n",
    "            start_ind = i * CHUNK_SIZE * FPS\n",
    "            end_ind = min(start_ind + CHUNK_SIZE * FPS, len(feat))\n",
    "            if start_ind >= len(feat):\n",
    "                break\n",
    "            # pooled_feat[i, :] = feat[start_ind:end_ind, :].mean(axis=0)\n",
    "            # pooled_feat[i, :] = feat[start_ind:end_ind, :].max(axis=0)\n",
    "            # center = (start_ind + end_ind) // 2\n",
    "            # pooled_feat[i, :] = feat[center, :].max(axis=0)\n",
    "        fw.create_dataset(video, data=pooled_feat, chunks=True)\n",
    "!mv $filename $persistent_file\n",
    "# commented as it may be harmful\n",
    "# !rm $dense_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[debug] Making sure that copy is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file1 = '/mnt/ilcompf9d1/user/escorcia/resnet152-0.h5'\n",
    "with h5py.File(filename, 'r') as f1, h5py.File(file1, 'r') as f2:\n",
    "    f1_keys = sorted(list(f1.keys()))\n",
    "    f2_keys = sorted(list(f2.keys()))\n",
    "    assert f1_keys == f2_keys\n",
    "    for i in f1.keys():\n",
    "        f1_i_keys = sorted(list(f1[i].keys()))\n",
    "        f2_i_keys = sorted(list(f2[i].keys()))\n",
    "        assert f1_i_keys == f2_i_keys\n",
    "        for j, v1 in f1[i].items():\n",
    "            x1 = v1[:]\n",
    "            x2 = f2[i][j][:]\n",
    "            np.testing.assert_array_almost_equal(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12572907@N00_2920156258_34d144bf1e.avi 0.36933506\n",
      "12644997@N04_4936603071_9a12b8cc5d.mp4 0.30536875\n",
      "14284621@N06_3944006339_85416993a7.mov 0.3719792\n",
      "16483298@N00_4331364236_f8e7cc40e8.avi 0.42610207\n",
      "16483298@N00_4893184599_197570445d.mp4 0.3697194\n",
      "26292851@N04_4497646769_c867658047.mp4 0.4500945\n",
      "50072196@N00_8243844603_e9a8bf01fe.mov 0.39579678\n",
      "51727341@N00_4913494887_25ba94c153.mp4 0.45705098\n",
      "56424258@N03_8926842688_91c14724ee. 0.44437027\n",
      "67211380@N00_2867483360_731aa9cab3.avi 0.33976325\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "filename = '/mnt/ilcompf9d1/user/escorcia/resnet152-0.h5'\n",
    "fid = h5py.File(filename, 'r')\n",
    "for k, v in fid.items():\n",
    "    print(k, v['resnet152'][:].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [legacy] VGG feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging because features where different to those provided by MCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015567@N08_3655084291_d8b58466fa.mov\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file1 = '/mnt/ilcompf9d1/user/escorcia/localizing-moments/data/average_fc7.h5'\n",
    "careful = {}\n",
    "with h5py.File(file1, 'r') as f1:\n",
    "    for k, v in f1.items():\n",
    "        feat = v[:]\n",
    "        if feat.sum() != 0:\n",
    "            print(k)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing extracted features with public features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 escorcia 5001 4933094 Jul 11 01:52 /mnt/ilcompf9d1/user/escorcia/tmp_didemo/fc7_subsample10_stock_10015567@N08_3655084291_d8b58466fa.mov.h5\n",
      "(150, 4096)\n",
      "8.976858139038086 0.0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "filename = '/mnt/ilcompf9d1/user/escorcia/tmp_didemo/fc7_subsample10_stock_44971549@N06_8077235126_bc346362b8.mov.h5'\n",
    "filename = '/mnt/ilcompf9d1/user/escorcia/tmp_didemo/fc7_subsample10_stock_10015567@N08_3655084291_d8b58466fa.mov.h5'\n",
    "!ls -la $filename\n",
    "fid = h5py.File(filename)\n",
    "feat = fid['features'][:]\n",
    "print(feat.shape)\n",
    "print(feat.max(), feat.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\n(mismatch 76.025390625%)\n x: array([[0.080325, 0.      , 0.618881, ..., 0.057373, 0.698398, 1.784408],\n       [0.781676, 0.014886, 0.20887 , ..., 0.      , 0.32405 , 0.338488],\n       [1.154795, 0.534264, 0.447821, ..., 0.      , 0.521238, 0.16278 ],...\n y: array([[0.907936, 0.09756 , 0.31482 , ..., 0.022781, 0.594315, 0.259575],\n       [1.023138, 0.467035, 0.362644, ..., 0.15047 , 1.121736, 0.08118 ],\n       [0.813921, 0.272168, 0.244465, ..., 0.245621, 1.05744 , 0.053195],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3739318f3c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfeat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/install/bin/miniconda3/envs/scientific/lib/python3.6/site-packages/numpy/testing/nose_tools/utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[0;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    961\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[1;32m    962\u001b[0m              \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m              precision=decimal)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/install/bin/miniconda3/envs/scientific/lib/python3.6/site-packages/numpy/testing/nose_tools/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    777\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\n(mismatch 76.025390625%)\n x: array([[0.080325, 0.      , 0.618881, ..., 0.057373, 0.698398, 1.784408],\n       [0.781676, 0.014886, 0.20887 , ..., 0.      , 0.32405 , 0.338488],\n       [1.154795, 0.534264, 0.447821, ..., 0.      , 0.521238, 0.16278 ],...\n y: array([[0.907936, 0.09756 , 0.31482 , ..., 0.022781, 0.594315, 0.259575],\n       [1.023138, 0.467035, 0.362644, ..., 0.15047 , 1.121736, 0.08118 ],\n       [0.813921, 0.272168, 0.244465, ..., 0.245621, 1.05744 , 0.053195],..."
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file1 = '/mnt/ilcompf9d1/user/escorcia/localizing-moments/data/average_fc7.h5'\n",
    "file2 = '/mnt/ilcompf9d1/user/escorcia/tmp_didemo/average_fc7.h5'\n",
    "\n",
    "with h5py.File(file1, 'r') as f1, h5py.File(file2, 'r') as f2:\n",
    "    video_id = list(f2.keys())[0]\n",
    "    feat2 = f2[video_id][:]\n",
    "    feat1 = f1[video_id][:]\n",
    "    np.testing.assert_array_almost_equal(feat1, feat2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
