{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for feature extraction\n",
    "\n",
    "Follow this to extract features and frames.\n",
    "\n",
    "__Warning__ this notebook is a tutorial/guide, __do NOT__ run all the cells without reading them.\n",
    "\n",
    "_Note_ this is a more detailed walkthrough for the notebook 4 which must be deprecated at some point.\n",
    "\n",
    "## 1. Create project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'didemo'\n",
    "root_dir = '/home/escorciav/mnt/marla-scratch/moments-retrieval'\n",
    "\n",
    "trial_dirname = f'{root_dir}/{trial_name}'\n",
    "!mkdir -p $trial_dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a text file with all the video names\n",
    "\n",
    "We need a single text-file with all the video names that need to process.\n",
    "\n",
    "__Note__: In a nutshell, read annotations and gather all the videos of the dataset into a file.\n",
    "\n",
    "- Do not over do it.\n",
    "- Keep it simple e.g. no hyper-security, only asssert things that boil up into errors.\n",
    "- Create as many cell as different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. DiDeMo\n",
    "\n",
    "Require annotation files provided [here](https://github.com/LisaAnne/LocalizingMoments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_fmt = 'data/raw/{}_data.json'\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "videos = []\n",
    "for i in ['train', 'val', 'test']:\n",
    "    annotation_file = annotation_fmt.format(i)\n",
    "    with open(annotation_file, 'r') as fr:\n",
    "        for instance in json.load(fr):\n",
    "            videos.append(instance['video'])\n",
    "videos = np.unique(videos).tolist()\n",
    "# randomized to add entropy\n",
    "random.shuffle(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make video list for frame extraction\n",
    "\n",
    "- Make list of videos used for `video-utils/tools/batch_dump_frames.py`\n",
    "\n",
    "- Get list of missing videos\n",
    "\n",
    "__Notes__\n",
    "\n",
    "1. Use hard symlink to link directly to the file and not to the path.\n",
    "\n",
    "2. The cell below is an example of the two (sometimes three) outputs of this step:\n",
    "\n",
    "    1. `missing-videos.txt`\n",
    "    2. `videos.txt`\n",
    "    3. make sure that all the videos are placed into a single folder.\n",
    "    \n",
    "### 3.a. DiDeMo\n",
    "\n",
    "Require to download DiDeMo videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "videos_root = Path('/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/videos/')\n",
    "with open(f'{trial_dirname}/missing-videos.txt', 'x') as fw_m, open(f'{trial_dirname}/videos.txt', 'x') as fw:\n",
    "    for video_name in videos:\n",
    "        if (videos_root / video_name).exists():\n",
    "            fw.write(f'{video_name}\\n')\n",
    "        else:\n",
    "            fw_m.write(f'{video_name}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[monitor]__ check what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of videos missing')\n",
    "!wc -l '{trial_dirname}/missing-videos.txt'\n",
    "print('Number of videos')\n",
    "!wc -l '{trial_dirname}/videos.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[debug]__\n",
    "\n",
    "Date: September 23 2018\n",
    "\n",
    "Apparently there were 31 videos \"missing\". However, we had the videos in the filesystem but those have a different name in the annotation file. We fixed that in the following way:\n",
    "\n",
    "1. rename old `videos` folder as `videos_original`.\n",
    "\n",
    "2. create a new folder called video.\n",
    "\n",
    "3. hardlink all the videos inside `videos_original` with their name in the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "videos_root = Path('/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/videos')\n",
    "videos_root_dirname = videos_root.parent\n",
    "!mv $videos_root $videos_root_dirname\"/videos_original\"\n",
    "!mkdir -p $videos_root\n",
    "for video_name in videos:\n",
    "    if (videos_root_dirname / 'videos_original' / video_name).exists():\n",
    "        !ln $videos_root_dirname/'videos_original'/$video_name $videos_root/$video_name\n",
    "    else:\n",
    "        pattern = videos_root_dirname / 'videos_original' / (Path(video_name).name[:-1] + '*')\n",
    "        files = glob.glob(str(pattern))\n",
    "        if len(files) != 1:\n",
    "            # even trickier videos :S\n",
    "            files = [i for i in files if Path(i).suffix != '.mp4']\n",
    "        video_name_tricky = Path(files[0]).name\n",
    "        !ln $videos_root_dirname/'videos_original'/$video_name_tricky $videos_root/$video_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [exceptional] 3?. Example for tricky cases\n",
    "\n",
    "Sample code when videos are placed in multiple folders and we start for a unique video identifier without extension.\n",
    "\n",
    "Given that we need to do this cruff fast:\n",
    "\n",
    "- We create symlinks for videos of interest. Basically, a DIY-pre-processing replacement of a missing database connector in `video-utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "videos_root = '/home/escorciav/mnt/marla-scratch/datasets/mantis/videos'\n",
    "video_dirnames = [\n",
    "    f'{videos_root}/visual_favorable/',\n",
    "    f'{videos_root}/visual_unfavorable/'\n",
    "]\n",
    "\n",
    "map_youtubeid2videos = {}\n",
    "for dirname in video_dirnames:\n",
    "    for basename in os.listdir(dirname):\n",
    "        basename_noext = os.path.splitext(basename)[0]\n",
    "        map_youtubeid2videos.update(\n",
    "            [(basename_noext, {'basename': basename, 'root': dirname})]\n",
    "        )\n",
    "\n",
    "trial_video_dirname = f'{videos_root}/{trial_name}'\n",
    "!mkdir -p $trial_video_dirname\n",
    "with open(video_list_file, 'r') as fr, open(f'{trial_dirname}/missing-videos.txt', 'x') as fw:\n",
    "    for video_id in fr:\n",
    "        video_id = video_id.strip()\n",
    "        file_info = map_youtubeid2videos.get(video_id)\n",
    "        if file_info:\n",
    "            # make hard symlink\n",
    "            basename, dirname = file_info['basename'], file_info['root']\n",
    "            !ln $dirname/$basename $trial_video_dirname/$basename\n",
    "        else:\n",
    "            fw.write(f'{video_id}\\n')\n",
    "            \n",
    "# video list is equivalent to listing the video folder of the trial ;)\n",
    "video_list = os.listdir(trial_video_dirname)\n",
    "# randomized to increase entropy\n",
    "random.shuffle(video_list)\n",
    "with open(f'{trial_dirname}/videos.txt', 'x') as fw:\n",
    "    for i in video_list:\n",
    "        fw.write(f'{i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dumping frames\n",
    "\n",
    "__Note__ from this point on going there is no more dataset specific code. Reason, we need to track the changes in code with git and parameters in log files. In that way, we can go back if required.\n",
    "\n",
    "1) Launch frame extraction with something similar to this:\n",
    "\n",
    "```bash\n",
    "python tools/batch_dump_frames.py \\\n",
    "  -i /mnt/scratch/moments-retrieval/didemo/videos.txt \\\n",
    "  -r /mnt/ssdscratch/datasets/didemo/videos/ \\\n",
    "  -o /mnt/ssdscratch/datasets/didemo/frames \\\n",
    "  -s /mnt/scratch/moments-retrieval/didemo/frame-dump.csv \\\n",
    "  -n 1 --verbose 5 --log INFO &> /mnt/scratch/moments-retrieval/didemo/frame-dump.log\n",
    "```\n",
    "\n",
    "For details about the meaning of the arguments run this:\n",
    "\n",
    "```bash\n",
    "python tools/batch_dump_frames.py -h\n",
    "```\n",
    "    \n",
    "__Runtime notes__:\n",
    "\n",
    "- currently this command is typed manually over tmux in a machine with 48 cores. Thus, don't forget to update the paths before executing it ðŸ˜‰\n",
    "\n",
    "- How to fix `ModuleNotFoundError`?\n",
    "\n",
    "    Devote a minute or two to understand the problem.\n",
    "\n",
    "    In this case, there are two elegant solutions for this:\n",
    "\n",
    "    a) Append the root folder of video-utils to the environment variable `PYTHONPATH`.\n",
    "\n",
    "    ```bash\n",
    "    cd [video-utils-folder]\n",
    "    export PYTHONPATH=$PWD\n",
    "    ```\n",
    "\n",
    "    b) Send a pull-request to video-utils to make a package out of it ;)\n",
    "\n",
    "    This is not to say that editing `sys.path` is incorrect. Indeed, that's the best solution to get rid of the problem hiding it under the rug.\n",
    "\n",
    "2) count tricky videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"False\" $trial_dirname/frame-dump.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. monitor\n",
    "\n",
    "__TLDR__: check info to monitor progress\n",
    "\n",
    "Don't forget to update the variable dirname before executing it ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dirname = Path('/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/frames/')\n",
    "print(len([x for x in dirname.iterdir() if x.is_dir()]))\n",
    "!wc -l $trial_dirname/videos.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. debug\n",
    "\n",
    "__TLDR__: check info below if you smell something fishy during frame extraction.\n",
    "\n",
    "small detour due to bug in ffmpeg with multiple threads\n",
    "\n",
    "1) pick a small subset of 100 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 100 '{trial_dirname}/videos.txt' > '{trial_dirname}/videos-debug.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensure my unix knowledge is not rusty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l '{trial_dirname}/videos-debug.txt'\n",
    "!head '{trial_dirname}/videos-debug.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) launch with single thread\n",
    "\n",
    "```\n",
    "python tools/batch_dump_frames.py \\\n",
    "    -i /mnt/scratch/datasets/mantis/trial-03/videos-debug.txt \\\n",
    "    -r /mnt/scratch/datasets/mantis/videos/trial-03/ \\\n",
    "    -o /mnt/ssdscratch/datasets/mantis/trial-03-debug-1 \\\n",
    "    -s /mnt/scratch/datasets/mantis/trial-03/frame-dump_video-debug_single-thread.csv \\\n",
    "    -n 1 --verbose 5 --log DEBUG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) launch with multiple threads\n",
    "\n",
    "```\n",
    "python tools/batch_dump_frames.py \\\n",
    "    -i /mnt/scratch/datasets/mantis/trial-03/videos-debug.txt \\\n",
    "    -r /mnt/scratch/datasets/mantis/videos/trial-03/ \\\n",
    "    -o /mnt/ssdscratch/datasets/mantis/trial-03-debug-2 \\\n",
    "    -s /mnt/scratch/datasets/mantis/trial-03/frame-dump_video-debug_multi-thread.csv \\\n",
    "    -n -1 --verbose 5 --log DEBUG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) compare number of frames per video over multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "dirname1 = Path('/home/escorciav/mnt/marla-ssdscratch/datasets/mantis/trial-03-debug-single-thread-1/')\n",
    "dirname2 = Path('/home/escorciav/mnt/marla-ssdscratch/datasets/mantis/trial-03-debug-single-thread-2/')\n",
    "for k in os.listdir(dirname1):\n",
    "    assert len(os.listdir(dirname1 / k)) == len(os.listdir(dirname2 / k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature extraction\n",
    "\n",
    "1) Dump list of videos that were successfully extracted\n",
    "\n",
    "__Note__: ignore the cell below if you were able to extract frames for all the videos successfully. Only perform the following command:\n",
    "\n",
    "```bash\n",
    "cp videos.txt features.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "dirname = Path(f'{trial_dirname}/frames')\n",
    "video_list = os.listdir(dirname)\n",
    "# randomized to increase entropy\n",
    "random.shuffle(video_list)\n",
    "with open(f'{trial_dirname}/feature.txt', 'x') as fw:\n",
    "    for i in os.listdir(dirname):\n",
    "        fw.write(f'{i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Split list above among GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l '{trial_dirname}/feature.txt'\n",
    "!split -n l/4 '{trial_dirname}/feature.txt' '{trial_dirname}/feature.txt.part'\n",
    "# next line is important because sometime ppl use split by bytes instead of lines ;)\n",
    "!tail -n 2 '{trial_dirname}/feature.txt.part'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Command for feature extraction\n",
    "\n",
    "```\n",
    "python pthv_models.py \\\n",
    "  -j 11 -b 512 -if --reduce \\\n",
    "  --arch resnet152 --layer-index -2 -h5dn resnet152 \\\n",
    "  -r /mnt/ssdscratch/datasets/didemo/frames \\\n",
    "  -f /mnt/scratch/moments-retrieval/didemo/feature.txt.partaa \\\n",
    "  -o /mnt/ssdscratch/datasets/didemo/features-partaa &> /mnt/scratch/moments-retrieval/didemo/feature-extraction.log.partaa \\\n",
    "```\n",
    "\n",
    "__remainders__:\n",
    "\n",
    "- Don't forget to set the `--resize` optional argument if you did not resize frames with ffmpeg\n",
    "\n",
    "__Runtime notes__:\n",
    "\n",
    "- Currently, this command is typed manually with tmux in a single machine with 4 GPUs.\n",
    "\n",
    "- Don't forget to update the paths before executing it ðŸ˜‰\n",
    "\n",
    "- Don't forget to set `CUDA_VISIBLE_DEVICES`. The current version hard-coded the device to 0.\n",
    "\n",
    "    ```bash\n",
    "    export CUDA_VISIBLE_DEVICES=1\n",
    "    ```\n",
    "    \n",
    "    Executing the above line will trick pytorch to believe that the gpu ID 1 is the device 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Pack HDF5s\n",
    "\n",
    "__Warning__: Make sure batch size during this step is the same as in step 3.\n",
    "\n",
    "```\n",
    "python pack_features.py -h5dn resnet152 -b 512 \\\n",
    "  -d /mnt/ssdscratch/datasets/didemo/features-partaa \\\n",
    "  -i /mnt/scratch/moments-retrieval/didemo/feature.txt.partaa-img.csv \\\n",
    "  -o /mnt/ssdscratch/datasets/didemo/features-partaa.hdf5 &> /mnt/scratch/moments-retrieval/didemo/packed-features.log.partaa\n",
    "```\n",
    "\n",
    "_Note_: It's normal that the log file is empty or with useless warnings. That would mean that you did not use the latest version (or appropriate?) branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Merge HDF5s\n",
    "\n",
    "```bash\n",
    "python merge_hdf5.py \\\n",
    "  --filename /mnt/ssdscratch/datasets/didemo/resnet152_5fps_320x240.h5 \\\n",
    "  --files /mnt/ssdscratch/datasets/didemo/features-parta*.hdf5 &> /mnt/scratch/moments-retrieval/didemo/merge-features.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[monitor]__ check that everything looks OK.\n",
    "\n",
    "[credits](https://support.hdfgroup.org/HDF5/Tutor/cmdtoolview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of videos')\n",
    "!h5ls '/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/resnet_5fps_320x240.h5' | wc -l\n",
    "print('Few examples')\n",
    "!h5ls '/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/resnet_5fps_320x240.h5' | head -n 2\n",
    "# To inspect the file\n",
    "!h5dump -H -A 0 '/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/resnet_5fps_320x240.h5' | head\n",
    "# To inspect a given group\n",
    "# print('Content of a given group')\n",
    "# !h5dump -g \"/--8xIYGTgEQ\" -H -A 0 '{dirname}/resnet_5fps_320x240.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[debug]__ \n",
    "\n",
    "TLDR: making sure the merge function did not make a mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "file1 = '/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/resnet_5fps_320x240.h5'\n",
    "files = glob.glob('/home/escorciav/mnt/marla-ssdscratch/datasets/didemo/features-parta*.hdf5')\n",
    "with h5py.File(file1) as f1:\n",
    "    videos = list(f1.keys())\n",
    "    random.shuffle(videos)\n",
    "    video_name = videos[0]\n",
    "    feat1 = f1[video_name][:]\n",
    "for i in files:\n",
    "    with h5py.File(i) as f2:\n",
    "        if video_name in f2:\n",
    "            feat2 = f2[video_name]\n",
    "            np.testing.assert_array_almost_equal(feat1, feat2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
