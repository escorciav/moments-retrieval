{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map tags to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tags: 2839\n",
      "Num clean tags: 2981\n",
      "CPU times: user 3min 15s, sys: 5.99 s, total: 3min 21s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load clean (scrapped) tags\n",
    "import csv\n",
    "filename = '../data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "# Lemmatizer to deal with plurals\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# Extract mapping tags 2 image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "\n",
    "# rows = []\n",
    "images = []\n",
    "tag2image = {}\n",
    "image_tags = []\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue \n",
    "            \n",
    "            counter = 0\n",
    "            image_url = f'{row[IMAGE_URL]}'\n",
    "            images.append(image_url)\n",
    "            image_tags.append([])\n",
    "            for tag in row[USER_TAGS].split(','):                    \n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag not in clean_tags:\n",
    "                    continue\n",
    "                \n",
    "                counter += 1\n",
    "                if lemmatized_tag in tag2image:\n",
    "                    tag2image[lemmatized_tag].append(len(images) - 1)\n",
    "                else:\n",
    "                    tag2image[lemmatized_tag] = [len(images) - 1]\n",
    "                image_tags[-1].append(tag)\n",
    "\n",
    "                if counter == TOPK:\n",
    "                    break\n",
    "print('Num tags:', len(tag2image))\n",
    "print('Num clean tags:', len(clean_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/interim/yfcc100m/tag_to_images-0.json', 'w') as fid:\n",
    "    json.dump({'images': images, 'image_tags': image_tags, 'tag2image': tag2image}, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check relevance of top-k tags in YFCC100M\n",
    "\n",
    "Compare raw top-k tags vs relevant* top-k tags for about 100 random images.\n",
    "\n",
    "- This generates the data from the webpage `yfcc100m_original_filtered_v2`\n",
    "\n",
    "* relevant means used in [this project](http://deep-tagging.cs.washington.edu/imagenet_correspondence.html)\n",
    "\n",
    "~~goal: visualize 100 random images with top-5 tags vs filtered top-5 tags~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 1.23 s, total: 58.7 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "filename = 'data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "# Those CSV are too big and we need these two lines\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import random\n",
    "\n",
    "SEED = 13029\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "SUBSET = 180 # None\n",
    "\n",
    "rows = []\n",
    "image_tag_mapping = {}\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            \n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "            if SUBSET is not None:\n",
    "                if random.random() > SUBSET / 10000000:\n",
    "                    continue  \n",
    "            \n",
    "            counter, counter2 = 0, 0\n",
    "            image_url = f'{row[IMAGE_URL]}'\n",
    "            topk_original_tags, topk_filtered_tags = [], []\n",
    "            for tag in row[USER_TAGS].split(','):\n",
    "                if counter < TOPK:\n",
    "                    counter += 1\n",
    "                    topk_original_tags.append(tag)\n",
    "                    \n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag in clean_tags:\n",
    "                    counter2 += 1\n",
    "                    topk_filtered_tags.append(lemmatized_tag)\n",
    "                    \n",
    "                if counter2 == TOPK:\n",
    "                    break\n",
    "            image_tag_mapping[image_url] = {'original': topk_original_tags,\n",
    "                                            'filtered': topk_filtered_tags}\n",
    "print('Num images:', len(image_tag_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/interim/yfcc100m/top5_tags_100.json', 'w') as fid:\n",
    "    json.dump(image_tag_mapping, fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
