{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images from Adobe repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take random row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import random\n",
    "\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "SUBSET = 100\n",
    "\n",
    "# rows = []\n",
    "images = []\n",
    "tag2image = {}\n",
    "image_tags = []\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for j, row in enumerate(reader):\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "            if SUBSET is not None:\n",
    "                if random.random() > SUBSET / 10000000:\n",
    "                    continue \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data at adobe is: `/mnt/ilcompf2d1/data/yfcc100m/`\n",
    "\n",
    "- find the image in the adobe data\n",
    "\n",
    "- show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 /mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0\n"
     ]
    }
   ],
   "source": [
    "!wc -l $i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494130\n",
      "['3948397176', '33938182@N04', 'Lucy+Takakura', '2009-09-21 13:11:55.0', '1253727701', 'Panasonic+DMC-TZ1', '%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%98%E3%83%B3%E3%83%AA%E3%83%BC%E3%82%BA%E3%82%B9%E3%82%BF%E3%83%87%E3%82%A3', 'shot+by+Lumix+DMC-TZ1', 'lumixdmc-tz1', '', '', '', '', 'http://www.flickr.com/photos/33938182@N04/3948397176/', 'http://farm3.staticflickr.com/2643/3948397176_22a2a41211.jpg', 'Attribution-ShareAlike License', 'http://creativecommons.org/licenses/by-sa/2.0/', '2643', '3', '22a2a41211', '2d609ea749', 'jpg', '0']\n"
     ]
    }
   ],
   "source": [
    "print(j)\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10000000 - 1) // 10000\n",
    "494130 // 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4130"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "494130 % 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/mnt/ilcompf2d1/data/yfcc100m/image/0-049.tar'\n",
    "import tarfile\n",
    "\n",
    "reader = tarfile.open(filename)\n",
    "fid = reader.extractfile('0-049/0494130.jpg')\n",
    "assert fid is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(fid)\n",
    "# dump image\n",
    "# img.save('hola.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YFCC100M $\\cap$ DiDeMo\n",
    "\n",
    "__Note__: You can also find the following cell into `scripts/` due to the long time they take too executed (> 30mins).\n",
    "\n",
    "## 1. Get subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num evaluation instances: 4180\n",
      "Spanned instances 4156\n",
      "NOUNs are underrepresented when appear less than 76\n",
      "Total NOUNs in train 3521\n",
      "NOUNs Under-represented 3358\n",
      "NOUNs Unseen during training 167\n",
      "Num descriptions with Under&Unseen NOUNs 2119\n",
      "Num descriptions with Well NOUNs 3814\n",
      "Pctg to impact 0.5069\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def create_vocabulary(underrepresented_threshold=10):\n",
    "    \"Create vocabulary of underrepresented NOUNs to augment\"\n",
    "\n",
    "    # load stats about nouns in DIDEMO\n",
    "    NOUNS_FILE = '../data/interim/didemo/nouns_to_video.json'\n",
    "    with open(NOUNS_FILE, 'r') as fid:\n",
    "        stats = json.load(fid)\n",
    "        for k, v in stats['nouns_per_subset'].items():\n",
    "            stats['nouns_per_subset'][k] = set(v)\n",
    "    \n",
    "    # load annotations in val-set\n",
    "    VAL_FILE = '../data/raw/val_data.json'\n",
    "    with open(VAL_FILE, 'r') as fid:\n",
    "        data = json.load(fid)\n",
    "    num_val_instances = len(data)\n",
    "    spanned_annotations = []\n",
    "    for k, v in stats['annotations_per_subset']['val'].items():\n",
    "        spanned_annotations.extend(v)\n",
    "    spanned_annotations = np.unique(spanned_annotations)\n",
    "    \n",
    "    nouns_vocab = set()\n",
    "    # TODO add val_ids to prioritize verification step\n",
    "    nouns_and_metadata = {}\n",
    "    num_nouns_train = len(stats['counts_per_subset']['train'])\n",
    "    num_nouns_underrepresented = 0\n",
    "    val_ids_toimpact = []\n",
    "    val_ids_represented = []\n",
    "    for k, v in stats['counts_per_subset']['train'].items():\n",
    "        if v <= underrepresented_threshold:\n",
    "            num_nouns_underrepresented += 1\n",
    "            if k in stats['counts_per_subset']['val']:\n",
    "                val_ids_toimpact.extend(\n",
    "                    stats['annotations_per_subset']['val'][k])\n",
    "                nouns_vocab.add(k)\n",
    "        else:\n",
    "            if k in stats['counts_per_subset']['val']:\n",
    "                val_ids_represented.extend(\n",
    "                    stats['annotations_per_subset']['val'][k])\n",
    "\n",
    "    num_samples_well_represented = len(np.unique(val_ids_represented))\n",
    "    nouns_only_val = (stats['nouns_per_subset']['val'] -\n",
    "                      stats['nouns_per_subset']['train'])\n",
    "    num_nouns_notrepresented = len(nouns_only_val)\n",
    "    for k in nouns_only_val:\n",
    "        val_ids_toimpact.extend(\n",
    "            stats['annotations_per_subset']['val'][k])\n",
    "        nouns_vocab.add(k)\n",
    "    num_samples_to_impact = len(np.unique(val_ids_toimpact))\n",
    "\n",
    "    print('Num evaluation instances:', num_val_instances)\n",
    "    print('Spanned instances', len(spanned_annotations))\n",
    "    print('NOUNs are underrepresented when appear less than', underrepresented_threshold + 1)\n",
    "    print('Total NOUNs in train', num_nouns_train)\n",
    "    print('NOUNs Under-represented', num_nouns_underrepresented)\n",
    "    print('NOUNs Unseen during training', num_nouns_notrepresented)\n",
    "    print('Num descriptions with Under&Unseen NOUNs', num_samples_to_impact)\n",
    "    print('Num descriptions with Well NOUNs', num_samples_well_represented)\n",
    "    print('Pctg to impact', f'{num_samples_to_impact / num_val_instances:.4f}')\n",
    "    return nouns_vocab\n",
    "\n",
    "nouns = create_vocabulary(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "IMAGES_PER_TAR = 10000\n",
    "TRAINABLE_THRESHOLD = 1000\n",
    "UNDERREPRESENTED_THRESHOLD = 75\n",
    "\n",
    "# Load clean (scrapped) tags\n",
    "import csv\n",
    "filename = '../data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "# load a set of interesting NOUNs from DiDeMo\n",
    "didemo_nouns = create_vocabulary(UNDERREPRESENTED_THRESHOLD)\n",
    "\n",
    "# Lemmatizer to deal with plurals\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# Extract mapping tags 2 image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "tag2image = {}\n",
    "images = []\n",
    "image_tags_topk_flickr = []\n",
    "image_tags_flickr = []\n",
    "image_tags_topk = []\n",
    "image_tags = []\n",
    "image_urls = []\n",
    "for filename in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-*'):\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        file_index = filename.split('-')[1]\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line_no, row in enumerate(reader):\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "\n",
    "            counter, amen = 0, False\n",
    "            tags_topk_flickr = []\n",
    "            tags_topk = []\n",
    "            for tag in row[USER_TAGS].split(','):\n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag not in clean_tags:\n",
    "                    continue\n",
    "                if lemmatized_tag not in didemo_nouns:\n",
    "                    continue\n",
    "                # after this point the image will be considered, thus we\n",
    "                # can add it to tag2image dict\n",
    "                counter += 1\n",
    "                tags_topk.append(lemmatized_tag)\n",
    "                tags_topk_flickr.append(tag)\n",
    "                amen = True\n",
    "                if lemmatized_tag in tag2image:\n",
    "                    tag2image[lemmatized_tag].append(len(images))\n",
    "                else:\n",
    "                    tag2image[lemmatized_tag] = [len(images)]\n",
    "                if counter == TOPK:\n",
    "                    # according to previous Bryan's project top-5 tags in YFCC100M\n",
    "                    # have purity >= 60%\n",
    "                    break\n",
    "            tags = [i for i in row[USER_TAGS].split(',')\n",
    "                    if lemmatizer(i, u'NOUN')[0] in clean_tags]\n",
    "\n",
    "            # I don't need this but just in case\n",
    "            if not amen:\n",
    "                continue\n",
    "            tar_index = line_no // IMAGES_PER_TAR\n",
    "            img_index = line_no % IMAGES_PER_TAR\n",
    "            image_loc = f'{file_index}-{tar_index:03d}/{tar_index:03d}{img_index:04d}.jpg'\n",
    "            image_url = row[IMAGE_URL]\n",
    "            images.append(image_loc)\n",
    "            image_urls.append(image_url)\n",
    "            image_tags_topk_flickr.append(';'.join(tags_topk_flickr))\n",
    "            image_tags_flickr.append(row[USER_TAGS].replace(',', ';'))\n",
    "            image_tags_topk.append(';'.join(tags_topk))\n",
    "            image_tags.append(';'.join(tags))\n",
    "\n",
    "print('Num clean tags:', len(clean_tags))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "NOUNS_FILE = '../data/interim/didemo/nouns_to_video.json'\n",
    "with open(NOUNS_FILE, 'r') as fid:\n",
    "    didemo_stats = json.load(fid)\n",
    "\n",
    "trainable_tags = 0\n",
    "val_instances = []\n",
    "for k, v in tag2image.items():\n",
    "    if len(v) > TRAINABLE_THRESHOLD:\n",
    "        trainable_tags += 1\n",
    "    val_instances.extend(didemo_stats[k])\n",
    "val_instances = np.unique(val_instances)\n",
    "print(f'Top-{TOPK} gives {len(tag2image)} '\n",
    "      f'and {trainable_tags} >= {TRAINABLE_THRESHOLD} images')\n",
    "print(f'Number of val instances with those NOUNs {len(val_instances)}')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([images, image_urls,\n",
    "                   image_tags_topk, image_tags,\n",
    "                   image_tags_topk_flickr, image_tags_flickr]).T\n",
    "df.columns = ['adobe_cil', 'url', 'topk_tags', 'tags', 'topk_tags_yfcc100m', 'tags_yfcc100m']\n",
    "basename = (f'yfcc100m_images_intersect_didemo_under-and-not-nouns-leq'\n",
    "            f'-{UNDERREPRESENTED_THRESHOLD}_topk-{TOPK}')\n",
    "df.to_csv(f'{basename}.csv')\n",
    "with open(f'{basename}.json', 'w') as fid:\n",
    "    json.dump(tag2image, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unpack and sub-sample of YFCC100M relevant for DiDeMo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "FILENAME = Path('../scripts/yfcc100m_images_intersect_didemo_under-and-not'\n",
    "                '-nouns-leq-150_topk-1.csv')\n",
    "OUTPUT_DIR = Path('/mnt/ssd/tmp/yfcc100m')\n",
    "ROOT_DIR = Path('/mnt/ilcompf2d1/data/yfcc100m/image')\n",
    "SAMPLES_PER_TAG = 1000\n",
    "SEED = 1701\n",
    "OUTPUT_FILE = Path(f'../data/interim/yfcc100m/sample-{SEED}_intersect-didemo'\n",
    "                   '_under-and-not-nouns-leq-150_topk-1')\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "df = pd.read_csv(FILENAME, index_col=0)\n",
    "df['adobe_cil_entry'] = df['adobe_cil'].apply(os.path.dirname)\n",
    "\n",
    "# subsample per NOUNs\n",
    "df_gbl = df.groupby('topk_tags')\n",
    "ind = []\n",
    "for label, df_i in df_gbl:\n",
    "    ind_i = df_i.index.tolist()\n",
    "    random.shuffle(ind_i)\n",
    "    ind.extend(ind_i[:min(len(ind_i), SAMPLES_PER_TAG)])\n",
    "\n",
    "if not OUTPUT_DIR.exists():\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "df = df.loc[ind, :]\n",
    "total_images = len(df)\n",
    "progress = 0\n",
    "skipped = 0\n",
    "df_gbep = df.groupby('adobe_cil_entry')\n",
    "for entry, grouped in df_gbep:\n",
    "    tar_file = ROOT_DIR / (entry + '.tar')\n",
    "    reader = tarfile.open(tar_file)\n",
    "    members = set(reader.getnames())\n",
    "    for i, row in grouped.iterrows():\n",
    "        if not row['adobe_cil'] in members:\n",
    "            progress += 1\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        fid = reader.extractfile(row['adobe_cil'])\n",
    "        assert fid is not None\n",
    "        img = Image.open(fid).convert('RGB')\n",
    "        img_file = OUTPUT_DIR / row['adobe_cil'].replace('/', '_')\n",
    "        img.save(img_file)\n",
    "\n",
    "        progress += 1\n",
    "        if progress % (total_images // 10) == 0:\n",
    "            print(f'[{progress}/{total_images}]')\n",
    "df.to_csv(OUTPUT_FILE, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count instances found in YFCC100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels to augment 388\n",
      "Num instances didemo 1137\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "filename = '../scripts/yfcc100m_images_intersect_didemo_under-and-not-nouns-leq-75_topk-1.json'\n",
    "with open(filename, 'r') as fid:\n",
    "    data = json.load(fid)\n",
    "print('Num labels to augment', len(data))\n",
    "\n",
    "val_instances = []\n",
    "for noun in data:\n",
    "    val_instances.extend(stats['annotations_per_subset']['val'][noun])\n",
    "val_instances = np.unique(val_instances)\n",
    "print('Num instances didemo', len(val_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats regarding number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1596205,25,27525.0,682898,4976,63848.2,129706.97083194874\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "blah = []\n",
    "for k, v in tag2image.items():\n",
    "    if len(v) > TRAINABLE_THRESHOLD:\n",
    "        blah.append(len(v))\n",
    "\n",
    "print(f'{TOPK},'\n",
    "      f'{len(images)},'\n",
    "      f'{trainable_tags},'\n",
    "      f'{np.median(blah)},'\n",
    "      f'{np.max(blah)},'\n",
    "      f'{np.min(blah)},'\n",
    "      f'{np.mean(blah)},'\n",
    "      f'{np.std(blah)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_k,num_images,trainable_tags,median_per_tag,max_per_tag,min_per_tag,mean_per_tag,std_per_tag\n",
    "\n",
    "1,30232885,820,13102.0,871782,1011,36866.656097560975,79653.0784979082\n",
    "\n",
    "3,30232934,823,25048.0,991906,1115,62644.690157958685,117116.83888841543\n",
    "\n",
    "5,30232221,822,28745.5,1118493,1044,69639.30535279805,126868.87990805514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.178077697753906\n",
      "2.446206597222222\n"
     ]
    }
   ],
   "source": [
    "num_images = 1596205\n",
    "feat_dim = 2048\n",
    "batch_size = 256\n",
    "MAGIC_TIME = 2\n",
    "print(num_images * 4 * 2048 / 1024 / 1024/ 1024)\n",
    "print(MAGIC_TIME * 1127212 / 256 / 3600 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst case we will spend ~XGB of space in features. We could shard blocks of images and hopefully see some compression.\n",
    "\n",
    "We might extract all those features in around ~Xhours in a single GPU. Disregarding transfer and unpacking from huge repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DiDeMo and YFCC100M tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags found: 951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'altair-data-41217a5eb806a0e8ca497668ef5f1003.csv',\n",
       " 'format': {'type': 'csv'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags, rows = {}, []\n",
    "found_tags = 0\n",
    "file_ref = 'data/interim/didemo/nouns_count.csv'\n",
    "filename = 'data/interim/yfcc100m/tag_frequency.csv'\n",
    "newfile = 'data/interim/didemo/nouns_yfcc100m.csv'\n",
    "\n",
    "# Get didemo tags\n",
    "with open(file_ref) as fid:\n",
    "    i = 0\n",
    "    for line in fid:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        tag, count = line.strip().split(',')\n",
    "        tags[tag] = None\n",
    "        rows.append({'tag': tag, 'instances': count, 'dataset': 'DiDeMo'})\n",
    "\n",
    "# Add YFCC100M tags that are in Didemo\n",
    "with open(filename) as fid:\n",
    "    i = 0\n",
    "    for line in fid:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        tag, count = line.strip().split(',')\n",
    "        if tag in tags:\n",
    "            found_tags += 1\n",
    "            rows.append({'tag': tag, 'instances': count, 'dataset': 'YFCC100M'})\n",
    "print(f'Tags found: {found_tags}')\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "#df.to_csv(newfile, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take NOUNs that only appear in validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map tags to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tags: 2839\n",
      "Num clean tags: 2981\n",
      "CPU times: user 3min 15s, sys: 5.99 s, total: 3min 21s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load clean (scrapped) tags\n",
    "import csv\n",
    "filename = '../data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "# Lemmatizer to deal with plurals\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# Extract mapping tags 2 image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "\n",
    "# rows = []\n",
    "images = []\n",
    "tag2image = {}\n",
    "image_tags = []\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue \n",
    "            \n",
    "            counter = 0\n",
    "            image_url = f'{row[IMAGE_URL]}'\n",
    "            images.append(image_url)\n",
    "            image_tags.append([])\n",
    "            for tag in row[USER_TAGS].split(','):                    \n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag not in clean_tags:\n",
    "                    continue\n",
    "                \n",
    "                counter += 1\n",
    "                if lemmatized_tag in tag2image:\n",
    "                    tag2image[lemmatized_tag].append(len(images) - 1)\n",
    "                else:\n",
    "                    tag2image[lemmatized_tag] = [len(images) - 1]\n",
    "                image_tags[-1].append(tag)\n",
    "\n",
    "                if counter == TOPK:\n",
    "                    break\n",
    "print('Num tags:', len(tag2image))\n",
    "print('Num clean tags:', len(clean_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/interim/yfcc100m/tag_to_images-0.json', 'w') as fid:\n",
    "    json.dump({'images': images, 'image_tags': image_tags, 'tag2image': tag2image}, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check relevance of top-k tags in YFCC100M\n",
    "\n",
    "Compare raw top-k tags vs relevant* top-k tags for about 100 random images.\n",
    "\n",
    "- This generates the data from the webpage `yfcc100m_original_filtered_v2`\n",
    "\n",
    "* relevant means used in [this project](http://deep-tagging.cs.washington.edu/imagenet_correspondence.html)\n",
    "\n",
    "~~goal: visualize 100 random images with top-5 tags vs filtered top-5 tags~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 1.23 s, total: 58.7 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "filename = 'data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "# Those CSV are too big and we need these two lines\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import random\n",
    "\n",
    "SEED = 13029\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "SUBSET = 180 # None\n",
    "\n",
    "rows = []\n",
    "image_tag_mapping = {}\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            \n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "            if SUBSET is not None:\n",
    "                if random.random() > SUBSET / 10000000:\n",
    "                    continue  \n",
    "            \n",
    "            counter, counter2 = 0, 0\n",
    "            image_url = f'{row[IMAGE_URL]}'\n",
    "            topk_original_tags, topk_filtered_tags = [], []\n",
    "            for tag in row[USER_TAGS].split(','):\n",
    "                if counter < TOPK:\n",
    "                    counter += 1\n",
    "                    topk_original_tags.append(tag)\n",
    "                    \n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag in clean_tags:\n",
    "                    counter2 += 1\n",
    "                    topk_filtered_tags.append(lemmatized_tag)\n",
    "                    \n",
    "                if counter2 == TOPK:\n",
    "                    break\n",
    "            image_tag_mapping[image_url] = {'original': topk_original_tags,\n",
    "                                            'filtered': topk_filtered_tags}\n",
    "print('Num images:', len(image_tag_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/interim/yfcc100m/top5_tags_100.json', 'w') as fid:\n",
    "    json.dump(image_tag_mapping, fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
