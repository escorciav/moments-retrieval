{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images from Adobe repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take random row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import random\n",
    "\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "SUBSET = 100\n",
    "\n",
    "# rows = []\n",
    "images = []\n",
    "tag2image = {}\n",
    "image_tags = []\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for j, row in enumerate(reader):\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "            if SUBSET is not None:\n",
    "                if random.random() > SUBSET / 10000000:\n",
    "                    continue \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data at adobe is: `/mnt/ilcompf2d1/data/yfcc100m/`\n",
    "\n",
    "- find the image in the adobe data\n",
    "\n",
    "- show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 /mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0\n"
     ]
    }
   ],
   "source": [
    "!wc -l $i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494130\n",
      "['3948397176', '33938182@N04', 'Lucy+Takakura', '2009-09-21 13:11:55.0', '1253727701', 'Panasonic+DMC-TZ1', '%E3%82%B8%E3%83%A7%E3%83%B3%E3%83%98%E3%83%B3%E3%83%AA%E3%83%BC%E3%82%BA%E3%82%B9%E3%82%BF%E3%83%87%E3%82%A3', 'shot+by+Lumix+DMC-TZ1', 'lumixdmc-tz1', '', '', '', '', 'http://www.flickr.com/photos/33938182@N04/3948397176/', 'http://farm3.staticflickr.com/2643/3948397176_22a2a41211.jpg', 'Attribution-ShareAlike License', 'http://creativecommons.org/licenses/by-sa/2.0/', '2643', '3', '22a2a41211', '2d609ea749', 'jpg', '0']\n"
     ]
    }
   ],
   "source": [
    "print(j)\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10000000 - 1) // 10000\n",
    "494130 // 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4130"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "494130 % 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/mnt/ilcompf2d1/data/yfcc100m/image/0-049.tar'\n",
    "import tarfile\n",
    "\n",
    "reader = tarfile.open(filename)\n",
    "fid = reader.extractfile('0-049/0494130.jpg')\n",
    "assert fid is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(fid)\n",
    "# dump image\n",
    "# img.save('hola.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494130"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49 * 10000 + 4130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YFCC100M $\\cap$ DiDeMo\n",
    "\n",
    "__Note__: You can also find the following cell into `scripts/` due to the long time they take too executed (> 30mins).\n",
    "\n",
    "## 1. Get subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num evaluation instances: 4180\n",
      "Spanned instances 4156\n",
      "NOUNs are underrepresented when appear less than 76\n",
      "Total NOUNs in train 3521\n",
      "NOUNs Under-represented 3358\n",
      "NOUNs Unseen during training 167\n",
      "Num descriptions with Under&Unseen NOUNs 2119\n",
      "Num descriptions with Well NOUNs 3814\n",
      "Pctg to impact 0.5069\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def create_vocabulary(underrepresented_threshold=10):\n",
    "    \"Create vocabulary of underrepresented NOUNs to augment\"\n",
    "\n",
    "    # load stats about nouns in DIDEMO\n",
    "    NOUNS_FILE = '../data/interim/didemo/nouns_to_video.json'\n",
    "    with open(NOUNS_FILE, 'r') as fid:\n",
    "        stats = json.load(fid)\n",
    "        for k, v in stats['nouns_per_subset'].items():\n",
    "            stats['nouns_per_subset'][k] = set(v)\n",
    "    \n",
    "    # load annotations in val-set\n",
    "    VAL_FILE = '../data/raw/val_data.json'\n",
    "    with open(VAL_FILE, 'r') as fid:\n",
    "        data = json.load(fid)\n",
    "    num_val_instances = len(data)\n",
    "    spanned_annotations = []\n",
    "    for k, v in stats['annotations_per_subset']['val'].items():\n",
    "        spanned_annotations.extend(v)\n",
    "    spanned_annotations = np.unique(spanned_annotations)\n",
    "    \n",
    "    nouns_vocab = set()\n",
    "    # TODO add val_ids to prioritize verification step\n",
    "    nouns_and_metadata = {}\n",
    "    num_nouns_train = len(stats['counts_per_subset']['train'])\n",
    "    num_nouns_underrepresented = 0\n",
    "    val_ids_toimpact = []\n",
    "    val_ids_represented = []\n",
    "    for k, v in stats['counts_per_subset']['train'].items():\n",
    "        if v <= underrepresented_threshold:\n",
    "            num_nouns_underrepresented += 1\n",
    "            if k in stats['counts_per_subset']['val']:\n",
    "                val_ids_toimpact.extend(\n",
    "                    stats['annotations_per_subset']['val'][k])\n",
    "                nouns_vocab.add(k)\n",
    "        else:\n",
    "            if k in stats['counts_per_subset']['val']:\n",
    "                val_ids_represented.extend(\n",
    "                    stats['annotations_per_subset']['val'][k])\n",
    "\n",
    "    num_samples_well_represented = len(np.unique(val_ids_represented))\n",
    "    nouns_only_val = (stats['nouns_per_subset']['val'] -\n",
    "                      stats['nouns_per_subset']['train'])\n",
    "    num_nouns_notrepresented = len(nouns_only_val)\n",
    "    for k in nouns_only_val:\n",
    "        val_ids_toimpact.extend(\n",
    "            stats['annotations_per_subset']['val'][k])\n",
    "        nouns_vocab.add(k)\n",
    "    num_samples_to_impact = len(np.unique(val_ids_toimpact))\n",
    "\n",
    "    print('Num evaluation instances:', num_val_instances)\n",
    "    print('Spanned instances', len(spanned_annotations))\n",
    "    print('NOUNs are underrepresented when appear less than', underrepresented_threshold + 1)\n",
    "    print('Total NOUNs in train', num_nouns_train)\n",
    "    print('NOUNs Under-represented', num_nouns_underrepresented)\n",
    "    print('NOUNs Unseen during training', num_nouns_notrepresented)\n",
    "    print('Num descriptions with Under&Unseen NOUNs', num_samples_to_impact)\n",
    "    print('Num descriptions with Well NOUNs', num_samples_well_represented)\n",
    "    print('Pctg to impact', f'{num_samples_to_impact / num_val_instances:.4f}')\n",
    "    return nouns_vocab\n",
    "\n",
    "nouns = create_vocabulary(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "IMAGES_PER_TAR = 10000\n",
    "TRAINABLE_THRESHOLD = 1000\n",
    "UNDERREPRESENTED_THRESHOLD = 75\n",
    "\n",
    "# Load clean (scrapped) tags\n",
    "import csv\n",
    "filename = '../data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "# load a set of interesting NOUNs from DiDeMo\n",
    "didemo_nouns = create_vocabulary(UNDERREPRESENTED_THRESHOLD)\n",
    "\n",
    "# Lemmatizer to deal with plurals\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# Extract mapping tags 2 image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "tag2image = {}\n",
    "images = []\n",
    "image_tags_topk_flickr = []\n",
    "image_tags_flickr = []\n",
    "image_tags_topk = []\n",
    "image_tags = []\n",
    "image_urls = []\n",
    "for filename in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-*'):\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        file_index = filename.split('-')[1]\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line_no, row in enumerate(reader):\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "\n",
    "            counter, amen = 0, False\n",
    "            tags_topk_flickr = []\n",
    "            tags_topk = []\n",
    "            for tag in row[USER_TAGS].split(','):\n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag not in clean_tags:\n",
    "                    continue\n",
    "                if lemmatized_tag not in didemo_nouns:\n",
    "                    continue\n",
    "                # after this point the image will be considered, thus we\n",
    "                # can add it to tag2image dict\n",
    "                counter += 1\n",
    "                tags_topk.append(lemmatized_tag)\n",
    "                tags_topk_flickr.append(tag)\n",
    "                amen = True\n",
    "                if lemmatized_tag in tag2image:\n",
    "                    tag2image[lemmatized_tag].append(len(images))\n",
    "                else:\n",
    "                    tag2image[lemmatized_tag] = [len(images)]\n",
    "                if counter == TOPK:\n",
    "                    # according to previous Bryan's project top-5 tags in YFCC100M\n",
    "                    # have purity >= 60%\n",
    "                    break\n",
    "            tags = [i for i in row[USER_TAGS].split(',')\n",
    "                    if lemmatizer(i, u'NOUN')[0] in clean_tags]\n",
    "\n",
    "            # I don't need this but just in case\n",
    "            if not amen:\n",
    "                continue\n",
    "            tar_index = line_no // IMAGES_PER_TAR\n",
    "            img_index = line_no % IMAGES_PER_TAR\n",
    "            image_loc = f'{file_index}-{tar_index:03d}/{tar_index:03d}{img_index:04d}.jpg'\n",
    "            image_url = row[IMAGE_URL]\n",
    "            images.append(image_loc)\n",
    "            image_urls.append(image_url)\n",
    "            image_tags_topk_flickr.append(';'.join(tags_topk_flickr))\n",
    "            image_tags_flickr.append(row[USER_TAGS].replace(',', ';'))\n",
    "            image_tags_topk.append(';'.join(tags_topk))\n",
    "            image_tags.append(';'.join(tags))\n",
    "\n",
    "print('Num clean tags:', len(clean_tags))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "NOUNS_FILE = '../data/interim/didemo/nouns_to_video.json'\n",
    "with open(NOUNS_FILE, 'r') as fid:\n",
    "    didemo_stats = json.load(fid)\n",
    "\n",
    "trainable_tags = 0\n",
    "val_instances = []\n",
    "for k, v in tag2image.items():\n",
    "    if len(v) > TRAINABLE_THRESHOLD:\n",
    "        trainable_tags += 1\n",
    "    val_instances.extend(didemo_stats[k])\n",
    "val_instances = np.unique(val_instances)\n",
    "print(f'Top-{TOPK} gives {len(tag2image)} '\n",
    "      f'and {trainable_tags} >= {TRAINABLE_THRESHOLD} images')\n",
    "print(f'Number of val instances with those NOUNs {len(val_instances)}')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([images, image_urls,\n",
    "                   image_tags_topk, image_tags,\n",
    "                   image_tags_topk_flickr, image_tags_flickr]).T\n",
    "df.columns = ['adobe_cil', 'url', 'topk_tags', 'tags', 'topk_tags_yfcc100m', 'tags_yfcc100m']\n",
    "basename = (f'yfcc100m_images_intersect_didemo_under-and-not-nouns-leq'\n",
    "            f'-{UNDERREPRESENTED_THRESHOLD}_topk-{TOPK}')\n",
    "df.to_csv(f'{basename}.csv')\n",
    "with open(f'{basename}.json', 'w') as fid:\n",
    "    json.dump(tag2image, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unpack and sub-sample of YFCC100M relevant for DiDeMo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "FILENAME = Path('../scripts/yfcc100m_images_intersect_didemo_under-and-not'\n",
    "                '-nouns-leq-150_topk-1.csv')\n",
    "OUTPUT_DIR = Path('/mnt/ssd/tmp/yfcc100m')\n",
    "ROOT_DIR = Path('/mnt/ilcompf2d1/data/yfcc100m/image')\n",
    "SAMPLES_PER_TAG = 1000\n",
    "SEED = 1701\n",
    "OUTPUT_FILE = Path(f'../data/interim/yfcc100m/sample-{SEED}_intersect-didemo'\n",
    "                   '_under-and-not-nouns-leq-150_topk-1')\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "df = pd.read_csv(FILENAME, index_col=0)\n",
    "df['adobe_cil_entry'] = df['adobe_cil'].apply(os.path.dirname)\n",
    "\n",
    "# subsample per NOUNs\n",
    "df_gbl = df.groupby('topk_tags')\n",
    "ind = []\n",
    "for label, df_i in df_gbl:\n",
    "    ind_i = df_i.index.tolist()\n",
    "    random.shuffle(ind_i)\n",
    "    ind.extend(ind_i[:min(len(ind_i), SAMPLES_PER_TAG)])\n",
    "\n",
    "if not OUTPUT_DIR.exists():\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "df = df.loc[ind, :]\n",
    "total_images = len(df)\n",
    "progress = 0\n",
    "skipped = 0\n",
    "df_gbep = df.groupby('adobe_cil_entry')\n",
    "for entry, grouped in df_gbep:\n",
    "    tar_file = ROOT_DIR / (entry + '.tar')\n",
    "    reader = tarfile.open(tar_file)\n",
    "    members = set(reader.getnames())\n",
    "    for i, row in grouped.iterrows():\n",
    "        if not row['adobe_cil'] in members:\n",
    "            progress += 1\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        fid = reader.extractfile(row['adobe_cil'])\n",
    "        assert fid is not None\n",
    "        img = Image.open(fid).convert('RGB')\n",
    "        img_file = OUTPUT_DIR / row['adobe_cil'].replace('/', '_')\n",
    "        img.save(img_file)\n",
    "\n",
    "        progress += 1\n",
    "        if progress % (total_images // 10) == 0:\n",
    "            print(f'[{progress}/{total_images}]')\n",
    "df.to_csv(OUTPUT_FILE, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count instances found in YFCC100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels to augment 388\n",
      "Num instances didemo 1137\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "filename = '../scripts/yfcc100m_images_intersect_didemo_under-and-not-nouns-leq-75_topk-1.json'\n",
    "with open(filename, 'r') as fid:\n",
    "    data = json.load(fid)\n",
    "print('Num labels to augment', len(data))\n",
    "\n",
    "val_instances = []\n",
    "for noun in data:\n",
    "    val_instances.extend(stats['annotations_per_subset']['val'][noun])\n",
    "val_instances = np.unique(val_instances)\n",
    "print('Num instances didemo', len(val_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats regarding number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1596205,25,27525.0,682898,4976,63848.2,129706.97083194874\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "blah = []\n",
    "for k, v in tag2image.items():\n",
    "    if len(v) > TRAINABLE_THRESHOLD:\n",
    "        blah.append(len(v))\n",
    "\n",
    "print(f'{TOPK},'\n",
    "      f'{len(images)},'\n",
    "      f'{trainable_tags},'\n",
    "      f'{np.median(blah)},'\n",
    "      f'{np.max(blah)},'\n",
    "      f'{np.min(blah)},'\n",
    "      f'{np.mean(blah)},'\n",
    "      f'{np.std(blah)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_k,num_images,trainable_tags,median_per_tag,max_per_tag,min_per_tag,mean_per_tag,std_per_tag\n",
    "\n",
    "1,30232885,820,13102.0,871782,1011,36866.656097560975,79653.0784979082\n",
    "\n",
    "3,30232934,823,25048.0,991906,1115,62644.690157958685,117116.83888841543\n",
    "\n",
    "5,30232221,822,28745.5,1118493,1044,69639.30535279805,126868.87990805514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3016128540039062\n",
      "0.9391254340277778\n"
     ]
    }
   ],
   "source": [
    "num_images = 432749\n",
    "feat_dim = 2048\n",
    "batch_size = 256\n",
    "MAGIC_TIME = 2\n",
    "print(num_images * 4 * 2048 / 1024 / 1024/ 1024)\n",
    "print(MAGIC_TIME * num_images / 256 / 3600 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst case we will spend ~XGB of space in features. We could shard blocks of images and hopefully see some compression.\n",
    "\n",
    "We might extract all those features in around ~Xhours in a single GPU. Disregarding transfer and unpacking from huge repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images for features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "FILENAME = '../data/interim/yfcc100m/intersect_didemo/sample-1701_under-and-not-nouns-leq-150_topk-1.csv'\n",
    "OUTPUT_FILE = '../data/interim/yfcc100m/intersect_didemo/feature-extraction_sample-1701_under-and-not-nouns-leq-150_topk-1.csv'\n",
    "DIRNAME = Path('/mnt/ssd/tmp/yfcc100m/')\n",
    "df = pd.read_csv(FILENAME)\n",
    "ind = df['adobe_cil'].apply(lambda x: os.path.exists(DIRNAME / x.replace('/', '_')))\n",
    "df['image'] = df['adobe_cil'].apply(lambda x: x.replace('/', '_'))\n",
    "df_ = df.loc[ind, ['image']]\n",
    "df_.to_csv(OUTPUT_FILE, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE experiment\n",
    "\n",
    "Metadata for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_FILE = '../data/interim/yfcc100m/intersect_didemo/tsne-metadata_under-and-not-nouns-leq-150_topk-1.tsv'\n",
    "TAGS = [\n",
    "    'penguin',\n",
    "    'mural',\n",
    "    'canoe',\n",
    "    'tattoo',\n",
    "    'parrot',\n",
    "    'playground',\n",
    "    'crab',\n",
    "    'scuba',\n",
    "    'cow',\n",
    "    'bride',\n",
    "    'tram',\n",
    "    'owl',\n",
    "    'fog'\n",
    "]\n",
    "keys = []\n",
    "times = []\n",
    "ind_label = []\n",
    "\n",
    "# Forming video meta-data\n",
    "NOUNS_FILE = '../data/interim/didemo/nouns_to_video.json'\n",
    "with open(NOUNS_FILE, 'r') as fid:\n",
    "    data = json.load(fid)\n",
    "    for i, tag in enumerate(TAGS):\n",
    "        video_names_i = data['videos'][tag]\n",
    "        keys.extend(video_names_i)\n",
    "        ind_label.extend([i] * len(video_names_i))\n",
    "        times.extend(data['time'][tag])\n",
    "num_videos = len(keys)\n",
    "\n",
    "# Forming image meta-data\n",
    "IMG_FILE = '../data/interim/yfcc100m/intersect_didemo/sample-1701_under-and-not-nouns-leq-150_topk-1.csv'\n",
    "HDF5_FILE = '/mnt/ssd/tmp/didemo/resnet152.h5'\n",
    "SAMPLES_PER_TAG = 100\n",
    "SEED = 1701\n",
    "\n",
    "fid = h5py.File(HDF5_FILE, 'r')\n",
    "df = pd.read_csv(IMG_FILE)\n",
    "df['key'] = df['adobe_cil'].apply(lambda x: x.replace('/', '_'))\n",
    "df_gbt = df.groupby('topk_tags')\n",
    "for i, tag in enumerate(TAGS):\n",
    "    df_i = df_gbt.get_group(tag)\n",
    "    ind = [i for i, row in df_i.iterrows()\n",
    "           if row['key'] in fid]\n",
    "    random.shuffle(ind)\n",
    "    assert len(ind) >= SAMPLES_PER_TAG\n",
    "    ind = ind[:SAMPLES_PER_TAG]\n",
    "    ind_label.extend([i] * SAMPLES_PER_TAG)\n",
    "    times.extend([None] * SAMPLES_PER_TAG)\n",
    "    keys.extend(df.loc[ind, 'key'].tolist())\n",
    "fid.close()\n",
    "    \n",
    "ind_source = np.concatenate([[0] * num_videos,\n",
    "                             [1] * (len(keys) - num_videos)])\n",
    "tags = np.array(TAGS)\n",
    "source = np.array(['video', 'image'])\n",
    "df = pd.DataFrame({'key': keys,\n",
    "                   'time': times,\n",
    "                   'label': tags[ind_label],\n",
    "                   'source': source[ind_source]})\n",
    "df['source_label'] = df['source'].astype('str') + '_' + df['label']\n",
    "df.to_csv(OUTPUT_FILE, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script for T-SNE visualization\n",
    "\n",
    "After (wasting) time, setting up everything in the cluster I found a website where we only need to pass the data 🤬\n",
    "\n",
    "[website](https://projector.tensorflow.org/)\n",
    "\n",
    "BTW, it seems that all the computation is done in from the browser in your laptop. Thus, it does not matter if you use the ommented part or the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "FILENAMES = ['../data/interim/didemo/resnet152/320x240_max.h5',\n",
    "             '../data/interim/yfcc100m/resnet152/320x240_001.h5']\n",
    "METADATA = '../data/interim/yfcc100m/intersect_didemo/tsne-metadata_under-and-not-nouns-leq-150_topk-1.tsv'\n",
    "OUTPUTDATA = '../data/interim/yfcc100m/intersect_didemo/tsne-data_under-and-not-nouns-leq-150_topk-1.tsv'\n",
    "LOG_DIR = 'scripts/log'\n",
    "D = 2048\n",
    "\n",
    "class HDF5sOnlyDatasets():\n",
    "    def __init__(self, filenames):\n",
    "        self.files = filenames\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        name, file_ind = key\n",
    "        with h5py.File(self.files[file_ind], 'r') as fid:\n",
    "            return fid[name][:]\n",
    "\n",
    "repo = HDF5sOnlyDatasets(FILENAMES)\n",
    "df = pd.read_csv(METADATA, sep='\\t')\n",
    "df['time'] = df['time'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else None)\n",
    "data = np.zeros((len(df), D), dtype=np.float32)\n",
    "for i, row in df.iterrows():\n",
    "    src_name = row['key']\n",
    "    if row['time']:\n",
    "        start, end = row['time']\n",
    "        feat = repo[src_name, 0]\n",
    "        data[i, :] = np.mean(feat[start:end + 1, :], axis=0)\n",
    "    else:\n",
    "        data[i, :] = repo[src_name, 1][0, :]\n",
    "\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.to_csv('data_all.tsv', sep='\\t', index=None, header=False)\n",
    "\n",
    "# This work but it runs locally so I found more practical to use the projector (if u have internet XD)\n",
    "# embedding_vars = []\n",
    "# metadata = []\n",
    "# # TODO: add dimension\n",
    "# embedding_vars.append(tf.get_variable(\"all\", initializer=data))\n",
    "# for tag, df_i in df.groupby('label'):\n",
    "#     ind = df_i.index\n",
    "#     embedding_vars.append(tf.get_variable(tag, initializer=data[ind, :]))\n",
    "# embedding_var = embedding_vars[0]\n",
    "\n",
    "# summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "# # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "# config = projector.ProjectorConfig()\n",
    "# # You can add multiple embeddings. Here we add only one.\n",
    "# embedding = config.embeddings.add()\n",
    "# embedding.tensor_name = embedding_var.name\n",
    "# # Link this tensor to its metadata file (e.g. labels).\n",
    "# embedding.metadata_path = os.path.join(os.getcwd(), METADATA)\n",
    "# # The next line writes a projector_config.pbtxt in the LOG_DIR.\n",
    "# # TensorBoard will read this file during startup.\n",
    "# projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "# # Specify where you find the sprite (we will create this later)\n",
    "# # embedding.sprite.image_path = path_for_mnist_sprites #'mnistdigits.png'\n",
    "# # embedding.sprite.single_image_dim.extend([28,28])\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver = tf.train.Saver()\n",
    "# saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/val partition from YFCC100M\n",
    "\n",
    "Last update: July 26. created: July 24.\n",
    "\n",
    "__Note__: Abandon this as we just wanna focus on improving video embedding.\n",
    "\n",
    "I only tackle the minimum minimorum to make sure that partitions are disjoint, filter by uploader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost uploader ID while sampling dataset. Thus, we need to trace it back.\n",
    "\n",
    "- We can do that base on the 'adobe_cil' field. In the following way:\n",
    "\n",
    "    - Get the first digit from the basename of dirname.\n",
    "    \n",
    "    - 49 * 10000 + 4130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "IMAGE_FILE = '../data/interim/yfcc100m/intersect_didemo/sample-1701_under-and-not-nouns-leq-150_topk-1.csv'\n",
    "CSV_YFCC100M_FMT = '/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-{}'\n",
    "IMAGES_PER_TAR = 10000\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "\n",
    "def get_back_to_row(filename):\n",
    "    filename = os.path.basename(os.path.splitext(filename)[0])\n",
    "    upper = int(filename[:3])\n",
    "    lower = int(filename[3:])\n",
    "    return upper * 10000 + lower\n",
    "    \n",
    "df = pd.read_csv(IMAGE_FILE)\n",
    "df['adobe_csv_index'] = df['adobe_cil_entry'].apply(lambda x: x[0])\n",
    "df['row_index'] = df['adobe_cil'].apply(get_back_to_row)\n",
    "df['image_id'] = 'dummy'\n",
    "df['user_id'] = 'dummy'\n",
    "df['row_index'] = df['adobe_cil'].apply(get_back_to_row)\n",
    "for index, df_i in df.groupby('adobe_csv_index'):\n",
    "    df_origin = pd.read_csv(CSV_YFCC100M_FMT.format(index), header=None, sep='\\t')\n",
    "    ind = df_i.index\n",
    "    df_i.loc[ind, 'user_id'] = df_origin.loc[df_i.loc[:, 'row_index'], USER_ID].values\n",
    "    df_i.loc[ind, 'image_id'] = df_origin.loc[df_i.loc[:, 'row_index'], IMAGE_ID].values\n",
    "\n",
    "# Re-dump CSV\n",
    "columns = set(df.columns.tolist()) - {'adobe_csv_index', 'row_index'}\n",
    "df.to_csv(IMAGE_FILE, index=None, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning CSV with dumped features\n",
    "\n",
    "Some images were not in the repo, those we gotta discard them. This cell cleans the CSV such that we only pass valid images to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_FILE = '../data/interim/yfcc100m/intersect_didemo/sample-1701_under-and-not-nouns-leq-150_topk-1.csv'\n",
    "OUTPUT_FILE = '../data/interim/yfcc100m/001.csv'\n",
    "HDF5_FILE = '../data/interim/yfcc100m/resnet152/320x240_001.h5'\n",
    "COLUMNS_OF_INTEREST = ['key', 'topk_tags']\n",
    "\n",
    "df = pd.read_csv(IMAGE_FILE)\n",
    "df['h5_id'] = df['adobe_cil'].apply(lambda x: x.replace('/', '_'))\n",
    "with h5py.File(HDF5_FILE, 'r') as fid:\n",
    "    ind = df['h5_id'].apply(lambda x: x in fid)\n",
    "df = df.loc[ind, :]\n",
    "df.to_csv(OUTPUT_FILE, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new JSON for training\n",
    "\n",
    "created: July 26. Last update: July 24.\n",
    "\n",
    "- We add a fake 'time' to image [0, 0] to index the feature matrix properly.\n",
    "\n",
    "- We add a new `bool` field to all instances called `source`. It's integer representing the source of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DIDEMO_FILE = '../data/raw/train_data.json'\n",
    "IMG_FILE = '../data/interim/yfcc100m/001.csv'\n",
    "TRAIN_FILE = '../data/interim/didemo_yfcc100m/train_data.json'\n",
    "\n",
    "# Unused because JSON cannot serialize number > int64\n",
    "def get_max_annotation_id():\n",
    "    max_annotation_id = -1000\n",
    "    for i in ['train', 'val', 'test']:\n",
    "        with open(f'../data/raw/{i}_data.json', 'r') as fid:\n",
    "            i_instances = json.load(fid)\n",
    "        df = pd.DataFrame(i_instances)\n",
    "        max_annotation_id = max(max_annotation_id, df['annotation_id'].max())\n",
    "    return max_annotation_id\n",
    "\n",
    "with open(DIDEMO_FILE, 'r') as fid:\n",
    "    didemo_instances = json.load(fid)\n",
    "for instance in didemo_instances:\n",
    "    instance['source'] = 0\n",
    "    \n",
    "assert didemo_instances[0]['source'] == 0\n",
    "offset_annotation_id = get_max_annotation_id() + 1\n",
    "\n",
    "df = pd.read_csv(IMG_FILE)\n",
    "for i, row in df.iterrows():\n",
    "    instance_i = {'video': row['h5_id'],\n",
    "                  'times': [[0, 0]],\n",
    "                  'description': row['topk_tags'],\n",
    "                  'num_segments': None,\n",
    "                  'source': 1,\n",
    "                  'annotation_id': None}\n",
    "                  # 'annotation_id': i + offset_annotation_id}\n",
    "    didemo_instances.append(instance_i)\n",
    "\n",
    "with open(TRAIN_FILE, 'w') as fid:\n",
    "    json.dump(didemo_instances, fid)\n",
    "# TODO: NOT HARD-CODE THIS\n",
    "with open(TRAIN_FILE.replace('data.json', 'metadata.csv'), 'w') as fid:\n",
    "    fid.write('source,id\\n')\n",
    "    fid.write('didemo/train_data.json,0\\n')\n",
    "    fid.write('yfcc100m/001.csv,1\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump list for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "IMG_FILE = '../data/interim/yfcc100m/001.csv'\n",
    "TRAIN_FILE = '../data/interim/yfcc100m/train_02/train_data.json'\n",
    "\n",
    "didemo_instances = []\n",
    "df = pd.read_csv(IMG_FILE)\n",
    "for i, row in df.iterrows():\n",
    "    instance_i = {'video': row['h5_id'],\n",
    "                  'times': [[0, 0]],\n",
    "                  'description': row['tags'].split(';'),\n",
    "                  'num_segments': None,\n",
    "                  'source': 1,\n",
    "                  'annotation_id': None}\n",
    "                  # 'annotation_id': i + offset_annotation_id}\n",
    "    didemo_instances.append(instance_i)\n",
    "\n",
    "!mkdir -p $(dirname $TRAIN_FILE)\n",
    "with open(TRAIN_FILE, 'x') as fid:\n",
    "    json.dump(didemo_instances, fid)\n",
    "# Please edit this to keep record of what u do\n",
    "with open(TRAIN_FILE.replace('train_data.json', 'metadata.csv'), 'x') as fid:\n",
    "    fid.write('source,id\\n')\n",
    "    fid.write('yfcc100m/001.csv,1\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DiDeMo and YFCC100M tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags found: 951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'altair-data-41217a5eb806a0e8ca497668ef5f1003.csv',\n",
       " 'format': {'type': 'csv'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags, rows = {}, []\n",
    "found_tags = 0\n",
    "file_ref = 'data/interim/didemo/nouns_count.csv'\n",
    "filename = 'data/interim/yfcc100m/tag_frequency.csv'\n",
    "newfile = 'data/interim/didemo/nouns_yfcc100m.csv'\n",
    "\n",
    "# Get didemo tags\n",
    "with open(file_ref) as fid:\n",
    "    i = 0\n",
    "    for line in fid:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        tag, count = line.strip().split(',')\n",
    "        tags[tag] = None\n",
    "        rows.append({'tag': tag, 'instances': count, 'dataset': 'DiDeMo'})\n",
    "\n",
    "# Add YFCC100M tags that are in Didemo\n",
    "with open(filename) as fid:\n",
    "    i = 0\n",
    "    for line in fid:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        tag, count = line.strip().split(',')\n",
    "        if tag in tags:\n",
    "            found_tags += 1\n",
    "            rows.append({'tag': tag, 'instances': count, 'dataset': 'YFCC100M'})\n",
    "print(f'Tags found: {found_tags}')\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "#df.to_csv(newfile, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take NOUNs that only appear in validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map tags to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tags: 2839\n",
      "Num clean tags: 2981\n",
      "CPU times: user 3min 15s, sys: 5.99 s, total: 3min 21s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load clean (scrapped) tags\n",
    "import csv\n",
    "filename = '../data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "# Lemmatizer to deal with plurals\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# Extract mapping tags 2 image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "# Those CSV-files are too big and we need these two lines\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "MACHINE_TAGS = 9\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "\n",
    "# rows = []\n",
    "images = []\n",
    "tag2image = {}\n",
    "image_tags = []\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue \n",
    "            \n",
    "            counter = 0\n",
    "            image_url = f'{row[IMAGE_URL]}'\n",
    "            images.append(image_url)\n",
    "            image_tags.append([])\n",
    "            for tag in row[USER_TAGS].split(','):                    \n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag not in clean_tags:\n",
    "                    continue\n",
    "                \n",
    "                counter += 1\n",
    "                if lemmatized_tag in tag2image:\n",
    "                    tag2image[lemmatized_tag].append(len(images) - 1)\n",
    "                else:\n",
    "                    tag2image[lemmatized_tag] = [len(images) - 1]\n",
    "                image_tags[-1].append(tag)\n",
    "\n",
    "                if counter == TOPK:\n",
    "                    break\n",
    "print('Num tags:', len(tag2image))\n",
    "print('Num clean tags:', len(clean_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/interim/yfcc100m/tag_to_images-0.json', 'w') as fid:\n",
    "    json.dump({'images': images, 'image_tags': image_tags, 'tag2image': tag2image}, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check relevance of top-k tags in YFCC100M\n",
    "\n",
    "Compare raw top-k tags vs relevant* top-k tags for about 100 random images.\n",
    "\n",
    "- This generates the data from the webpage `yfcc100m_original_filtered_v2`\n",
    "\n",
    "* relevant means used in [this project](http://deep-tagging.cs.washington.edu/imagenet_correspondence.html)\n",
    "\n",
    "~~goal: visualize 100 random images with top-5 tags vs filtered top-5 tags~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 1.23 s, total: 58.7 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "filename = 'data/interim/yfcc100m/tag_frequency.csv'\n",
    "clean_tags = set()\n",
    "with open(filename, 'r') as fid:\n",
    "    reader = csv.DictReader(fid, delimiter=',')\n",
    "    for row in reader:\n",
    "        clean_tags.add(row['tag'])\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "# Those CSV are too big and we need these two lines\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import random\n",
    "\n",
    "SEED = 13029\n",
    "if SEED is not None:\n",
    "    random.seed(SEED)\n",
    "TOPK = 5\n",
    "IMAGE_ID = 0\n",
    "USER_ID = 1\n",
    "USER_TAGS = 8\n",
    "IMAGE_URL = 14\n",
    "PHOTO_OR_VIDEO = 22\n",
    "SUBSET = 180 # None\n",
    "\n",
    "rows = []\n",
    "image_tag_mapping = {}\n",
    "for i in glob.glob('/mnt/ilcompf2d1/data/yfcc100m/yfcc100m_dataset-0*'):\n",
    "    with open(i, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            \n",
    "            if int(row[PHOTO_OR_VIDEO]) != 0 or len(row[USER_TAGS]) == 0:\n",
    "                continue\n",
    "            if SUBSET is not None:\n",
    "                if random.random() > SUBSET / 10000000:\n",
    "                    continue  \n",
    "            \n",
    "            counter, counter2 = 0, 0\n",
    "            image_url = f'{row[IMAGE_URL]}'\n",
    "            topk_original_tags, topk_filtered_tags = [], []\n",
    "            for tag in row[USER_TAGS].split(','):\n",
    "                if counter < TOPK:\n",
    "                    counter += 1\n",
    "                    topk_original_tags.append(tag)\n",
    "                    \n",
    "                lemmatized_tag = lemmatizer(tag, u'NOUN')[0]\n",
    "                if lemmatized_tag in clean_tags:\n",
    "                    counter2 += 1\n",
    "                    topk_filtered_tags.append(lemmatized_tag)\n",
    "                    \n",
    "                if counter2 == TOPK:\n",
    "                    break\n",
    "            image_tag_mapping[image_url] = {'original': topk_original_tags,\n",
    "                                            'filtered': topk_filtered_tags}\n",
    "print('Num images:', len(image_tag_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/interim/yfcc100m/top5_tags_100.json', 'w') as fid:\n",
    "    json.dump(image_tag_mapping, fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
