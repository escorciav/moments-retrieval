{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown retrieval results\n",
    "\n",
    "## Breakdown per NOUNs\n",
    "\n",
    "Study single video retrieval results per NOUNs.\n",
    "\n",
    "- Use cell below to dump CSV and add data to G-sheets. To quickly swift through data and plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "yfcc100m_csv = '../data/interim/yfcc100m/001.csv'\n",
    "nouns_to_video_didemo_json = '../data/interim/didemo/nouns_to_video.json'\n",
    "exp_csv = '../data/interim/hsmcn_07/2.csv'\n",
    "OUTPUT_FILE = '../data/interim/hsmcn_07/2_breakdown.csv'\n",
    "SUBSET = 'val'\n",
    "METRICS = ['iou', 'r@1', 'r@5']\n",
    "\n",
    "yfcc100m_df = pd.read_csv(yfcc100m_csv)\n",
    "with open(nouns_to_video_didemo_json, 'r') as fid:\n",
    "    nouns_to_instances = json.load(fid)['annotations_per_subset'][SUBSET]\n",
    "exp_df = pd.read_csv(exp_csv)\n",
    "\n",
    "nouns_metrics = []\n",
    "underrep_ids = []\n",
    "for tag, _ in yfcc100m_df.groupby('topk_tags'):\n",
    "    annotation_ids = nouns_to_instances[tag]\n",
    "    underrep_ids.extend(annotation_ids)\n",
    "    ind = np.in1d(exp_df['annotation_id'], annotation_ids)\n",
    "    if len(ind) == 0:\n",
    "        continue\n",
    "    # groupby if slow as hell\n",
    "    metrics = [exp_df.loc[ind, i].mean() for i in METRICS]\n",
    "    nouns_metrics.append([tag] + metrics)\n",
    "\n",
    "# Add results from under-represented NOUNs\n",
    "ind = np.in1d(exp_df['annotation_id'], underrep_ids)\n",
    "metrics = [exp_df.loc[ind, i].mean() for i in METRICS]\n",
    "nouns_metrics.append(['NOUNs Underrepresented/Unseen'] + metrics)\n",
    "wellrep_ids = np.setdiff1d(exp_df['annotation_id'], underrep_ids)\n",
    "assert set(underrep_ids).isdisjoint(wellrep_ids.tolist())\n",
    "ind = np.in1d(exp_df['annotation_id'], wellrep_ids)\n",
    "metrics = [exp_df.loc[ind, i].mean() for i in METRICS]\n",
    "nouns_metrics.append(['NOUNs Overrepresented'] + metrics)\n",
    "\n",
    "df = pd.DataFrame(nouns_metrics, columns=['attribute'] + METRICS)\n",
    "df.to_csv(OUTPUT_FILE, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence retrieval evaluation\n",
    "\n",
    "Updated: Aug 10. Created: Aug 9\n",
    "\n",
    "Study phrase retrieval from visual moments holistically.\n",
    "\n",
    "__Instructions__\n",
    "\n",
    "Use cell below to dump CSV and add data to G-sheets. To quickly swift through data and plot results.\n",
    "\n",
    "__Assumptions__\n",
    "\n",
    "- Each row represents a visual moment and colums represents phrases.\n",
    "\n",
    "- data was loaded from a JSON containing a list. ðŸ¤ž it's read on the same order.\n",
    "\n",
    "- TODO (minor): dump annotation_id in hdf5 for cross-matching\n",
    "\n",
    "- TODO (minor): cross-check JSON always return the list in the correct order. Why wouldn't do it?\n",
    "\n",
    "By August 9, there is a strong assumption of ordering in the code without assertion i.e.\n",
    "\n",
    "<img src=\"https://www.generadormemes.com/download/77megl\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from np_segments_ops import iou as iou_op\n",
    "\n",
    "def compute_phrase_rank(filename, ind_blockout=None):\n",
    "    # TODO - blah\n",
    "    # each row represents who well a moment is described by a phrase we wish to see\n",
    "    # a diagonal matrix, dreaming has 0 cost, coming back to reality that's not\n",
    "    # gonna happen. Thus we gotta look where are the phrases associated with our\n",
    "    # moments. If they are far from the diagonal, we are doing something wrong or\n",
    "    # there is a lot of correlation among moments/descriptions\n",
    "    # @escorciav couldn't find another way to do this, but he is a good guy here is\n",
    "    # the explanation. BTW, PR is welcome.\n",
    "    # Do u remember the diagoanl stuff? OK, it means we are looking for the\n",
    "    # location of the row index in each row. What? here we go again. In the first\n",
    "    # row, the phrase of that moment is on the first column. Is it clear now? just\n",
    "    # to be clear, for the i-th row the phrase it's on the i-th column.\n",
    "    # Given that numpy broadcast along axis 0, we better transpose. Later, we\n",
    "    # substract the vector of number of rows. When the element is zero, it means\n",
    "    # that's the spot of our phrase. To recover back its rank, you better transpose\n",
    "    # back. Phew... that's how you throw away 1.5 hour of thoughts and dump\n",
    "    # documentation about it (hopefully useful for someone else?)\n",
    "    with h5py.File(filename, 'r') as fid:\n",
    "        use_similarity = fid['similarity'].value\n",
    "        distance_matrix = fid['prediction_matrix'][:]\n",
    "    \n",
    "    if ind_blockout is not None:\n",
    "        fill_value = distance_matrix.max()\n",
    "    if use_similarity:\n",
    "        fill_value = 0\n",
    "    distance_matrix[ind_blockout] = fill_value\n",
    "    \n",
    "    num_moments = len(distance_matrix)\n",
    "    sorted_ind_matrix = distance_matrix.argsort()\n",
    "    if use_similarity:\n",
    "        raise NotImplementedError('WIP: we didnt care about')\n",
    "    phrases_rank = np.where(\n",
    "        (sorted_ind_matrix.T - np.arange(num_moments)).T == 0)[1]\n",
    "    return phrases_rank, sorted_ind_matrix\n",
    "\n",
    "# relevant inputs\n",
    "# the YFCC100 files allow to study results for a set of under-represented\n",
    "# nouns. Choose them basd on the log-files or the set of your interest\n",
    "FILENAME_REF = '../data/interim/hsmcn_07/9_phrase_retrieval.h5'\n",
    "# only-videos\n",
    "FILENAME = '../data/interim/smcn_12/a/5_phrase_retrieval.h5'\n",
    "# only-images\n",
    "#FILENAME = '../data/interim/hsmcn_07/9_phrase_retrieval.h5'\n",
    "YFCC100_JSON = '../data/interim/yfcc100m/train_02/train_data.json'\n",
    "YFCC100_CSV = '../data/interim/yfcc100m/001.csv'\n",
    "# only-images clean-up-S2\n",
    "# FILENAME = '../data/interim/hsmcn_08/c/0_phrase_retrieval.h5'\n",
    "# YFCC100_JSON = '../data/interim/yfcc100m/train_04/train_data.json'\n",
    "# YFCC100_CSV = '../data/interim/yfcc100m/003-25-1.csv'\n",
    "IND_MOMENT = 0  # sync with retrieval program otherwise it's buggy\n",
    "# minor inputs\n",
    "GT_JSON = '../data/raw/val_data.json'\n",
    "NOUNS_TO_VIDEO_DIDEMO_JSON = '../data/interim/didemo/nouns_to_video.json'\n",
    "SUBSET = 'val'\n",
    "OVERLAP = 1e-32\n",
    "TOPK = 10  # for REST\n",
    "\n",
    "# Create mapping btw instance and NOUNs and viceversa\n",
    "with open(NOUNS_TO_VIDEO_DIDEMO_JSON, 'r') as fid:\n",
    "    nouns_to_instance_list = json.load(fid)['annotations_per_subset'][SUBSET]\n",
    "    nouns_to_instances = {k: set(v) for k, v in nouns_to_instance_list.items()}\n",
    "\n",
    "# Load vocabulary and create mapping of underrepresented and unseen instances\n",
    "# and the nouns on them.\n",
    "with open(YFCC100_JSON, 'r') as fid:\n",
    "    # get under/unseen nouns used for image collection\n",
    "    underrepresented_unseen_nouns = set(\n",
    "        pd.read_csv(YFCC100_CSV)['topk_tags'].unique().tolist())\n",
    "    \n",
    "    # get nouns seens during training\n",
    "    image_nouns = set()\n",
    "    for _, moment_i in enumerate(json.load(fid)):\n",
    "        image_nouns.update(\n",
    "            moment_i['description'].split(' '))\n",
    "\n",
    "    # map instances to untargeted nouns \n",
    "    untargeted_nouns = {}\n",
    "    for noun_i in image_nouns:\n",
    "        if (noun_i in underrepresented_unseen_nouns or\n",
    "            noun_i not in nouns_to_instances):\n",
    "            continue\n",
    "        for instance_id in nouns_to_instances[noun_i]:\n",
    "            if instance_id not in untargeted_nouns:\n",
    "                untargeted_nouns[instance_id] = [noun_i]\n",
    "            else:\n",
    "                untargeted_nouns[instance_id].append(noun_i)\n",
    "\n",
    "    # map instances to under/unseen subset\n",
    "    underrepresented_unseen = {}\n",
    "    underrepresented_unseen_nouns2id = {\n",
    "        i: [] for i in underrepresented_unseen_nouns}\n",
    "    for noun_i in underrepresented_unseen_nouns:\n",
    "        if noun_i not in nouns_to_instances:\n",
    "            continue\n",
    "        for instance_id in nouns_to_instances[noun_i]:\n",
    "            if instance_id not in underrepresented_unseen:\n",
    "                underrepresented_unseen[instance_id] = [noun_i]\n",
    "            else:\n",
    "                underrepresented_unseen[instance_id].append(noun_i)\n",
    "            underrepresented_unseen_nouns2id[noun_i].append(instance_id)\n",
    "    \n",
    "# mapping annotation_id to index\n",
    "id2ind, ind2id = {}, {}\n",
    "all_moments = []\n",
    "with open(GT_JSON, 'r') as fid:  \n",
    "    for i, moment_i in enumerate(json.load(fid)):\n",
    "        id2ind[moment_i['annotation_id']] = i\n",
    "        ind2id[i] = moment_i['annotation_id']\n",
    "        all_moments.append(moment_i)\n",
    "    ind_underrepresented_unseen = [id2ind[i]\n",
    "                                   for i in underrepresented_unseen]\n",
    "    underrepresented_unseen_nouns2ind = {\n",
    "        k: [id2ind[i] for i in v]\n",
    "        for k, v in underrepresented_unseen_nouns2id.items()\n",
    "    }\n",
    "    ind_complement = np.setdiff1d(np.arange(len(all_moments)),\n",
    "                                  ind_underrepresented_unseen)\n",
    "\n",
    "    # collect indices of phrases where visual moments overlap in time\n",
    "    ind_blockout = None\n",
    "    if OVERLAP > 0:\n",
    "        _ind_blockout_i, _ind_blockout_j = [], []\n",
    "        all_moments_df = pd.DataFrame(all_moments)\n",
    "        for _, moments_per_video in all_moments_df.groupby('video'):\n",
    "            moments_time = np.array(\n",
    "                moments_per_video['times'].apply(\n",
    "                    lambda x: x[IND_MOMENT]).tolist())\n",
    "            # make it continuous\n",
    "            moments_time *= 1 \n",
    "            moments_time[:, 1] += 1\n",
    "            iou_among_moments = iou_op(moments_time, moments_time)\n",
    "            # remove the yo-con-yo, busque pareja mi llave!\n",
    "            np.fill_diagonal(iou_among_moments, 0)\n",
    "            ind_overlap_i, ind_overlap_j = np.where(\n",
    "                iou_among_moments >= OVERLAP)\n",
    "            _ind_blockout_i.append(moments_per_video.index[ind_overlap_i])\n",
    "            _ind_blockout_j.append(moments_per_video.index[ind_overlap_j])\n",
    "        ind_blockout = (np.concatenate(_ind_blockout_i),\n",
    "                        np.concatenate(_ind_blockout_j))\n",
    "\n",
    "phrases_length = [len(moment_i['description']) for moment_i in all_moments]\n",
    "phrases_rank, sorted_ind_matrix = compute_phrase_rank(FILENAME, ind_blockout)\n",
    "phrases_rank_ref, _ = compute_phrase_rank(FILENAME_REF, ind_blockout)\n",
    "rank_diff = phrases_rank - phrases_rank_ref  # lower is better\n",
    "\n",
    "phrases_rank_underrepresented_unseen = (\n",
    "    phrases_rank[ind_underrepresented_unseen])\n",
    "phrases_rank_complement = phrases_rank[ind_complement]\n",
    "\n",
    "# TODO (critical): compute auc of recall vs rank-k\n",
    "\n",
    "# Display results\n",
    "print('Median (overall/under+unseeen/comeplement)')\n",
    "print(np.median(phrases_rank))\n",
    "print(np.median(phrases_rank_underrepresented_unseen))\n",
    "print(np.median(phrases_rank_complement))\n",
    "print('Average (overall/under+unseeen/comeplement)')\n",
    "print(np.mean(phrases_rank))\n",
    "print(np.mean(phrases_rank_underrepresented_unseen))\n",
    "print(np.mean(phrases_rank_complement))\n",
    "print('Std (overall/under+unseeen/comeplement)')\n",
    "print(np.std(phrases_rank))\n",
    "print(np.std(phrases_rank_underrepresented_unseen))\n",
    "print(np.std(phrases_rank_complement))\n",
    "print('Rank diff wrt reference +/-/None')\n",
    "print(f'{(rank_diff < 0).sum() / len(rank_diff):.4f}')\n",
    "print(f'{(rank_diff > 0).sum() / len(rank_diff):.4f}')\n",
    "print(f'{(rank_diff == 0).sum() / len(rank_diff):.4f}')\n",
    "print('Rank diff under+unseen wrt reference +/-/None')\n",
    "print(f'{(rank_diff[ind_underrepresented_unseen] < 0).sum() / len(ind_underrepresented_unseen):.4f}')\n",
    "print(f'{(rank_diff[ind_underrepresented_unseen] > 0).sum() / len(ind_underrepresented_unseen):.4f}')\n",
    "print(f'{(rank_diff[ind_underrepresented_unseen] == 0).sum() / len(ind_underrepresented_unseen):.4f}')\n",
    "print('Number instances with under+unseen')\n",
    "print(f'{len(ind_underrepresented_unseen)}')\n",
    "\n",
    "underrepresented_unseen_nouns_improvement_rate = {\n",
    "    k: (rank_diff[v] < 0).sum() / len(v)\n",
    "    for k, v in underrepresented_unseen_nouns2ind.items()\n",
    "}\n",
    "\n",
    "# Dump CSV to plot in G-sheets\n",
    "# Format rank and length together\n",
    "underrepresented_unseen_nouns_improvement_rate_df = pd.DataFrame.from_dict(\n",
    "    underrepresented_unseen_nouns_improvement_rate, orient='index')\n",
    "summary_file = FILENAME.replace('phrase_retrieval.h5', 'rate_pos-rank-diff.csv')\n",
    "with open(summary_file, 'x') as fid:\n",
    "    underrepresented_unseen_nouns_improvement_rate_df.to_csv(fid)\n",
    "\n",
    "# Dump CSV to plot in G-sheets\n",
    "# Format rank and length together\n",
    "rank_vs_length = pd.DataFrame([phrases_rank, phrases_length]).T\n",
    "rank_vs_length.columns = ['rank', 'length']\n",
    "# summary_file = 'blah'\n",
    "# with open(summary_file, 'x') as fid:\n",
    "#     rank_vs_length.to_csv(fid, index=None)\n",
    "\n",
    "# Dump REST to explore results\n",
    "summary = {}\n",
    "for i in range(len(all_moments)):\n",
    "    annotation_id = all_moments[i]['annotation_id']\n",
    "    summary[annotation_id] = {\n",
    "        'description': all_moments[i]['description'],\n",
    "        # this format the video such that we can plug it in in HTML\n",
    "        'video': '/'.join(all_moments[i]['video'].split('_')[:2]),\n",
    "        'time': all_moments[i]['times'][IND_MOMENT],\n",
    "        'rank': int(phrases_rank[i]),\n",
    "        'topk': [all_moments[j]['description']\n",
    "                 for j in sorted_ind_matrix[i, :TOPK]],\n",
    "        # TODO: add nouns from underrepresented_unseen\n",
    "        'untargeted_nouns': [i for i in untargeted_nouns.get(annotation_id) or []],\n",
    "        'noun_subset': [i for i in underrepresented_unseen.get(annotation_id) or []],\n",
    "        'rank_diff': int(rank_diff[i]),\n",
    "    }\n",
    "summary_file = FILENAME.replace('phrase_retrieval.h5', 'pr_rest.json')\n",
    "with open(summary_file, 'x') as fid:\n",
    "    json.dump(summary, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe, soft-evaluation it's the way to go. At least for `cyclist` and `diver`, it looks the rank difference of the image-only model is better.\n",
    "\n",
    "Double check the following cases. It seems, there were POS errors: \n",
    "    - Cyclists racing.\n",
    "    - first cyclist exits frame left\n",
    "    - the first cyclist rides over the triangle on the road\n",
    "    - diver first comes into view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment retrieval evaluation\n",
    "\n",
    "Updated: Aug 23. Created: Aug 16\n",
    "\n",
    "Study moment retrieval from descriptions.\n",
    "\n",
    "_Note_:\n",
    "\n",
    "- We move the evaluation code to the respectinve `evaluation.py` and `corpus.py` modules.\n",
    "\n",
    "- TODO: dump data for REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[experiments] computing numbers for meeting on Aug. 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from evaluation import CorpusVideoMomentRetrievalEvalFromMatrix\n",
    "\n",
    "h5_files = [\n",
    "    '../data/interim/smcn_10/a/4_moment_retrieval.h5',\n",
    "    '../data/interim/smcn_12/a/5_moment_retrieval.h5',\n",
    "    '../data/interim/hsmcn_10/3_moment_retrieval.h5',\n",
    "    '../data/interim/hsmcn_07/9_moment_retrieval.h5',\n",
    "    '../data/interim/smcn_06/a/4_moment_retrieval.h5',\n",
    "    '../data/interim/mcn/_corpus_val_rgb_matrix.hdf5',\n",
    "]\n",
    "tags = [\n",
    "    'SMCN OnlyVideo-Inter+Intra-Local',\n",
    "    'SMCN OnlyVideo-Inter-Local',\n",
    "    'HSMCN Joint-Inter+Intra-Local',\n",
    "    'HSMCN OnlyImage-Inter-Local',\n",
    "    'SMCN OnlyVideo-Inter+Intra-Local+Global+TEF',\n",
    "    'MCN OnlyVideo-Inter+Intra-Local+Global+TEF'\n",
    "]\n",
    "assert len(h5_files) == len(tags)\n",
    "\n",
    "RECALL_VALUES = (1, 5, 10, 100, 1000, 2000, 10000)\n",
    "json_filename = '../data/raw/val_data_wwa.json'\n",
    "rows = []\n",
    "for i, h5_filename in enumerate(h5_files):\n",
    "    judge = CorpusVideoMomentRetrievalEvalFromMatrix(\n",
    "        json_filename, h5_filename, RECALL_VALUES, 0.1)\n",
    "    recall, mrank = judge.eval()\n",
    "    rows.append([tags[i]] + recall + [mrank])\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.columns = ['model'] + [f'R@{i}' for i in RECALL_VALUES] + ['mean-rank']\n",
    "df.to_csv('2018-08-21.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[experiments] computing numbers for meeting on Aug. 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from evaluation import CorpusVideoMomentRetrievalEvalFromMatrix\n",
    "\n",
    "h5_files = [\n",
    "    '../data/interim/smcn_13/3_moment_retrieval.h5',\n",
    "    '../data/interim/mcn_pytorch_12/3_moment_retrieval.h5',\n",
    "]\n",
    "tags = [\n",
    "    'SMCN OnlyVideo-Inter+Intra-Local+Global',\n",
    "    'MCN OnlyVideo-Inter+Intra-ResNet'\n",
    "]\n",
    "assert len(h5_files) == len(tags)\n",
    "\n",
    "RECALL_VALUES = (1, 5, 10, 100, 1000, 2000, 10000)\n",
    "json_filename = '../data/raw/val_data_wwa.json'\n",
    "rows = []\n",
    "for i, h5_filename in enumerate(h5_files):\n",
    "    judge = CorpusVideoMomentRetrievalEvalFromMatrix(\n",
    "        json_filename, h5_filename, RECALL_VALUES, 0.1)\n",
    "    recall, mrank = judge.eval()\n",
    "    rows.append([tags[i]] + recall + [mrank])\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.columns = ['model'] + [f'R@{i}' for i in RECALL_VALUES] + ['mean-rank']\n",
    "df.to_csv('2018-08-23.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[debug] retrieval with corpus matrix\n",
    "\n",
    "- compute distance matrix of original MCN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def video_to_iid(video):\n",
    "    # return video integer id\n",
    "    return int(hashlib.sha256(video.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "\n",
    "_filename1 = '../data/interim/mcn/corpus_val_rgb.hdf5'\n",
    "_filename2 = '../data/interim/mcn/queries_val_rgb.hdf5'\n",
    "with h5py.File(_filename1, 'r')as f:\n",
    "    visual_features = []\n",
    "    videos_order = []\n",
    "    for i, (k, v) in enumerate(f.items()):\n",
    "        visual_features.append(v[:])\n",
    "        videos_order.append((i, video_to_iid(k)))\n",
    "    visual_features = np.concatenate(visual_features)\n",
    "    \n",
    "with h5py.File(_filename2, 'r')as f:\n",
    "    queries_features = []\n",
    "    moments_order = []\n",
    "    for i, (k, v) in enumerate(f.items()):\n",
    "        queries_features.append(v[:].reshape((1, -1)))\n",
    "        moments_order.append((i, int(k)))\n",
    "    queries_features = np.concatenate(queries_features)\n",
    "    \n",
    "prediction_matrix = cdist(queries_features, visual_features, 'sqeuclidean')\n",
    "\n",
    "with h5py.File('../data/interim/mcn/_corpus_val_rgb_matrix.hdf5', 'x') as fid:\n",
    "        fid['prediction_matrix'] = prediction_matrix\n",
    "        fid['similarity'] = False\n",
    "        fid['_video_index'] = np.array(videos_order)\n",
    "        fid['_moments_index'] = np.array(moments_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit-test to double check moment retrieval with distance matrix vs previous approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import h5py\n",
    "from evaluation import RetrievalEvaluation\n",
    "from evaluation import CorpusVideoMomentRetrievalEvalFromMatrix\n",
    "\n",
    "_filename1 = '../data/interim/mcn/corpus_val_rgb.hdf5'\n",
    "_filename2 = '../data/raw/val_data_wwa.json'\n",
    "_judge = RetrievalEvaluation(_filename1, _filename2, (1, 5, 10), 0.1)\n",
    "_filename = '../data/interim/mcn/queries_val_rgb.hdf5'\n",
    "\n",
    "with h5py.File(_filename, 'r') as fid:\n",
    "    for _sample_key, h5ds in fid.items():\n",
    "        _query_id = int(_sample_key)\n",
    "        _query_vector = h5ds[:]\n",
    "        _judge.eval_single_vector(_query_vector, _query_id)\n",
    "    _performace = _judge.eval(full=True)\n",
    "    print('R@{0:}={2:};\\nR@{0:},{1:}={3:};\\nR@{0:},didemo={4:};\\n'\n",
    "          'mIOU={5:.4f};\\nmRank={6:.2f};'\n",
    "          .format(_judge.k, _judge.iou_threshold,\n",
    "                  *_performace))\n",
    "    \n",
    "_h5_filename = '../data/interim/mcn/_corpus_val_rgb_matrix.hdf5'\n",
    "_json_filename = '../data/raw/val_data_wwa.json'\n",
    "_judge2 = CorpusVideoMomentRetrievalEvalFromMatrix(\n",
    "    _json_filename, _h5_filename, (1, 5, 10), 0.1)\n",
    "vale_mine = _judge2.eval(True)\n",
    "print('R@{0:}={2:};\\nR@{0:},{1:}={3:};\\nR@{0:},didemo={4:};\\n'\n",
    "      'mIOU={5:.4f};\\nmRank={6:.2f};'\n",
    "      .format(_judge.k, _judge.iou_threshold,\n",
    "              *vale_mine))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
