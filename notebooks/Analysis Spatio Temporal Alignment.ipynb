{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for visualization of Chamfer Distance alignmet between objects/clips and language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs to be dumped from the _eval_item function in dataset untrimmed and from the chamfer distance class. <br>\n",
    "In particular we will try to trace back the mapping between each word and the relative object/scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soldanm/anaconda3/envs/moments-retrieval-devel/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import time\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from utils import LanguageRepresentationMCN_glove, compare_glove, sentences_to_words\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 9)        # small images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 89.35 seconds.\n",
      "(300,)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "## Initialization glove\n",
    "t = time.time()\n",
    "lang_interface = LanguageRepresentationMCN_glove(max_words=1)\n",
    "print('Done in {:.2f} seconds.'.format(time.time()-t))\n",
    "## Test GLove\n",
    "word = 'hello'\n",
    "feature = lang_interface(word)\n",
    "print(feature.shape)\n",
    "print(lang_interface.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load obj vocab\n",
    "classes_VG = []\n",
    "classes_file = '../data/raw/language/visual_genome/objects_vocab.txt'\n",
    "with open(classes_file, 'r') as f:\n",
    "    for object in f.readlines():\n",
    "        classes_VG.append(object.split(',')[0].lower().strip())\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique codewords: 1078\n"
     ]
    }
   ],
   "source": [
    "classes_VG_revisited = []\n",
    "map_glove_word = []\n",
    "for c in classes_VG:\n",
    "    v = lang_interface(c)\n",
    "    if np.abs(v).sum() != 0.0:\n",
    "        map_glove_word.append(v)\n",
    "        classes_VG_revisited.append(c)   \n",
    "\n",
    "print('Number of unique codewords: {}'.format(len(map_glove_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_VG_dict = {c:lang_interface(c) for c in classes_VG if np.abs(lang_interface(c)).sum()!=0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos in split 1037\n",
      "Tot number of videos in dataset: 10642\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary images from the one I downloaded\n",
    "filename = '../data/processed/didemo/test-01.json'\n",
    "data     = json.load(open(f'{filename}','r'))\n",
    "moments  = data['moments']\n",
    "videos   = list(set([m['video'] for m in moments]))\n",
    "print('Number of videos in split {}'.format(len(videos)))\n",
    "frames   = os.listdir('../data/interim/matching_evaluation/didemo_frames/')\n",
    "print('Tot number of videos in dataset: {}'.format(len(frames)))\n",
    "not_     = [f for f in frames if f not in videos ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 3.36 s\n"
     ]
    }
   ],
   "source": [
    "# Read dumped data:\n",
    "t = time.time()\n",
    "path = '../data/interim/matching_evaluation/dump_done/eval_items/'\n",
    "files = sorted(os.listdir(path))[1:]\n",
    "dumped_data_input = {}\n",
    "for f in files:\n",
    "    d   = json.load(open(f'{path}{f}','r'))\n",
    "    idx = list(d.keys())[0]\n",
    "    \n",
    "    # Convert back to numpy\n",
    "    d[idx]['times']     = np.asarray(d[idx]['times'])\n",
    "    d[idx]['proposals'] = np.asarray(d[idx]['proposals'])\n",
    "    d[idx]['feat']      = {k:np.asarray(v) for k,v in d[idx]['feat'].items()}\n",
    "    \n",
    "    # Store in variable for later use - aggregate all information\n",
    "    dumped_data_input[int(idx)] = d[idx]\n",
    "print('Done in {:.2f} s'.format(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.02 s\n"
     ]
    }
   ],
   "source": [
    "# Read dumped data:\n",
    "t = time.time()\n",
    "path  = '../data/interim/matching_evaluation/dump_done/chamfer_distance/'\n",
    "files = sorted(os.listdir(path))\n",
    "idx   = 0\n",
    "dumped_data_chamfer = {}\n",
    "for i in range(0,len(files),2):\n",
    "    d_rgb = np.load(f'{path}{i:04d}.npz')\n",
    "    d_obj = np.load(f'{path}{i+1:04d}.npz')\n",
    "    dumped_data_chamfer[idx]= {'rgb': d_rgb['arr_0'], \n",
    "                               'obj': d_obj['arr_0']}    \n",
    "    idx += 1\n",
    "print('Done in {:.2f} s'.format(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 12, 50)\n",
      "(21, 120, 50)\n",
      "dict_keys(['description', 'times', 'video', 'annotation_id', 'annotation_id_original', 'time', 'video_index', 'proposals', 'len_query', 'feat'])\n",
      "[[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[ 0.  5.]\n",
      " [ 5. 10.]\n",
      " [10. 15.]\n",
      " [15. 20.]\n",
      " [20. 25.]\n",
      " [25. 30.]\n",
      " [ 0. 10.]\n",
      " [ 0. 15.]\n",
      " [ 0. 20.]\n",
      " [ 0. 25.]\n",
      " [ 0. 30.]\n",
      " [ 5. 15.]\n",
      " [ 5. 20.]\n",
      " [ 5. 25.]\n",
      " [ 5. 30.]\n",
      " [10. 20.]\n",
      " [10. 25.]\n",
      " [10. 30.]\n",
      " [15. 25.]\n",
      " [15. 30.]\n",
      " [20. 30.]]\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(dumped_data_chamfer[0]['rgb'].shape)\n",
    "# print(dumped_data_chamfer[0]['rgb'][0])\n",
    "print(dumped_data_chamfer[0]['obj'].shape)\n",
    "print(dumped_data_input[0].keys())\n",
    "print(dumped_data_input[0]['feat']['mask-rgb'])\n",
    "print(dumped_data_input[0]['proposals'])\n",
    "print(dumped_data_input[0]['len_query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Read mapping between clips and object classes\n",
    "input_file = '../data/processed/didemo/obj_classes_per_clip.json'\n",
    "mapping_obj_with_boxes = json.load(open(input_file,'r'))\n",
    "\n",
    "keys_ = list(mapping_obj_with_boxes.keys())\n",
    "mapping_obj = {}\n",
    "for k in keys_:\n",
    "    clip_keys_ = list(mapping_obj_with_boxes[k].keys())\n",
    "    clip_dict_ = {}\n",
    "    for ck in clip_keys_:\n",
    "        clip_list_ = mapping_obj_with_boxes[k][ck]\n",
    "        reduced_list = [elem[0] for elem in clip_list_]\n",
    "        clip_dict_[ck] = reduced_list\n",
    "    mapping_obj[k] = clip_dict_\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisys and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables:\n",
    "P_size = 21                  # number of proposals\n",
    "clip_size = 2.5              # clip size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute objects in moments\n",
    "def _compute_obj_in_moment(objects,loc):    \n",
    "    try:\n",
    "        moment_obj = []\n",
    "        im_start = int(int(loc[0]) // clip_size)\n",
    "        im_end   = int(int(loc[1]) // clip_size)\n",
    "        for i in range(im_start,im_end):\n",
    "            for obj in objects[str(i)]:\n",
    "                moment_obj.append([obj,i])\n",
    "    except:\n",
    "        return []\n",
    "    return moment_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from language to video\n",
    "def _mapping_tokens_to_video(tokens, pred_obj, reduced_pairwise,k):\n",
    "    proposal_mapping_lang_to_obj = {key:[] for key in tokens}\n",
    "    for ii,word in enumerate(tokens):\n",
    "        if len(reduced_pairwise[:ii]) > 0:\n",
    "            ind_  = int(np.argmin(reduced_pairwise[:,ii]))       # per ogni parola della descrizione prendo l'indice dell'oggetto cui distanza dell'embedding e minore.\n",
    "            score = float(reduced_pairwise[ind_,ii])                    # prendo anche lo score\n",
    "            if k == 'obj':\n",
    "                obj = pred_obj[ind_]\n",
    "                proposal_mapping_lang_to_obj[word].append({\"object\": obj[0],\n",
    "                                                           \"obj_idx\":ind_,\n",
    "                                                           \"score\":  score,\n",
    "                                                           \"frame\": obj[1]})\n",
    "            else:\n",
    "                proposal_mapping_lang_to_obj[word].append({\"clip_name\":f'clip_{ind_}',\n",
    "                                                           \"clip_idx:\":ind_,\n",
    "                                                           \"score\":    score})\n",
    "    return proposal_mapping_lang_to_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from video to language\n",
    "def _mapping_video_to_tokens(tokens, pred_obj, reduced_pairwise, k, len_video):\n",
    "    if k=='obj':\n",
    "        proposal_mapping_obj_to_lang = {}\n",
    "        for i in range(len(pred_obj)):\n",
    "            proposal_mapping_obj_to_lang[pred_obj[i][0]] = [] \n",
    "        for ii, origin in enumerate(pred_obj):\n",
    "            ind_     = int(np.argmin(reduced_pairwise[ii,:]))\n",
    "            score    = float(reduced_pairwise[ii,ind_])\n",
    "            token    = tokens[ind_]\n",
    "            proposal_mapping_obj_to_lang[origin[0]].append({\"token\":  token,\n",
    "                                                            \"obj_idx\":ind_,\n",
    "                                                            \"score\":  score,\n",
    "                                                            \"frame\":  origin[1]})\n",
    "    else:\n",
    "        proposal_mapping_obj_to_lang = {'clip_{}'.format(i):[] for i in range(len_video)}\n",
    "        for ii in range(len_video):\n",
    "            ind_     = int(np.argmin(reduced_pairwise[ii,:]))\n",
    "            score    = float(reduced_pairwise[ii,ind_])\n",
    "            token    = tokens[ind_]\n",
    "            origin   = f'clip_{ii}'\n",
    "            proposal_mapping_obj_to_lang[origin].append({\"token\":   token,\n",
    "                                                         \"clip_idx\":ind_,\n",
    "                                                         \"score\":   score})\n",
    "    return proposal_mapping_obj_to_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bae4357c528a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdump_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# annotations information:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0melem\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mdumped_data_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdump_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_id\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpairwise\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mdumped_data_chamfer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdump_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "# Ciclo sui proposals\n",
    "\n",
    "keys = list(dumped_data_chamfer.keys())\n",
    "\n",
    "annotations_data = {}\n",
    "for dump_id in tqdm.tqdm(keys):\n",
    "    # annotations information:\n",
    "    elem        = dumped_data_input[dump_id]\n",
    "    video_id    = elem['video']\n",
    "    pairwise    = dumped_data_chamfer[dump_id]\n",
    "    description = elem['description']\n",
    "    proposals   = elem['proposals']\n",
    "    tokens      = sentences_to_words(description)\n",
    "    # Tiene l'informazione di ogni proposal nel video\n",
    "    proposals_mapping = {i:{} for i in range(P_size)}                 # per ogni proposal appendi 2 dizionari ogniuno con il mapping di ogni parola con ogni oggetto e viceversa\n",
    "    \n",
    "    for i, moment in enumerate(proposals):\n",
    "        moment   = [str(int(e)) for e in moment] \n",
    "        pred_obj = _compute_obj_in_moment(mapping_obj[video_id], moment)\n",
    "        if len(pred_obj) > 0:\n",
    "            single_proposal_mapping={'lang_to_rgb':None, 'rgb_to_lang':None,\n",
    "                                     'lang_to_obj':None, 'obj_to_lang':None}\n",
    "            for k in ['rgb','obj']:\n",
    "                input_feat  = elem['feat'][k][i]\n",
    "                input_mask  = elem['feat']['-'.join(['mask',k])][i]\n",
    "                feat_paiwise= pairwise[k][i]\n",
    "                len_video   = int(np.sum(input_mask))\n",
    "                len_lang    = elem['len_query'][i]\n",
    "\n",
    "                # Only take as many elements as the GT langugage and video info\n",
    "                reduced_pairwise = feat_paiwise[:len_video, :len_lang]\n",
    "\n",
    "                single_proposal_mapping['lang_to_{}'.format(k)] = _mapping_tokens_to_video(tokens, pred_obj, reduced_pairwise,k)\n",
    "                single_proposal_mapping['{}_to_lang'.format(k)] = _mapping_video_to_tokens(tokens, pred_obj, reduced_pairwise,k, len_video)\n",
    "\n",
    "            # Append whole information in proposal summary\n",
    "            proposals_mapping[i] = single_proposal_mapping           \n",
    "    \n",
    "    # Append all proposals informations to annotations.\n",
    "    annotations_data[dump_id] = proposals_mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump data, let's try the fucking visualization:\n",
    "filename = '../data/interim/matching_evaluation/matchings.json'\n",
    "with open(filename,'w') as f:\n",
    "    json.dump(annotations_data,f)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read mapping between clips and object classes\n",
    "input_file = '../data/processed/didemo/obj_classes_per_clip.json'\n",
    "mapping_obj_with_boxes = json.load(open(input_file,'r'))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_keys_ = list(annotations_data.keys())\n",
    "print(annotation_keys_[0])\n",
    "elem        = dumped_data_input[annotation_keys_[0]]\n",
    "video_id    = elem['video']\n",
    "print(elem.keys())\n",
    "print(video_id)\n",
    "print(mapping_obj_with_boxes[video_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grande ciclo per plottare tutto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data/processed/didemo/obj_detection/visual_genome/'\n",
    "raw_predictions_file = f'{root}didemo_raw_obj_detection.json'\n",
    "data = json.load(open(raw_predictions_file, 'r'))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def _get_frames_indices(k):\n",
    "    path = '../data/interim/matching_evaluation/didemo_frames/{}/'.format(video_id)\n",
    "    frame_keys = list(data[k].keys())                # get list of frames for video\n",
    "    num_frames = len(frame_keys)                     # compute number of frames\n",
    "    num_windows = math.ceil(num_frames/clip_size)    # compute number of clips\n",
    "    idx = [i for i in range(num_frames)]             # ancillary indexes variable\n",
    "    selected_idx = sorted([1] + idx[3::int(clip_size*2)] + idx[6::int(clip_size*2)])   # Select best indexes \n",
    "    selected_frames = [frame_keys[i] for i in selected_idx]   # distill the wanted frames\n",
    "    selected_frames = ['{}{}'.format(path,f) for f in selected_frames]\n",
    "    return selected_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def _create_proposals_frames_list(list_frames, proposals):\n",
    "    proposals_frames = []\n",
    "    for loc in proposals:\n",
    "        im_start = int(int(loc[0]) // clip_size)\n",
    "        im_end   = int(int(loc[1]) // clip_size)\n",
    "        tmp      = [list_frames[i] for i in range(im_start,im_end)]\n",
    "        proposals_frames.append(tmp)\n",
    "    return proposals_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def _gather_obj_info_per_proposal(bbox_per_video, proposals):\n",
    "    proposals_objects = []\n",
    "    for loc in proposals:\n",
    "        im_start = int(int(loc[0]) // clip_size)\n",
    "        im_end   = int(int(loc[1]) // clip_size)\n",
    "        tmp      = [bbox_per_video[str(i)] for i in range(im_start,im_end)]\n",
    "        tmp      = [item for sublist in tmp for item in sublist]\n",
    "        proposals_objects.append(tmp)\n",
    "    return proposals_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mapping_info_per_proposal(mapping_, proposals):\n",
    "    return [mapping_[i] for i in range(len(proposals))]                       # Mapping is already computed for proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inverse_map_box(box, height, width):\n",
    "    c1,c2,w,h = box\n",
    "    return [(c1-w)*width,(c2-h)*height,(c1+w)*width,(c2+h)*height ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_box(frames_objects,proposals_mapping, height, width, ii):\n",
    "    map_ = proposals_mapping['lang_to_obj']\n",
    "    bboxes_ = []\n",
    "    names   = []\n",
    "    for token in map_.keys():\n",
    "        data_ = map_[token]\n",
    "        if len(data_) ==0:\n",
    "            obj_index = 0\n",
    "            box = _inverse_map_box(frames_objects[obj_index][1:], height, width)\n",
    "            bboxes_.append(box)\n",
    "            names.append(frames_objects[obj_index][0])\n",
    "#             print(token, map_[token], frames_objects[0])\n",
    "        else:\n",
    "            obj_index = map_[token][0]['obj_idx']\n",
    "            frame_idx = map_[token][0]['frame']\n",
    "            if frame_idx == ii:\n",
    "                box = _inverse_map_box(frames_objects[obj_index][1:], height, width)\n",
    "                bboxes_.append(box)\n",
    "                names.append(frames_objects[obj_index][0])\n",
    "#                 print(token, map_[token], frames_objects[obj_index], ii,frame_idx)\n",
    "    return names, bboxes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dump = '../data/interim/matching_evaluation/images/'\n",
    "def _plot_proposals(video_id,description, proposals_frames, proposals_objects, proposals_mapping):\n",
    "    print(description)\n",
    "    folder = '{}{}/'.format(path_dump,video_id)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    for i, frames in enumerate(proposals_frames):\n",
    "        frames_objects = proposals_objects[i]\n",
    "        frames_mapping = proposals_mapping[i]\n",
    "        my_dpi=1000\n",
    "        fig = plt.figure()\n",
    "        DPI = fig.get_dpi()\n",
    "        fig.set_size_inches(len(frames)*320/float(DPI),2*240.0/float(DPI))# figsize=(len(frames)*3,len(frames)/2))\n",
    "#         plt.title(description, fontsize=20)\n",
    "        plt.axis('off')\n",
    "#         fig.tight_layout()\n",
    "        fig.subplots_adjust(left=None, bottom=0.0, right=None, top=None, wspace=0.01, hspace=None)\n",
    "        for ii, frame in enumerate(frames):\n",
    "            im = cv2.imread(frame)\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            height, width, _ = im.shape\n",
    "            \n",
    "            ax=fig.add_subplot(1,len(frames),  ii + 1)\n",
    "            ax.axis('off')\n",
    "            ax.imshow(im)\n",
    "         \n",
    "            names, bboxes = _compute_box(frames_objects,frames_mapping, height, width, ii)\n",
    "            for n,bbox in zip(names,bboxes):\n",
    "                ax.add_patch(patches.Rectangle((bbox[0], bbox[1]),\n",
    "                            bbox[2] - bbox[0],\n",
    "                            bbox[3] - bbox[1], fill=False,\n",
    "                            edgecolor='red', linewidth=3, alpha=0.5))\n",
    "                ax.text(bbox[0], bbox[1] - 2,\n",
    "                            '%s' % (n),\n",
    "                            bbox=dict(facecolor='blue', alpha=0.1),\n",
    "                            fontsize=10, color='white')\n",
    "        \n",
    "        f_name = frame.split('/')[-1]\n",
    "        dump_path = '{}{}'.format(folder,i)\n",
    "        plt.savefig(dump_path,bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciclo su tt le chiavi di annotation_keys_\n",
    "idx_=4\n",
    "for annotation_index in annotation_keys_[idx_:idx_+1]: \n",
    "    video_id          = dumped_data_input[annotation_index]['video']           #2 Get the video id\n",
    "    description       = dumped_data_input[annotation_index]['description']\n",
    "    list_frames       = _get_frames_indices(video_id)                          #3 Leggo i frame che sono contenuti nella cartella:\n",
    "    proposals         =  dumped_data_input[annotation_index]['proposals']\n",
    "    proposals_frames  = _create_proposals_frames_list(list_frames, proposals)  #4 Frames per proposal - moment\n",
    "    bbox_per_video    = mapping_obj_with_boxes[video_id]   #5 Get BBox information\n",
    "    proposals_objects = _gather_obj_info_per_proposal(bbox_per_video,proposals)\n",
    "    mapping_          = annotations_data[annotation_index]                     #6 Get mapping between everything\n",
    "    proposals_mapping = _get_mapping_info_per_proposal(mapping_, proposals)\n",
    "\n",
    "    # Important variables = proposals_frames, proposals_objects, proposals_mapping\n",
    "    print(dumped_data_input[annotation_index]['times'])\n",
    "    _plot_proposals(video_id, description, proposals_frames,proposals_objects,proposals_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_frames[0])\n",
    "im = cv2.imread(list_frames[0])\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(im)\n",
    "\n",
    "dump_path = '../data/interim/matching_evaluation/images/test.png'\n",
    "plt.savefig(dump_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
