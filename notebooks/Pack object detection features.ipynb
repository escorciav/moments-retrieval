{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is used to merge the features extracted by the objects detector. We will move the format from frame predictions to clip predictions. \n",
    "\n",
    "## These are the features to be used in the notebook *Preprocessing obj detection for training*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiDeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import math \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "CLIP_SIZE = 2.5 \n",
    "FPS = 1\n",
    "root = '../data/processed/didemo/obj_detection/visual_genome/'\n",
    "raw_predictions_file = f'{root}didemo_raw_obj_detection.json'\n",
    "data = json.load(open(raw_predictions_file, 'r'))\n",
    "\n",
    "\n",
    "perc=50\n",
    "thresholds_file = f'{root}didemo_thresholds_percentile_{perc}.json'\n",
    "thresholds = json.load(open(thresholds_file,'r'))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_features(boxes,image_w,image_h):\n",
    "    normalizes_boxes = [[bb[0]/image_w, bb[1]/image_h, bb[2]/image_w, bb[3]/image_h ] for bb in boxes]\n",
    "    \n",
    "    features = []\n",
    "    for bb in normalizes_boxes:\n",
    "        center = [(bb[0]+bb[2])/2,(bb[1]+bb[3])/2]\n",
    "        width  = [(bb[2]-bb[0])/2]\n",
    "        heigth = [(bb[3]-bb[1])/2]\n",
    "        features.append(center+width+heigth)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom threshold to each predicted objects \n",
    "def threshold_predictions(data, thresholds):\n",
    "    obj_classes = data['objects']\n",
    "    obj_conf_score = data['obj_conf']\n",
    "    boxes = data['boxes']\n",
    "    image_w = data['image_w']\n",
    "    image_h = data['image_h']\n",
    "    boxes_feat = boxes_features(boxes,image_w,image_h)\n",
    "    w_obj_pred = []\n",
    "    for obj,conf,bb in zip(obj_classes,obj_conf_score,boxes_feat):\n",
    "        if conf > thresholds[obj]:\n",
    "            w_obj_pred.append([obj,conf,bb])\n",
    "    return w_obj_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10642/10642 [00:06<00:00, 1631.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "post_processed_data = {}\n",
    "video_keys = list(data.keys())\n",
    "for k in tqdm.tqdm(video_keys):\n",
    "    frame_keys = list(data[k].keys())                # get list of frames for video\n",
    "    num_frames = len(frame_keys)                     # compute number of frames\n",
    "    num_windows = math.ceil(num_frames/CLIP_SIZE)    # compute number of clips\n",
    "    idx = [i for i in range(num_frames)]             # ancillary indexes variable\n",
    "    selected_idx = sorted([1] + idx[3::int(CLIP_SIZE*2)] + idx[6::int(CLIP_SIZE*2)])   # Select best indexes \n",
    "    selected_frames = [frame_keys[i] for i in selected_idx]   # distill the wanted frames\n",
    "    post_processed_data[k] = {}                 \n",
    "    for i,kk in enumerate(selected_frames):          # loop over the frames and threshold the predictions\n",
    "        post_processed_data[k][i] = threshold_predictions(data[k][kk], thresholds)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Dump data\n",
    "dump_name = f'{root}didemo_obj_detection_perc_{perc}_with_scores.json'\n",
    "with open(dump_name, \"w\") as write_file:\n",
    "            json.dump(post_processed_data, write_file)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charades-sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "CLIP_SIZE = 3\n",
    "FPS = 1\n",
    "root = '../data/processed/charades-sta/obj_detection/visual_genome/'\n",
    "raw_predictions_file = f'{root}charades_sta_raw_obj_detection.json'\n",
    "data = json.load(open(raw_predictions_file, 'r'))\n",
    "\n",
    "\n",
    "perc=50\n",
    "thresholds_file = f'{root}charades_sta_thresholds_percentile_{perc}.json'\n",
    "thresholds = json.load(open(thresholds_file,'r'))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok for charades if win dnow size 1.5 otherwise standard\n",
    "# CLIP_SIZE = 3\n",
    "# a = [i for i in range(len(frame_keys))]\n",
    "# b = sorted([0] + a[2::CLIP_SIZE] + a[3::CLIP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_boxes_charades(boxes,image_w,image_h):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define useful function\n",
    "def post_process_windod_perc_and_scores(data,i,w_size,thresholds):\n",
    "    video_keys = list(data.keys())\n",
    "    num_frames = len(video_keys)\n",
    "    w_obj_pred = []\n",
    "    for idx in range(i*w_size, min(num_frames,(i+1)*w_size)):   # for each frame in window\n",
    "        pred = data[video_keys[idx]]                            # get predictions\n",
    "        num_detections = len(pred['obj_conf'])                  # count predictions\n",
    "        for ii in range(num_detections):                        # go through predictions\n",
    "            if pred['obj_conf'][ii] > thresholds[pred['objects'][ii]]:                   # check if score is above a threshold\n",
    "                tmp = [pred['objects'][ii], \n",
    "                       pred['obj_conf'][ii],\n",
    "                       normalize_boxes_charades(pred['boxes'][ii], pred['image_w'][ii],pred['image_h'][ii])\n",
    "                      ]\n",
    "                w_obj_pred.append(tmp)          # save the prediction for objects\n",
    "    return w_obj_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [00:01<00:00, 4239.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "post_processed_data = {}\n",
    "video_keys = list(data.keys())\n",
    "for k in tqdm.tqdm(video_keys):\n",
    "    frame_keys = list(data[k].keys())                # get list of frames for video\n",
    "    num_frames = len(frame_keys)                     # compute number of frames\n",
    "    num_windows = math.ceil(num_frames/CLIP_SIZE)    # compute number of clips\n",
    "    post_processed_data[k] = {i:post_process_windod_perc_and_scores(data[k],i,CLIP_SIZE,thresholds) \n",
    "                                                    for i in range(num_windows)}\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Dump data\n",
    "dump_name = f'{root}charades_sta_obj_detection_perc_{perc}_with_scores.json'\n",
    "with open(dump_name, \"w\") as write_file:\n",
    "            json.dump(post_processed_data, write_file)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activitynet - Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "CLIP_SIZE = 2.5 \n",
    "FPS = 1\n",
    "root = '../data/processed/activitynet-captions/obj_detection/visual_genome/'\n",
    "raw_predictions_file = f'{root}activitynet_captions_raw_obj_detection.json'\n",
    "data = json.load(open(raw_predictions_file, 'r'))\n",
    "\n",
    "\n",
    "# perc=50\n",
    "# thresholds_file = f'{root}activitynet_captions_thresholds_percentile_{perc}.json'\n",
    "# thresholds = json.load(open(thresholds_file,'r'))\n",
    "th = 0.4\n",
    "thresholds = [th for _ in range(1601)]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19994/19994 [00:08<00:00, 2270.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "post_processed_data = {}\n",
    "video_keys = list(data.keys())\n",
    "for k in tqdm.tqdm(video_keys):\n",
    "    frame_keys = list(data[k].keys())                # get list of frames for video\n",
    "    num_frames = len(frame_keys)                     # compute number of frames\n",
    "    num_windows = math.ceil(num_frames/CLIP_SIZE)    # compute number of clips\n",
    "    idx = [i for i in range(num_frames)]             # ancillary indexes variable\n",
    "    if num_frames == 1:\n",
    "        selected_idx = [0]\n",
    "    else:\n",
    "        selected_idx = sorted([1] + idx[3::int(CLIP_SIZE*2)] + idx[6::int(CLIP_SIZE*2)])   # Select best indexes \n",
    "    selected_frames = [frame_keys[i] for i in selected_idx]   # distill the wanted frames\n",
    "    post_processed_data[k] = {}                 \n",
    "    for i,kk in enumerate(selected_frames):          # loop over the frames and threshold the predictions\n",
    "        post_processed_data[k][i] = threshold_predictions(data[k][kk], thresholds)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Dump data\n",
    "dump_name = f'{root}activitynet_captions_obj_detection_th_{th}_with_scores.json'\n",
    "with open(dump_name, \"w\") as write_file:\n",
    "            json.dump(post_processed_data, write_file)\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
