{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebooks takes care of preprocessing the obj detection (after thresholding) to achieve an HDF5 file to use as input for training using this information.\n",
    "\n",
    "NB: We need the files containing both obj predictions and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soldanm/anaconda3/envs/moments-retrieval-devel/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime \n",
    "import tqdm \n",
    "import numpy as np\n",
    "import time\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From glove.py \n",
    "GLOVE_DIM = 300\n",
    "GLOVE_FILE = '../data/raw/glove.6B.%dd.txt' % GLOVE_DIM\n",
    "VOCAB_FILE = '../data/raw/vocab_glove_complete.txt'\n",
    "\n",
    "\n",
    "class GloveEmbedding(object):\n",
    "    \"Creates glove embedding object\"\n",
    "\n",
    "    def __init__(self, glove_file=GLOVE_FILE, glove_dim=GLOVE_DIM):\n",
    "        with open(glove_file, encoding='utf-8') as fid:\n",
    "            glove_txt = fid.readlines()\n",
    "        glove_txt = [g.strip() for g in glove_txt]\n",
    "        glove_vector = [g.split(' ') for g in glove_txt]\n",
    "        glove_words = [g[0] for g in glove_vector]\n",
    "        glove_dict = {w: i for i, w in enumerate(glove_words)}\n",
    "        glove_vecs = [g[1:] for g in glove_vector]\n",
    "        glove_array = np.zeros((glove_dim, len(glove_words)))\n",
    "        for i, vec in enumerate(glove_vecs):\n",
    "            glove_array[:,i] = np.array(vec)\n",
    "        self.glove_array = glove_array\n",
    "        self.glove_dict = glove_dict\n",
    "        self.glove_words = glove_words\n",
    "        self.glove_dim = glove_dim\n",
    "        \n",
    "\n",
    "class RecurrentEmbedding(object):\n",
    "    \"TODO\"\n",
    "\n",
    "    def __init__(self, glove_file=GLOVE_FILE, glove_dim=GLOVE_DIM,\n",
    "                 vocab_file=VOCAB_FILE):\n",
    "        self.glove_file = glove_file\n",
    "        self.embedding = GloveEmbedding(self.glove_file, glove_dim)\n",
    "\n",
    "        with open(vocab_file, encoding='utf-8') as fid:\n",
    "            vocab = fid.readlines()\n",
    "        vocab = [v.strip() for v in vocab]\n",
    "        if '<unk>' in vocab:\n",
    "            # don't have an <unk> vector.  Alternatively, could map to random\n",
    "            # vector...\n",
    "            vocab.remove('<unk>')\n",
    "\n",
    "        self.vocab_dict = {}\n",
    "        for i, word in enumerate(vocab):\n",
    "            try:\n",
    "                self.vocab_dict[word] = self.embedding.glove_array[\n",
    "                    :, self.embedding.glove_dict[word]]\n",
    "            except:\n",
    "                print(f'{word} not in glove embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageRepresentationMCN_glove(object):\n",
    "    \"Get representation of sentence\"\n",
    "\n",
    "    def __init__(self, max_words=50):\n",
    "        self.max_words = max_words\n",
    "        self.dim = None\n",
    "        self.embedding = None\n",
    "        self.embedding = RecurrentEmbedding()\n",
    "        self.dim = self.embedding.embedding.glove_dim\n",
    "\n",
    "    def __call__(self, word):\n",
    "        \"Return padded sentence feature\"\n",
    "        if word in self.embedding.vocab_dict:\n",
    "            return self.embedding.vocab_dict[word]\n",
    "        else:\n",
    "            return np.zeros((self.dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 121.92 seconds.\n"
     ]
    }
   ],
   "source": [
    "## Initialization glove\n",
    "t = time.time()\n",
    "lang_interface = LanguageRepresentationMCN_glove(max_words=1)\n",
    "print('Done in {:.2f} seconds.'.format(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "## Test GLove\n",
    "word = 'hello'\n",
    "feature = lang_interface(word)\n",
    "print(feature.shape)\n",
    "print(lang_interface.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "class BERTEmbedding(object):\n",
    "    def __init__(self, data_directory=None, model_name='bert-base-uncased', features_combination_mode=0):\n",
    "        self.model_name = model_name\n",
    "        self.features_combination_mode = features_combination_mode\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        self.model.eval()\n",
    "        # Determine the modality in which layers are combined to obtain the final features\n",
    "        self._select_combination_mode()\n",
    "        self._setup_dim()\n",
    "        if data_directory:\n",
    "            self.bert_dict = {}\n",
    "            self._load_preprocessed_features(data_directory=data_directory)\n",
    "\n",
    "    def __call__(self, key):\n",
    "        '''\n",
    "            returns tuple (feat[numpy],query_length) if integer key is provided\n",
    "            return features[torch tensor] if tuple is provided.\n",
    "\n",
    "            Usage:\n",
    "                - In training we use the preprocessed sentences through annotation_id of each moment\n",
    "                - For standalone processing we need to compute first the tokenized version of the \n",
    "                sentence and then call the model on that tokenization. \n",
    "                Check below UNIT TEST in main for more details.\n",
    "        '''\n",
    "        if type(key) == int:\n",
    "            return self.bert_dict[str(key)]\n",
    "        elif type(key) == tuple:\n",
    "            return self._compute_features(key)\n",
    "        else:\n",
    "            raise('Invalid input to bert module')\n",
    "\n",
    "    def _compute_features(self, tokens):\n",
    "        # Compute tokens from sentence\n",
    "        tokens_tensor, segments_tensors, num_tokens = tokens\n",
    "        # Predict hidden states features for each layer\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, check = self.model(tokens_tensor, segments_tensors)\n",
    "        # Convert the hidden state embeddings into single token vectors [# tokens, # layers, # features]\n",
    "        token_embeddings = self._compute_tokens_vectors(encoded_layers, num_tokens)\n",
    "        # Word Vectors, compute features for each token\n",
    "        features = self.features_combination(token_embeddings)\n",
    "        # remove the special tokens [CLS]/[SEP] and transform to tensor\n",
    "        features = torch.stack(features[1:-1])   \n",
    "        return features\n",
    "\n",
    "    def _load_preprocessed_features(self, data_directory):\n",
    "        print('Loading language features')\n",
    "        t = time.time()\n",
    "        m = self.model_name.replace('-','_')\n",
    "        f = self.features_combination_mode\n",
    "        max_words = 50\n",
    "        filename = f'./data/processed/{data_directory}/bert/{m}_comb_mode_{f}.json'\n",
    "        feat = json.load(open(filename, 'r'))\n",
    "        for k,f in feat.items():\n",
    "            len_query = min(len(f), max_words)\n",
    "            padding_size = max_words - len_query\n",
    "            feature = np.pad(np.asarray(f), [(0,padding_size),(0,0)], mode='constant')\n",
    "            self.bert_dict[k] = (torch.from_numpy(feature).type(torch.FloatTensor),len_query)\n",
    "        print(\"Time to load precomputed language features {:.2f}\".format(time.time()-t))\n",
    "\n",
    "    def _tokenization(self, text):\n",
    "        #TODO: return a dictionary and non a tuple, increase readability\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        num_tokens = len(tokenized_text)\n",
    "        segments_ids = [1] * num_tokens\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        return tokens_tensor, segments_tensors, num_tokens\n",
    "\n",
    "    def _compute_tokens_vectors(self, encoded_layers, num_tokens):\n",
    "        token_embeddings = [] \n",
    "        for token_i in range(num_tokens):\n",
    "            hidden_layers = [] \n",
    "            for layer_i in range(len(encoded_layers)):\n",
    "                vec = encoded_layers[layer_i][0][token_i]\n",
    "                hidden_layers.append(vec)\n",
    "            token_embeddings.append(hidden_layers)\n",
    "        return token_embeddings\n",
    "\n",
    "    def _select_combination_mode(self):\n",
    "        mode = self.features_combination_mode\n",
    "        if mode == 0:\n",
    "            self.features_combination = self._last_layer\n",
    "        elif mode == 1:\n",
    "            self.features_combination = self._summation_last_four_layers\n",
    "        elif mode == 2:\n",
    "            self.features_combination = self._concatenation_last_four_layers\n",
    "        elif mode == 3:\n",
    "            self.features_combination = self._summation_second_to_last\n",
    "        else:\n",
    "            raise('Feature combination modality unknown, specify value in list [0,1]')\n",
    "        \n",
    "    def _concatenation_last_four_layers(self, token_embeddings):\n",
    "        return [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] \n",
    "    \n",
    "    def _summation_last_four_layers(self, token_embeddings):\n",
    "        return [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] \n",
    "    \n",
    "    def _last_layer(self, token_embeddings):\n",
    "        return [layer[-1] for layer in token_embeddings]\n",
    "    \n",
    "    def _summation_second_to_last(self, token_embeddings):\n",
    "        return [torch.sum(torch.stack(layer)[1:], 0) for layer in token_embeddings] \n",
    "\n",
    "    def _setup_dim(self):\n",
    "        self.dim = 768\n",
    "        if 'large' in self.model_name:\n",
    "            self.dim = 1024\n",
    "        if self.features_combination_mode == 2:\n",
    "            self.dim = self.dim * 4\n",
    "        \n",
    "    def compute_text_tokens(self, text):\n",
    "        '''\n",
    "        DEPRECATED, USED FOR DEBUGGIN PURPOSES\n",
    "        '''\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)[1:-1]\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_word_BERT(word):\n",
    "    BERT_clip_features = BERT(BERT._tokenization(word)).numpy()\n",
    "    if BERT_clip_features.shape[0] > 1:\n",
    "        BERT_clip_features = np.mean(np.stack(BERT_clip_features,axis=0),axis=0)\n",
    "    else: \n",
    "        BERT_clip_features = np.squeeze(BERT_clip_features)\n",
    "    return BERT_clip_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Set up model\n",
    "model_name = 'bert-base-uncased'\n",
    "features_combination_mode = 0\n",
    "BERT = BERTEmbedding(model_name=model_name, features_combination_mode=features_combination_mode)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n",
      "float32\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# Test BERT\n",
    "text = \"KABIR\"\n",
    "feat = BERT(BERT._tokenization(text)).numpy()\n",
    "print(feat.shape)\n",
    "print(feat.dtype)\n",
    "print(np.mean(np.stack(feat,axis=0),axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiDeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load data predictions\n",
    "obj_file = '../data/processed/didemo/obj_detection/visual_genome/didemo_obj_detection_perc_50_with_scores.json'\n",
    "obj_data = json.load(open(obj_file,'r'))\n",
    "\n",
    "# Load obj vocab\n",
    "classes_VG = ['__background__']\n",
    "classes_file = '../data/raw/language/visual_genome/objects_vocab.txt'\n",
    "with open(classes_file, 'r') as f:\n",
    "    for object in f.readlines():\n",
    "        classes_VG.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "# Load metadata for all splits\n",
    "metadata_val   = json.load(open('../data/processed/didemo/val-01.json','r'))\n",
    "metadata_train = json.load(open('../data/processed/didemo/train-01.json','r'))\n",
    "metadata_test  = json.load(open('../data/processed/didemo/test-01.json','r'))\n",
    "metadata = {}\n",
    "\n",
    "for k in metadata_val['videos'].keys():\n",
    "    metadata[k] = metadata_val['videos'][k]\n",
    "    \n",
    "for k in metadata_train['videos'].keys():\n",
    "    metadata[k] = metadata_train['videos'][k]\n",
    "    \n",
    "for k in metadata_test['videos'].keys():\n",
    "    metadata[k] = metadata_test['videos'][k]\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP_SIZE=2.5, MAX_DURATION=30, MAX_NUMBER_OF_CLIPS=12\n"
     ]
    }
   ],
   "source": [
    "# Set clip size of pooled features\n",
    "CLIP_SIZE = 2.5                                       # clip size in seconds\n",
    "MAX_DURATION = 30                                     # max video duration (Didemo convention)\n",
    "MAX_NUMBER_OF_CLIPS = int(MAX_DURATION // CLIP_SIZE)  # maximum number of clips per video\n",
    "\n",
    "print(f'CLIP_SIZE={CLIP_SIZE}, MAX_DURATION={MAX_DURATION}, MAX_NUMBER_OF_CLIPS={MAX_NUMBER_OF_CLIPS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1814/10642 [1:22:29<6:41:27,  2.73s/it]"
     ]
    }
   ],
   "source": [
    "# Let's create the vector of logits we want to dump\n",
    "\n",
    "data = {}\n",
    "expanded_features  = []\n",
    "glove_features_avg = {}\n",
    "glove_features_weighted_avg = {}\n",
    "glove_features_max = {}\n",
    "glove_features_bb = {}\n",
    "glove_features_bb_spatial = {}\n",
    "# BERT_features_avg  = {}\n",
    "BERT_features_bb_spatial  = {}\n",
    "# BERT_features_max  = {}\n",
    "metadata_keys = list(metadata.keys())\n",
    "\n",
    "for k in tqdm.tqdm(metadata_keys):                              # Cycle over the videos key\n",
    "\n",
    "    pred_obj_video = obj_data[k]                       # get video obj predictions\n",
    "    clip_keys = list(pred_obj_video.keys())            # get number of features we computed\n",
    "    gt_duration = metadata[k]['duration']//CLIP_SIZE   # get actual video duration and compute the number of clips\n",
    "    \n",
    "    if gt_duration > len(clip_keys):                   # store information about the videos for which we expand\n",
    "        expanded_features.append(k)                    # the features size to match the number of clips\n",
    "    \n",
    "    video_feat_logit     = np.zeros((max(MAX_NUMBER_OF_CLIPS,len(clip_keys)),len(classes_VG)-1),dtype='float32')         #placeholder for features\n",
    "    video_feat_glove_avg = np.zeros((max(MAX_NUMBER_OF_CLIPS,len(clip_keys)),lang_interface.dim),dtype='float32')    #placeholder for features\n",
    "    video_feat_glove_bb  = np.zeros((max(MAX_NUMBER_OF_CLIPS,len(clip_keys)),lang_interface.dim + 4),dtype='float32')    #placeholder for features\n",
    "    video_feat_glove_weighted_avg = np.zeros((max(MAX_NUMBER_OF_CLIPS,len(clip_keys)),lang_interface.dim),dtype='float32')    #placeholder for features\n",
    "    \n",
    "    video_feat_glove_bb_spatial = []\n",
    "    video_feat_BERT_bb_spatial  = []\n",
    "\n",
    "    for clip_idx, kk in enumerate(clip_keys):          # cycle over the clip features\n",
    "        clip_data = pred_obj_video[kk]                 \n",
    "        clip_feat = {}\n",
    "        clip_w_feat = {}\n",
    "        clip_feat_BERT = {}\n",
    "        clip_feat_bb = {}\n",
    "        for elem in clip_data:                         # For each predicted objects check the confidence score and the class\n",
    "            if elem[1] > video_feat_logit[clip_idx,elem[0]-1]:      # if confidence score of current obj greater than previously seen objects for the same class and clip\n",
    "                video_feat_logit[clip_idx,elem[0]-1] = elem[1]      # save the prediction in the logits variable. \n",
    "            \n",
    "            word = classes_VG[elem[0]]                              # get obj class name\n",
    "            clip_feat[elem[0]] = lang_interface(word)               # embedd with glove and save\n",
    "            clip_w_feat[elem[0]] = elem[1] * lang_interface(word)   # embedd with glove and weight with the confidence score.\n",
    "            clip_feat_bb[elem[0]] = np.asarray(list(lang_interface(word) ) + elem[2])  # embedd with glove, and concatenate the position information\n",
    "            \n",
    "            # Bert infor extraction in clip\n",
    "            BERT_clip_features = _process_word_BERT(word)            # embedd with BERT, and concatenate the position information\n",
    "            clip_feat_BERT[elem[0]] = np.asarray(list(BERT_clip_features) + elem[2])\n",
    "            \n",
    "        detected_obj      = [clip_feat[c] for c in clip_feat.keys()]\n",
    "        detected_w_obj    = [clip_w_feat[c] for c in clip_w_feat.keys()]\n",
    "        detected_bb_obj   = [clip_feat_bb[c] for c in clip_feat_bb.keys()]\n",
    "        detected_obj_BERT = [clip_feat_BERT[c] for c in clip_feat_BERT.keys()]\n",
    "       \n",
    "        if len(detected_obj) == 0:\n",
    "            detected_obj      = [np.zeros((lang_interface.dim,))]       # if no obj was detected we input the zero vector\n",
    "            detected_w_obj    = [np.zeros((lang_interface.dim,))]       # if no obj was detected we input the zero vector\n",
    "            detected_bb_obj   = [np.zeros((lang_interface.dim+4,))]     # if no obj was detected we input the zero vector\n",
    "            detected_obj_BERT = [np.zeros((BERT.dim+4,))]               # if no obj was detected we input the zero vector\n",
    "\n",
    "        video_feat_glove_avg[clip_idx] = np.mean(np.stack(detected_obj,axis=0),axis=0)\n",
    "        video_feat_glove_weighted_avg[clip_idx] = np.mean(np.stack(detected_w_obj,axis=0),axis=0)\n",
    "        video_feat_glove_bb[clip_idx] =  np.mean(np.stack(detected_bb_obj,axis=0),axis=0)\n",
    "        video_feat_glove_bb_spatial.append(np.asarray(detected_bb_obj))\n",
    "        video_feat_BERT_bb_spatial.append(np.asarray(detected_obj_BERT))\n",
    "    \n",
    "    \n",
    "    glove_features_avg[k]= video_feat_glove_avg[:MAX_NUMBER_OF_CLIPS,:]\n",
    "    glove_features_bb[k] = video_feat_glove_bb[:MAX_NUMBER_OF_CLIPS,:]\n",
    "    glove_features_weighted_avg[k]= video_feat_glove_weighted_avg[:MAX_NUMBER_OF_CLIPS,:]\n",
    "    glove_features_bb_spatial[k]  = video_feat_glove_bb_spatial[:MAX_NUMBER_OF_CLIPS]\n",
    "    BERT_features_bb_spatial[k] = video_feat_BERT_bb_spatial[:MAX_NUMBER_OF_CLIPS]\n",
    "    \n",
    "    data[k] = video_feat_logit[:MAX_NUMBER_OF_CLIPS,:]                          # Create a dictionary of features  \n",
    "    \n",
    "print(f'Number of videos for which we expanded the features {len(expanded_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10642/10642 [00:03<00:00, 2852.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/didemo/obj_predictions_perc_50_max_logit.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=data[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10642/10642 [00:03<00:00, 3289.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/didemo/obj_predictions_perc_50_avg_glove.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_avg[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10642/10642 [00:03<00:00, 3462.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/didemo/obj_predictions_perc_50_weighted_avg_glove.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_weighted_avg[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 442/10642 [00:00<00:04, 2199.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos 10642\n",
      "Number of clips first video 12\n",
      "Number of detections first clip 10\n",
      "Shape of feature of first detection (304,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10642/10642 [00:05<00:00, 1950.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/didemo/obj_predictions_perc_50_glove_bb_spatial.h5'\n",
    "\n",
    "# Lets padd the data\n",
    "_keys_ = list(glove_features_bb_spatial.keys())\n",
    "max_ = 0\n",
    "feat_dim = glove_features_bb_spatial[_keys_[0]][0].shape[1]\n",
    "\n",
    "for k in _keys_:\n",
    "    for i in range(len(glove_features_bb_spatial[k])):\n",
    "        tmp =  glove_features_bb_spatial[k][i].shape[0]\n",
    "        if tmp > max_:\n",
    "            max_ = tmp\n",
    "\n",
    "for k in _keys_:\n",
    "    for i,feat in enumerate(glove_features_bb_spatial[k]):\n",
    "        tmp = np.zeros((max_, feat_dim))\n",
    "        tmp[:feat.shape[0]] += feat \n",
    "        glove_features_bb_spatial[k][i] = tmp\n",
    "    glove_features_bb_spatial[k] = np.asarray(glove_features_bb_spatial[k])\n",
    "\n",
    "__keys__ = list(glove_features_bb_spatial.keys())\n",
    "print('Number of videos {}'.format(len(__keys__)))\n",
    "print('Number of clips first video {}'.format(len(glove_features_bb_spatial[__keys__[0]])))\n",
    "print('Number of detections first clip {}'.format(len(glove_features_bb_spatial[__keys__[0]][0])))\n",
    "print('Shape of feature of first detection {}'.format(glove_features_bb_spatial[__keys__[0]][0][0].shape))\n",
    "\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_bb_spatial[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/didemo/obj_predictions_perc_50_BERT_bb_spatial.h5'\n",
    "\n",
    "# Lets padd the data\n",
    "_keys_ = list(BERT_features_bb_spatial.keys())\n",
    "max_ = 0\n",
    "feat_dim = BERT_features_bb_spatial[_keys_[0]][0].shape[1]\n",
    "\n",
    "for k in _keys_:\n",
    "    for i in range(len(BERT_features_bb_spatial[k])):\n",
    "        tmp =  BERT_features_bb_spatial[k][i].shape[0]\n",
    "        if tmp > max_:\n",
    "            max_ = tmp\n",
    "\n",
    "for k in _keys_:\n",
    "    for i,feat in enumerate(BERT_features_bb_spatial[k]):\n",
    "        tmp = np.zeros((max_, feat_dim))\n",
    "        tmp[:feat.shape[0]] += feat \n",
    "        BERT_features_bb_spatial[k][i] = tmp\n",
    "    BERT_features_bb_spatial[k] = np.asarray(BERT_features_bb_spatial[k])\n",
    "\n",
    "__keys__ = list(BERT_features_bb_spatial.keys())\n",
    "print('Number of videos {}'.format(len(__keys__)))\n",
    "print('Number of clips first video {}'.format(len(BERT_features_bb_spatial[__keys__[0]])))\n",
    "print('Number of detections first clip {}'.format(len(BERT_features_bb_spatial[__keys__[0]][0])))\n",
    "print('Shape of feature of first detection {}'.format(BERT_features_bb_spatial[__keys__[0]][0][0].shape))\n",
    "\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=BERT_features_bb_spatial[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charades-sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: BERT embeding for obj classes is not yet available for charades. Please check the above function for didemo to extend charades for that functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load data predictions\n",
    "obj_file = '../data/processed/charades-sta/obj_detection/visual_genome/charades_sta_obj_detection_perc_50_with_scores.json'\n",
    "obj_data = json.load(open(obj_file,'r'))\n",
    "\n",
    "# Load obj vocab\n",
    "classes_VG = ['__background__']\n",
    "classes_file = '../data/raw/language/visual_genome/objects_vocab.txt'\n",
    "with open(classes_file, 'r') as f:\n",
    "    for object in f.readlines():\n",
    "        classes_VG.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "# Load metadata for all splits\n",
    "metadata_train = json.load(open('../data/processed/charades-sta/train-01.json','r'))\n",
    "metadata_test  = json.load(open('../data/processed/charades-sta/test-01.json','r'))\n",
    "metadata = {}\n",
    "    \n",
    "for k in metadata_train['videos'].keys():\n",
    "    metadata[k] = metadata_train['videos'][k]\n",
    "    \n",
    "for k in metadata_test['videos'].keys():\n",
    "    metadata[k] = metadata_test['videos'][k]\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP_SIZE=3\n"
     ]
    }
   ],
   "source": [
    "# Set clip size of pooled features\n",
    "CLIP_SIZE = 3                                         # clip size in seconds\n",
    "\n",
    "print(f'CLIP_SIZE={CLIP_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [00:13<00:00, 501.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos for which we expanded the features 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create the vector of logits we want to dump\n",
    "\n",
    "data = {}\n",
    "expanded_features = []\n",
    "glove_features_avg = {}\n",
    "glove_features_max = {}\n",
    "\n",
    "for k in tqdm.tqdm(metadata.keys()):                              # Cycle over the videos key\n",
    "    \n",
    "    pred_obj_video = obj_data[k]                       # get video obj predictions\n",
    "    clip_keys = list(pred_obj_video.keys())            # get number of features we computed\n",
    "    gt_duration = metadata[k]['duration']//CLIP_SIZE   # get actual video duration and compute the number of clips\n",
    "    \n",
    "#     if gt_duration > len(clip_keys):                   # store information about the videos for which we expand\n",
    "#         expanded_features.append(k)                    # the features size to match the number of clips\n",
    "    \n",
    "    video_feat_logit = np.zeros((len(clip_keys),len(classes_VG)-1),dtype='float32')         #placeholder for features\n",
    "    video_feat_glove_avg = np.zeros((len(clip_keys),lang_interface.dim),dtype='float32')    #placeholder for features\n",
    "    video_feat_glove_max = np.zeros((len(clip_keys),lang_interface.dim),dtype='float32')    #placeholder for features\n",
    "    \n",
    "    for clip_idx, kk in enumerate(clip_keys):          # cycle over the clip features\n",
    "        clip_data = pred_obj_video[kk]                 \n",
    "        clip_feat = {}\n",
    "        for elem in clip_data:                         # For each predicted objects check the confidence score and the class\n",
    "            if elem[1] > video_feat_logit[clip_idx,elem[0]-1]:      # if confidence score of current obj greater than previously seen objects for the same class and clip\n",
    "                video_feat_logit[clip_idx,elem[0]-1] = elem[1]      # save the prediction in the logits variable. \n",
    "                word = classes_VG[elem[0]]             # get obj class name\n",
    "            clip_feat[elem[0]] = lang_interface(word)           # embedd with glove and save\n",
    "        detected_obj = [clip_feat[k] for k in clip_feat.keys()]\n",
    "        if len(detected_obj) == 0:\n",
    "            detected_obj = [np.zeros((lang_interface.dim,))]       # if no obj was detected we input the zero vector\n",
    "\n",
    "        video_feat_glove_avg[clip_idx] = np.mean(np.stack(detected_obj,axis=0),axis=0)\n",
    "        video_feat_glove_max[clip_idx] = np.max(np.stack(detected_obj,axis=0),axis=0)\n",
    "\n",
    "    glove_features_avg[k] = video_feat_glove_avg\n",
    "    glove_features_max[k] = video_feat_glove_max\n",
    "    data[k]               = video_feat_logit                          # Create a dictionary of features  \n",
    "    \n",
    "print(f'Number of videos for which we expanded the features {len(expanded_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [00:01<00:00, 3718.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/charades-sta/obj_predictions_perc_50_max_logit.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=data[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [00:01<00:00, 4394.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/charades-sta/obj_predictions_perc_50_avg_glove.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_avg[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6670/6670 [00:01<00:00, 4391.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/charades-sta/obj_predictions_perc_50_max_glove.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_max[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activitynet Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Load data predictions\n",
    "obj_file = '../data/processed/activitynet-captions/obj_detection/visual_genome/activitynet_captions_obj_detection_perc_50_with_scores.json'\n",
    "obj_data = json.load(open(obj_file,'r'))\n",
    "\n",
    "# Load obj vocab\n",
    "classes_VG = ['__background__']\n",
    "classes_file = '../data/raw/language/visual_genome/objects_vocab.txt'\n",
    "with open(classes_file, 'r') as f:\n",
    "    for object in f.readlines():\n",
    "        classes_VG.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "# Load metadata for all splits\n",
    "metadata_val   = json.load(open('../data/processed/activitynet-captions/val.json','r'))\n",
    "metadata_train = json.load(open('../data/processed/activitynet-captions/train.json','r'))\n",
    "metadata = {}\n",
    "\n",
    "for k in metadata_val['videos'].keys():\n",
    "    metadata[k] = metadata_val['videos'][k]\n",
    "    \n",
    "for k in metadata_train['videos'].keys():\n",
    "    metadata[k] = metadata_train['videos'][k]\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14926\n",
      "19994\n"
     ]
    }
   ],
   "source": [
    "print(len(list(metadata.keys())))\n",
    "print(len(list(obj_data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP_SIZE=2.5\n"
     ]
    }
   ],
   "source": [
    "# Set clip size of pooled features\n",
    "CLIP_SIZE = 2.5                                         # clip size in seconds\n",
    "\n",
    "print(f'CLIP_SIZE={CLIP_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14926/14926 [01:27<00:00, 171.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos for which we expanded the features 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create the vector of logits we want to dump\n",
    "\n",
    "data = {}\n",
    "expanded_features = []\n",
    "glove_features_avg = {}\n",
    "glove_features_max = {}\n",
    "\n",
    "for k in tqdm.tqdm(metadata.keys()):                              # Cycle over the videos key\n",
    "    \n",
    "    pred_obj_video = obj_data[k]                       # get video obj predictions\n",
    "    clip_keys = list(pred_obj_video.keys())            # get number of features we computed\n",
    "    gt_duration = metadata[k]['duration']//CLIP_SIZE   # get actual video duration and compute the number of clips\n",
    "    \n",
    "#     if gt_duration > len(clip_keys):                   # store information about the videos for which we expand\n",
    "#         expanded_features.append(k)                    # the features size to match the number of clips\n",
    "    \n",
    "    video_feat_logit = np.zeros((len(clip_keys),len(classes_VG)-1),dtype='float32')         #placeholder for features\n",
    "    video_feat_glove_avg = np.zeros((len(clip_keys),lang_interface.dim),dtype='float32')    #placeholder for features\n",
    "    video_feat_glove_max = np.zeros((len(clip_keys),lang_interface.dim),dtype='float32')    #placeholder for features\n",
    "    \n",
    "    for clip_idx, kk in enumerate(clip_keys):          # cycle over the clip features\n",
    "        clip_data = pred_obj_video[kk]                 \n",
    "        clip_feat = {}\n",
    "        for elem in clip_data:                         # For each predicted objects check the confidence score and the class\n",
    "            if elem[1] > video_feat_logit[clip_idx,elem[0]-1]:      # if confidence score of current obj greater than previously seen objects for the same class and clip\n",
    "                video_feat_logit[clip_idx,elem[0]-1] = elem[1]      # save the prediction in the logits variable. \n",
    "                word = classes_VG[elem[0]]             # get obj class name\n",
    "            clip_feat[elem[0]] = lang_interface(word)           # embedd with glove and save\n",
    "        detected_obj = [clip_feat[k] for k in clip_feat.keys()]\n",
    "        if len(detected_obj) == 0:\n",
    "            detected_obj = [np.zeros((lang_interface.dim,))]       # if no obj was detected we input the zero vector\n",
    "\n",
    "        video_feat_glove_avg[clip_idx] = np.mean(np.stack(detected_obj,axis=0),axis=0)\n",
    "        video_feat_glove_max[clip_idx] = np.max(np.stack(detected_obj,axis=0),axis=0)\n",
    "\n",
    "    glove_features_avg[k] = video_feat_glove_avg\n",
    "    glove_features_max[k] = video_feat_glove_max\n",
    "    data[k]               = video_feat_logit                          # Create a dictionary of features  \n",
    "    \n",
    "print(f'Number of videos for which we expanded the features {len(expanded_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/charades-sta/obj_predictions_perc_50_max_logit.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=data[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/charades-sta/obj_predictions_perc_50_avg_glove.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_avg[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the data into a hdf5 file\n",
    "output_file = '../data/processed/charades-sta/obj_predictions_perc_50_max_glove.h5'\n",
    "\n",
    "with h5py.File(output_file, \"w\") as f:\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        f.create_dataset(k, data=glove_features_max[k])\n",
    "    f.create_dataset('metadata/time_unit', data=np.asarray(CLIP_SIZE))     # dump information regarding the clip size\n",
    "    \n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
