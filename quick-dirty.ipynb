{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit-test (kinda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"model.py\", line 291, in <module>\n",
      "    a, b, c, d = net(y_padded, z, x, x, x)\n",
      "  File \"/mnt/ilcompf9d1/user/escorcia/install/bin/miniconda3/envs/scientific/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"model.py\", line 52, in forward\n",
      "    last_output = output[range(B), query_length - 1, :]\n",
      "TypeError: unsupported operand type(s) for -: 'list' and 'int'\n"
     ]
    }
   ],
   "source": [
    "# Really quick-test\n",
    "!python model.py\n",
    "!python loss.py\n",
    "#!python loupe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/escorciav/install/bin/miniconda3/envs/scientific/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "simple test\n",
      "Time loading data/raw/train_data.json:  114.0846495628357\n",
      "33005\n",
      "{'num_segments': 6, 'description': \"a brown rat goes into someone's hand then onto a cage.\", 'dl_link': 'https://www.flickr.com/video_download.gne?id=2408598493', 'times': [[2, 2], [2, 2], [2, 2], [2, 2]], 'video': '54322086@N00_2408598493_274c77d26a.avi', 'annotation_id': 2, 'language_input': ['a', 'brown', 'rat', 'goes', 'into', 'someone', 's', 'hand', 'then', 'onto', 'a', 'cage']}\n",
      "0 (50, 300)\n",
      "1 12\n",
      "rgb (8194,)\n",
      "flow (2050,)\n",
      "rgb (8194,)\n",
      "flow (2050,)\n",
      "rgb (8194,)\n",
      "flow (2050,)\n",
      "0 (21, 50, 300)\n",
      "1 21\n",
      "rgb (21, 8194)\n",
      "flow (21, 2050)\n"
     ]
    }
   ],
   "source": [
    "# This takes a couple of minutes\n",
    "!python didemo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Add NetVLAD\n",
    "\n",
    "- Experiment NetVLAD vs LSTM\n",
    "\n",
    "# Bash\n",
    "\n",
    "Test training loop\n",
    "```bash\n",
    "for i in {001..015}; do python train.py --epochs 2 --gpu-id 1 &> $i\".log\"; done\n",
    "```\n",
    "\n",
    "# Sandbox\n",
    "\n",
    "Always stacking, It's better than scrolling :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 11,  12,  13],\n",
      "        [ 21,  22,  23],\n",
      "        [ 31,  32,  33]])\n",
      "torch.Size([3])\n",
      "tensor([ 11,  22,  33])\n",
      "torch.Size([3, 3])\n",
      "tensor([[ 11,  22,  33],\n",
      "        [ 11,  22,  33],\n",
      "        [ 11,  22,  33]])\n",
      "torch.Size([9, 1])\n",
      "tensor([[ 11],\n",
      "        [ 22],\n",
      "        [ 33],\n",
      "        [ 11],\n",
      "        [ 22],\n",
      "        [ 33],\n",
      "        [ 11],\n",
      "        [ 22],\n",
      "        [ 33]])\n",
      "tensor([[ 11],\n",
      "        [ 12],\n",
      "        [ 13],\n",
      "        [ 21],\n",
      "        [ 22],\n",
      "        [ 23],\n",
      "        [ 31],\n",
      "        [ 32],\n",
      "        [ 33]])\n",
      "tensor([[ 11],\n",
      "        [ 21],\n",
      "        [ 31],\n",
      "        [ 12],\n",
      "        [ 22],\n",
      "        [ 32],\n",
      "        [ 13],\n",
      "        [ 23],\n",
      "        [ 33]])\n",
      "tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 0.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n",
    "print(x)\n",
    "n = x.size()[0]        \n",
    "x1 = torch.diag(x)\n",
    "print(x1.shape)\n",
    "print(x1)\n",
    "x1 = x1.expand(n, n)\n",
    "print(x1.shape)\n",
    "print(x1)\n",
    "x1 = x1.contiguous().view(-1,1)\n",
    "print(x1.shape)\n",
    "print(x1)\n",
    "x1 = torch.cat((x1, x1),0) \n",
    "\n",
    "x2 = x.view(-1, 1)\n",
    "print(x2)\n",
    "x3 = x.transpose(0, 1).contiguous().view(-1,1)\n",
    "print(x3)\n",
    "\n",
    "x2 = torch.cat((x2,x3),0)\n",
    "\n",
    "y = (1 - torch.eye(n)).view(-1, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0]), {'x': tensor([ 0]), 'y': tensor([ 5])}]\n",
      "[tensor([ 2]), {'x': tensor([ 1]), 'y': tensor([ 6])}]\n",
      "[tensor([ 4]), {'x': tensor([ 2]), 'y': tensor([ 7])}]\n",
      "[tensor([ 6]), {'x': tensor([ 3]), 'y': tensor([ 8])}]\n",
      "[tensor([ 8]), {'x': tensor([ 4]), 'y': tensor([ 9])}]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Simple(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = list(range(5))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        return x * 2, {'x': x, 'y': x + 5}\n",
    "    \n",
    "data = Simple()\n",
    "data[0]\n",
    "\n",
    "loader = DataLoader(data)\n",
    "\n",
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class NetVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.cluster_size = cluster_size\n",
    "        self.add_batch_norm = add_batch_norm\n",
    "        self.out_dim = cluster_size * feature_size\n",
    "        self.clusters = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * torch.randn(feature_size, cluster_size))\n",
    "        self.clusters2 = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * th.randn(1, feature_size, cluster_size))\n",
    "        if add_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_sample = x.shape[1]\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        assignment = th.matmul(x, self.clusters)\n",
    "\n",
    "        if self.add_batch_norm:\n",
    "            assignment = self.batch_norm(assignment)\n",
    "\n",
    "        assignment = F.softmax(assignment, dim=1)\n",
    "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
    "\n",
    "        a_sum = th.sum(assignment, -2, keepdim=True)\n",
    "        a = a_sum * self.clusters2\n",
    "        assignment = assignment.transpose(1, 2)\n",
    "\n",
    "        x = x.view(-1, max_sample, self.feature_size)\n",
    "        vlad = th.matmul(assignment, x)\n",
    "        vlad = vlad.transpose(1, 2)\n",
    "        vlad = vlad - a\n",
    "\n",
    "        # L2 intra norm\n",
    "        vlad = F.normalize(vlad)\n",
    "        \n",
    "        # flattening + L2 norm\n",
    "        vlad = vlad.view(-1, self.cluster_size * self.feature_size)\n",
    "        vlad = F.normalize(vlad)\n",
    "\n",
    "        return vlad\n",
    "\n",
    "class NetRVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
    "        super(NetRVLAD, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.cluster_size = cluster_size\n",
    "        self.add_batch_norm = add_batch_norm\n",
    "        self.out_dim = cluster_size * feature_size\n",
    "        self.clusters = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * th.randn(feature_size, cluster_size))\n",
    "        if self.add_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        max_sample = x.shape[1]\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        assignment = th.matmul(x, self.clusters)\n",
    "\n",
    "        if self.add_batch_norm:\n",
    "            assignment = self.batch_norm(assignment)\n",
    "\n",
    "        assignment = F.softmax(assignment, dim=1)\n",
    "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
    "        assignment = assignment.transpose(1, 2)\n",
    "\n",
    "        x = x.view(-1, max_sample, self.feature_size)\n",
    "        rvlad = th.matmul(assignment, x)\n",
    "        rvlad = rvlad.transpose(-1, 1)\n",
    "\n",
    "        # L2 intra norm\n",
    "        rvlad = F.normalize(rvlad)\n",
    "        \n",
    "        # flattening + L2 norm\n",
    "        rvlad = rvlad.view(-1, self.cluster_size * self.feature_size)\n",
    "        rvlad = F.normalize(rvlad)\n",
    "\n",
    "        return rvlad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "- Used to check if parameter are changing\n",
    "\n",
    "```python\n",
    "for k, v in net.img_encoder.named_parameters(): print(k, v.sum())\n",
    "for k, v in net.sentence_encoder.named_parameters(): print(k, v.sum())\n",
    "```\n",
    "\n",
    "- Pick last step of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacked, padded output: \n",
      "tensor([[[-0.8222,  0.7389],\n",
      "         [-0.8997,  0.7998],\n",
      "         [-0.9645,  0.8589]],\n",
      "\n",
      "        [[-0.8222,  0.7389],\n",
      "         [-0.8997,  0.7998],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.8222,  0.7389],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9388,  0.8147],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]]])\n",
      "tensor([[-0.9645,  0.8589],\n",
      "        [-0.8997,  0.7998],\n",
      "        [-0.8222,  0.7389],\n",
      "        [-0.9388,  0.8147]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 4\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers = 1\n",
    "input_dim = 1\n",
    "batch_first = True\n",
    "\n",
    "# Data\n",
    "vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "vec_3 = torch.FloatTensor([[1, 0, 0]])\n",
    "vec_4 = torch.FloatTensor([[2, 0, 0]])\n",
    "\n",
    "# Put the data into a tensor.\n",
    "batch_in = torch.cat([vec_1, vec_2, vec_3, vec_4])\n",
    "batch_in = torch.unsqueeze(batch_in, -1)\n",
    "\n",
    "# Wrap RNN input in a Variable. Shape: (batch_size, max_length, input_dim)\n",
    "# The lengths of each example in the batch. Padding is 0.\n",
    "lengths = torch.LongTensor([3, 2, 1, 1])\n",
    "\n",
    "# Wrap input in packed sequence, with batch_first=True\n",
    "packed_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "    batch_in, lengths, batch_first=True)\n",
    "\n",
    "# Create an RNN object, set batch_first=True\n",
    "rnn = nn.RNN(input_dim, hidden_size, n_layers, batch_first=True) \n",
    "\n",
    "# Run input through RNN \n",
    "packed_output, _ = rnn(packed_input)\n",
    "\n",
    "# Unpack, with batch_first=True.\n",
    "output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "    packed_output, batch_first=True)\n",
    "print(\"Unpacked, padded output: \")\n",
    "print(output)\n",
    "last_step = output[range(batch_size), lengths - 1, :]\n",
    "print(last_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
