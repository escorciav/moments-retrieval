{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit-test (kinda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unit test for SMCN non-decomposoable search \n",
    "\n",
    "Making sure operations do what we wanna do.\n",
    "\n",
    "In case we need to speed up things, we may need to write our own cuda kernel. The operations resembles an ROI-like operation. We would have to trace that the bottleneck is in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "for i in range(100):\n",
    "    clips_per_segment = torch.randint(1, 8, (4,))\n",
    "    clips_per_segment_list = clips_per_segment.tolist()\n",
    "    list_clips_score = [torch.rand(int(i)) for i in clips_per_segment]\n",
    "    clip_score = torch.cat(list_clips_score)\n",
    "    clip_score_ = clip_score.split(clips_per_segment_list)    \n",
    "    sorted_clips_per_segment, ind = clips_per_segment.sort(descending=True)\n",
    "    _, original_ind = ind.sort(descending=False)\n",
    "    clip_distance_padded = pad_sequence([clip_score_[i] for i in ind], batch_first=True)\n",
    "    sorted_segment_distance = (clip_distance_padded.sum(dim=1) /\n",
    "                               sorted_clips_per_segment)\n",
    "    _, original_ind = ind.sort(descending=False)\n",
    "    segment_distance = sorted_segment_distance[original_ind]\n",
    "    gt = torch.tensor([i.sum() / len(i) for i in list_clips_score])\n",
    "    # print(segment_distance)\n",
    "    # print(gt)\n",
    "    assert torch.nn.functional.mse_loss(segment_distance, gt) < 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Debuging mix of heterogenous data sources (video and images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from didemo import DidemoSMCNHeterogeneous\n",
    "\n",
    "filename = 'data/interim/didemo_yfcc100m/train_data.json'\n",
    "h5_video = 'data/interim/didemo/resnet152/320x240_max.h5'\n",
    "h5_img = 'data/interim/yfcc100m/resnet152/320x240_001.h5'\n",
    "cues = {'rgb': {'file': [h5_video, h5_img]}}\n",
    "blah = DidemoSMCNHeterogeneous(filename, cues, DEBUG=True)\n",
    "\n",
    "for i, v in enumerate(blah):\n",
    "    if blah.metadata[i]['source'] == 1:\n",
    "        aja = blah[i]\n",
    "        break```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a couple of minutes\n",
    "!python didemo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash\n",
    "\n",
    "Test training loop\n",
    "```bash\n",
    "for i in {001..015}; do python train.py --epochs 2 --gpu-id 1 &> $i\".log\"; done\n",
    "```\n",
    "\n",
    "# Sandbox\n",
    "\n",
    "Always stacking, It's better than scrolling :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that at least two annotators in DiDeMo agrees with tIOU=1.0\n",
    "\n",
    "Requested by Bryan on Oct 22 to adapt DiDeMo metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from np_segments_ops import iou as segment_iou\n",
    "\n",
    "filename = 'data/raw/train_data.json'\n",
    "min_iou = \n",
    "with open(filename, 'r') as fid:\n",
    "    data = json.load(fid)\n",
    "    for i, moment in enumerate(data):\n",
    "        # trasnform to seconds\n",
    "        moment['times'] = np.array(moment['times'])\n",
    "        moment['times'] *= 5\n",
    "        moment['times'][:, 1] += 5\n",
    "        \n",
    "        # compute iou among annotators\n",
    "        iou_matrix = segment_iou(\n",
    "            moment['times'], moment['times'])\n",
    "        ind = np.where(iou_matrix < 1)\n",
    "        iou_matrix[ind] = 0\n",
    "        assert np.any(iou_matrix.sum(axis=1) >= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce train/val set for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "file1 = 'workers/skynet/data/processed/activitynet-captions/train.json'\n",
    "file2 = 'workers/skynet/data/processed/activitynet-captions/train-reduced.json'\n",
    "# size of dataset\n",
    "K = 1280\n",
    "\n",
    "with open(file1, 'r') as fr, open(file2, 'w') as fw:\n",
    "    data = json.load(fr)\n",
    "    copy = {}\n",
    "    for k, v in data.items():\n",
    "        if k != 'moments':\n",
    "            copy[k] = v\n",
    "        else:\n",
    "            copy[k] = random.choices(v, k=K)\n",
    "    json.dump(copy, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Simple(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = list(range(5))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        return x * 2, {'x': x, 'y': x + 5}\n",
    "    \n",
    "data = Simple()\n",
    "data[0]\n",
    "\n",
    "loader = DataLoader(data)\n",
    "\n",
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class NetVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.cluster_size = cluster_size\n",
    "        self.add_batch_norm = add_batch_norm\n",
    "        self.out_dim = cluster_size * feature_size\n",
    "        self.clusters = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * torch.randn(feature_size, cluster_size))\n",
    "        self.clusters2 = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * th.randn(1, feature_size, cluster_size))\n",
    "        if add_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_sample = x.shape[1]\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        assignment = th.matmul(x, self.clusters)\n",
    "\n",
    "        if self.add_batch_norm:\n",
    "            assignment = self.batch_norm(assignment)\n",
    "\n",
    "        assignment = F.softmax(assignment, dim=1)\n",
    "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
    "\n",
    "        a_sum = th.sum(assignment, -2, keepdim=True)\n",
    "        a = a_sum * self.clusters2\n",
    "        assignment = assignment.transpose(1, 2)\n",
    "\n",
    "        x = x.view(-1, max_sample, self.feature_size)\n",
    "        vlad = th.matmul(assignment, x)\n",
    "        vlad = vlad.transpose(1, 2)\n",
    "        vlad = vlad - a\n",
    "\n",
    "        # L2 intra norm\n",
    "        vlad = F.normalize(vlad)\n",
    "        \n",
    "        # flattening + L2 norm\n",
    "        vlad = vlad.view(-1, self.cluster_size * self.feature_size)\n",
    "        vlad = F.normalize(vlad)\n",
    "\n",
    "        return vlad\n",
    "\n",
    "class NetRVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
    "        super(NetRVLAD, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.cluster_size = cluster_size\n",
    "        self.add_batch_norm = add_batch_norm\n",
    "        self.out_dim = cluster_size * feature_size\n",
    "        self.clusters = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * th.randn(feature_size, cluster_size))\n",
    "        if self.add_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        max_sample = x.shape[1]\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        assignment = th.matmul(x, self.clusters)\n",
    "\n",
    "        if self.add_batch_norm:\n",
    "            assignment = self.batch_norm(assignment)\n",
    "\n",
    "        assignment = F.softmax(assignment, dim=1)\n",
    "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
    "        assignment = assignment.transpose(1, 2)\n",
    "\n",
    "        x = x.view(-1, max_sample, self.feature_size)\n",
    "        rvlad = th.matmul(assignment, x)\n",
    "        rvlad = rvlad.transpose(-1, 1)\n",
    "\n",
    "        # L2 intra norm\n",
    "        rvlad = F.normalize(rvlad)\n",
    "        \n",
    "        # flattening + L2 norm\n",
    "        rvlad = rvlad.view(-1, self.cluster_size * self.feature_size)\n",
    "        rvlad = F.normalize(rvlad)\n",
    "\n",
    "        return rvlad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used to check if parameter are changing\n",
    "\n",
    "```python\n",
    "for k, v in net.img_encoder.named_parameters(): print(k, v.sum())\n",
    "for k, v in net.sentence_encoder.named_parameters(): print(k, v.sum())\n",
    "```\n",
    "\n",
    "- Checking that we can hash video-name with 8 integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from didemo import DidemoSMCNRetrieval\n",
    "from didemo import RetrievalMode\n",
    "import numpy as np\n",
    "\n",
    "RGB_FEAT_PATH = 'data/interim/didemo/resnet152/320x240_max.h5'\n",
    "args = dict(context=False, loc=False,\n",
    "            cues=dict(rgb=dict(file=RGB_FEAT_PATH)))\n",
    "\n",
    "for subset in ['val', 'test']:\n",
    "    LIST_PATH = f'data/raw/{subset}_data_wwa.json'\n",
    "    val_dataset = DidemoSMCNRetrieval(LIST_PATH, **args)\n",
    "    val_dataset.mode = RetrievalMode.VIDEO_TO_DESCRIPTION\n",
    "    video_ids = []\n",
    "    for video_j_data in val_dataset:\n",
    "        video_j_ind = video_j_data[0]\n",
    "        video_id = val_dataset.metada_per_video[video_j_ind][0]\n",
    "        video_id_int = int(hashlib.sha256(video_id.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "        video_ids.append(video_id_int)\n",
    "    print(len(video_ids), len(np.unique(video_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick last step of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 4\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers = 1\n",
    "input_dim = 1\n",
    "batch_first = True\n",
    "\n",
    "# Data\n",
    "vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "vec_3 = torch.FloatTensor([[1, 0, 0]])\n",
    "vec_4 = torch.FloatTensor([[2, 0, 0]])\n",
    "\n",
    "# Put the data into a tensor.\n",
    "batch_in = torch.cat([vec_1, vec_2, vec_3, vec_4])\n",
    "batch_in = torch.unsqueeze(batch_in, -1)\n",
    "\n",
    "# Wrap RNN input in a Variable. Shape: (batch_size, max_length, input_dim)\n",
    "# The lengths of each example in the batch. Padding is 0.\n",
    "lengths = torch.LongTensor([3, 2, 1, 1])\n",
    "\n",
    "# Wrap input in packed sequence, with batch_first=True\n",
    "packed_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "    batch_in, lengths, batch_first=True)\n",
    "\n",
    "# Create an RNN object, set batch_first=True\n",
    "rnn = nn.RNN(input_dim, hidden_size, n_layers, batch_first=True) \n",
    "\n",
    "# Run input through RNN \n",
    "packed_output, _ = rnn(packed_input)\n",
    "\n",
    "# Unpack, with batch_first=True.\n",
    "output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "    packed_output, batch_first=True)\n",
    "print(\"Unpacked, padded output: \")\n",
    "print(output)\n",
    "last_step = output[range(batch_size), lengths - 1, :]\n",
    "print(last_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"empirical\" evidence that sort indices return the correct array\n",
    "\n",
    "Huge credits to @ModarTensai who came with the idea of sorting indices faster than @escorciav could find the solution in Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for i in range(100000):\n",
    "    a = torch.rand(100)\n",
    "    # print(a)\n",
    "    b, ind = torch.sort(a, descending=True)\n",
    "    #print(b)\n",
    "    #print(ind)\n",
    "    _, ind2 = torch.sort(ind, descending=False)\n",
    "    assert (b[ind2] == a).sum() == len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making sure my IOU does what it should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from np_segments_ops import iou\n",
    "\n",
    "def random_segments(n):\n",
    "    x_ = np.random.rand(n, 2).astype(np.float32)\n",
    "    x = np.empty_like(x_)\n",
    "    x[:, 0] = np.min(x_, axis=1)\n",
    "    x[:, 1] = np.max(x_, axis=1)\n",
    "    return x\n",
    "\n",
    "N, M = 50, 75\n",
    "a = random_segments(N)\n",
    "b = random_segments(M)\n",
    "a_list = a.tolist()\n",
    "b_list = b.tolist()\n",
    "gt = np.empty((N, M))\n",
    "def python_iou(s_pred, s_gt):\n",
    "    # adapted from didemo evaluation for time\n",
    "    intersection = max(\n",
    "        0, min(s_pred[1], s_gt[1]) - max(s_pred[0], s_gt[0]))\n",
    "    union = max(s_pred[1], s_gt[1]) - min(s_pred[0], s_gt[0])\n",
    "    return intersection / union\n",
    "for i, a_i in enumerate(a_list):\n",
    "    for j, b_j in enumerate(b_list):\n",
    "        gt[i, j] = python_iou(a_i, b_j)\n",
    "np.testing.assert_array_almost_equal(iou(a, b), gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making sure evaluation is equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from np_segments_ops import torch_iou\n",
    "\n",
    "def approach1(true_segments, pred_segments, k_iou):\n",
    "    iou_matrix = torch_iou(pred_segments, true_segments)\n",
    "    max_k = k_iou[-1][0]\n",
    "    if iou_matrix.shape[0] < max_k:\n",
    "        n_times = round(max_k / iou_matrix.shape[0])\n",
    "        iou_matrix = iou_matrix.repeat(n_times, 1)\n",
    "    hit_topk_iou = []\n",
    "    for top_k, iou_threshold in k_iou:\n",
    "        best_iou_topk, _ = iou_matrix[:top_k, :].max(dim=0)\n",
    "        hit_topk_iou.append(best_iou_topk >= iou_threshold)\n",
    "    return hit_topk_iou\n",
    "\n",
    "def approach2(true_segments, pred_segments, iou_thresholds, topk):\n",
    "    concensus_among_annotators = 1 if len(true_segments) == 1 else 2\n",
    "    P, Q = len(iou_thresholds), len(topk)\n",
    "    iou_matrix = torch_iou(pred_segments, true_segments)\n",
    "    # TODO: check type\n",
    "    hit_k_iou = torch.empty(P * Q, dtype=iou_matrix.dtype,\n",
    "                            device=iou_matrix.device)\n",
    "    for i, threshold in enumerate(iou_thresholds):\n",
    "        hit_iou = ((iou_matrix >= threshold).sum(dim=1) >=\n",
    "                    concensus_among_annotators)\n",
    "        rank_iou = (hit_iou != 0).nonzero()\n",
    "        if len(rank_iou) == 0:\n",
    "            hit_k_iou[i * Q:(i + 1) * Q] = 0\n",
    "        else:\n",
    "            # 0-indexed -> +1\n",
    "            hit_k_iou[i * Q:(i + 1) * Q] = topk >= (rank_iou[0] + 1)\n",
    "    return hit_k_iou\n",
    "\n",
    "def random_segments(n):\n",
    "    x_ = torch.rand(n, 2)\n",
    "    x = torch.empty_like(x_)\n",
    "    x[:, 0] = torch.min(x_, dim=1)[0]\n",
    "    x[:, 1] = torch.max(x_, dim=1)[0]\n",
    "    return x\n",
    "\n",
    "for i in range(100):\n",
    "    M = 1\n",
    "    N = random.randint(3, 15)\n",
    "    k_iou = [(1, 0.5), (5, 0.5), (1, 0.7), (5, 0.7)]\n",
    "    ious = [0.5, 0.7]\n",
    "    topk = torch.tensor([1, 5])\n",
    "\n",
    "    x1 = random_segments(M)\n",
    "    x2 = random_segments(N)\n",
    "    gt = torch.cat(approach1(x1, x2, k_iou)).float()\n",
    "    pred = approach2(x1, x2, ious, topk).float()\n",
    "    assert torch.nn.functional.mse_loss(pred, gt) < 1e-9\n",
    "    \n",
    "for i in range(1000):\n",
    "    M = random.randint(1, 10)\n",
    "    N = random.randint(3, 15)\n",
    "    k_iou = [(1, 0.5), (5, 0.5), (1, 0.7), (5, 0.7)]\n",
    "    ious = [0.5, 0.7]\n",
    "    topk = torch.tensor([1, 5])\n",
    "\n",
    "    x1 = random_segments(M)\n",
    "    x2 = random_segments(N)\n",
    "    pred = approach2(x1, x2, ious, topk).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get MD5SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "gt_fmt = 'data/processed/didemo/{}.json'\n",
    "file_fmt = 'data/interim/didemo/{}.json'\n",
    "for i in ['train', 'test', 'val']:\n",
    "    # Only edit metadata-related to tracking\n",
    "    # if u over-write everything, of course they will match!\n",
    "    new_file = file_fmt.format(i)\n",
    "    with open(gt_fmt.format(i), 'r') as fr, open(new_file, 'r') as fn:\n",
    "        data_r = json.load(fr)\n",
    "        data_n = json.load(fn)\n",
    "        data_n['date'] = data_r['date']\n",
    "        data_n['git_hash'] = data_r['git_hash']\n",
    "    with open(new_file, 'w') as fn:\n",
    "        json.dump(data_n, fn)\n",
    "    # TODO: automate comparison\n",
    "    !md5sum $new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform warining into errors, useful to debug zero division\n",
    "\n",
    "  ```python\n",
    "  import warnings\n",
    "  warnings.filterwarnings(\"error\")\n",
    "  ```\n",
    "\n",
    "## Crude ideas\n",
    "\n",
    "1. Feature engineering\n",
    "\n",
    "    - Images\n",
    "\n",
    "        - resnet 152/101\n",
    "\n",
    "        - facenet\n",
    "\n",
    "    - Text\n",
    "\n",
    "        - word2vec\n",
    "\n",
    "        - openai language model\n",
    "\n",
    "        - skipthought\n",
    "\n",
    "    - audio\n",
    "\n",
    "        - VGGs from google\n",
    "\n",
    "        - AudioNet Vondrick and others\n",
    "\n",
    "    - Video\n",
    "\n",
    "        - I3D\n",
    "\n",
    "        - NLN\n",
    "\n",
    "1. LSTM with variable sequence length\n",
    "\n",
    "    - query features\n",
    "    \n",
    "    - unroll until the end and predict sequence length\n",
    "    \n",
    "    - 0/1 representing time to match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
