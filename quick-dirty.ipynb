{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit-test (kinda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from didemo import DidemoSMCNHeterogeneous\n",
    "\n",
    "filename = 'data/interim/didemo_yfcc100m/train_data.json'\n",
    "h5_video = 'data/interim/didemo/resnet152/320x240_max.h5'\n",
    "h5_img = 'data/interim/yfcc100m/resnet152/320x240_001.h5'\n",
    "cues = {'rgb': {'file': [h5_video, h5_img]}}\n",
    "blah = DidemoSMCNHeterogeneous(filename, cues, DEBUG=True)\n",
    "\n",
    "for i, v in enumerate(blah):\n",
    "    if blah.metadata[i]['source'] == 1:\n",
    "        aja = blah[i]\n",
    "        break```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a couple of minutes\n",
    "!python didemo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash\n",
    "\n",
    "Test training loop\n",
    "```bash\n",
    "for i in {001..015}; do python train.py --epochs 2 --gpu-id 1 &> $i\".log\"; done\n",
    "```\n",
    "\n",
    "# Sandbox\n",
    "\n",
    "Always stacking, It's better than scrolling :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Simple(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = list(range(5))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        return x * 2, {'x': x, 'y': x + 5}\n",
    "    \n",
    "data = Simple()\n",
    "data[0]\n",
    "\n",
    "loader = DataLoader(data)\n",
    "\n",
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class NetVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
    "        super(NetVLAD, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.cluster_size = cluster_size\n",
    "        self.add_batch_norm = add_batch_norm\n",
    "        self.out_dim = cluster_size * feature_size\n",
    "        self.clusters = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * torch.randn(feature_size, cluster_size))\n",
    "        self.clusters2 = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * th.randn(1, feature_size, cluster_size))\n",
    "        if add_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_sample = x.shape[1]\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        assignment = th.matmul(x, self.clusters)\n",
    "\n",
    "        if self.add_batch_norm:\n",
    "            assignment = self.batch_norm(assignment)\n",
    "\n",
    "        assignment = F.softmax(assignment, dim=1)\n",
    "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
    "\n",
    "        a_sum = th.sum(assignment, -2, keepdim=True)\n",
    "        a = a_sum * self.clusters2\n",
    "        assignment = assignment.transpose(1, 2)\n",
    "\n",
    "        x = x.view(-1, max_sample, self.feature_size)\n",
    "        vlad = th.matmul(assignment, x)\n",
    "        vlad = vlad.transpose(1, 2)\n",
    "        vlad = vlad - a\n",
    "\n",
    "        # L2 intra norm\n",
    "        vlad = F.normalize(vlad)\n",
    "        \n",
    "        # flattening + L2 norm\n",
    "        vlad = vlad.view(-1, self.cluster_size * self.feature_size)\n",
    "        vlad = F.normalize(vlad)\n",
    "\n",
    "        return vlad\n",
    "\n",
    "class NetRVLAD(nn.Module):\n",
    "    \n",
    "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
    "        super(NetRVLAD, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.cluster_size = cluster_size\n",
    "        self.add_batch_norm = add_batch_norm\n",
    "        self.out_dim = cluster_size * feature_size\n",
    "        self.clusters = nn.Parameter(\n",
    "            (1 / math.sqrt(feature_size)) * th.randn(feature_size, cluster_size))\n",
    "        if self.add_batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        max_sample = x.shape[1]\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        assignment = th.matmul(x, self.clusters)\n",
    "\n",
    "        if self.add_batch_norm:\n",
    "            assignment = self.batch_norm(assignment)\n",
    "\n",
    "        assignment = F.softmax(assignment, dim=1)\n",
    "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
    "        assignment = assignment.transpose(1, 2)\n",
    "\n",
    "        x = x.view(-1, max_sample, self.feature_size)\n",
    "        rvlad = th.matmul(assignment, x)\n",
    "        rvlad = rvlad.transpose(-1, 1)\n",
    "\n",
    "        # L2 intra norm\n",
    "        rvlad = F.normalize(rvlad)\n",
    "        \n",
    "        # flattening + L2 norm\n",
    "        rvlad = rvlad.view(-1, self.cluster_size * self.feature_size)\n",
    "        rvlad = F.normalize(rvlad)\n",
    "\n",
    "        return rvlad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used to check if parameter are changing\n",
    "\n",
    "```python\n",
    "for k, v in net.img_encoder.named_parameters(): print(k, v.sum())\n",
    "for k, v in net.sentence_encoder.named_parameters(): print(k, v.sum())\n",
    "```\n",
    "\n",
    "- Checking that we can hash video-name with 8 integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from didemo import DidemoSMCNRetrieval\n",
    "from didemo import RetrievalMode\n",
    "import numpy as np\n",
    "\n",
    "RGB_FEAT_PATH = 'data/interim/didemo/resnet152/320x240_max.h5'\n",
    "args = dict(context=False, loc=False,\n",
    "            cues=dict(rgb=dict(file=RGB_FEAT_PATH)))\n",
    "\n",
    "for subset in ['val', 'test']:\n",
    "    LIST_PATH = f'data/raw/{subset}_data_wwa.json'\n",
    "    val_dataset = DidemoSMCNRetrieval(LIST_PATH, **args)\n",
    "    val_dataset.mode = RetrievalMode.VIDEO_TO_DESCRIPTION\n",
    "    video_ids = []\n",
    "    for video_j_data in val_dataset:\n",
    "        video_j_ind = video_j_data[0]\n",
    "        video_id = val_dataset.metada_per_video[video_j_ind][0]\n",
    "        video_id_int = int(hashlib.sha256(video_id.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "        video_ids.append(video_id_int)\n",
    "    print(len(video_ids), len(np.unique(video_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick last step of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 4\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers = 1\n",
    "input_dim = 1\n",
    "batch_first = True\n",
    "\n",
    "# Data\n",
    "vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "vec_3 = torch.FloatTensor([[1, 0, 0]])\n",
    "vec_4 = torch.FloatTensor([[2, 0, 0]])\n",
    "\n",
    "# Put the data into a tensor.\n",
    "batch_in = torch.cat([vec_1, vec_2, vec_3, vec_4])\n",
    "batch_in = torch.unsqueeze(batch_in, -1)\n",
    "\n",
    "# Wrap RNN input in a Variable. Shape: (batch_size, max_length, input_dim)\n",
    "# The lengths of each example in the batch. Padding is 0.\n",
    "lengths = torch.LongTensor([3, 2, 1, 1])\n",
    "\n",
    "# Wrap input in packed sequence, with batch_first=True\n",
    "packed_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "    batch_in, lengths, batch_first=True)\n",
    "\n",
    "# Create an RNN object, set batch_first=True\n",
    "rnn = nn.RNN(input_dim, hidden_size, n_layers, batch_first=True) \n",
    "\n",
    "# Run input through RNN \n",
    "packed_output, _ = rnn(packed_input)\n",
    "\n",
    "# Unpack, with batch_first=True.\n",
    "output, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "    packed_output, batch_first=True)\n",
    "print(\"Unpacked, padded output: \")\n",
    "print(output)\n",
    "last_step = output[range(batch_size), lengths - 1, :]\n",
    "print(last_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform warining into errors, useful to debug zero division\n",
    "\n",
    "  ```python\n",
    "  import warnings\n",
    "  warnings.filterwarnings(\"error\")\n",
    "  ```\n",
    "\n",
    "## Crude ideas\n",
    "\n",
    "1. Feature engineering\n",
    "\n",
    "    - Images\n",
    "\n",
    "        - resnet 152/101\n",
    "\n",
    "        - facenet\n",
    "\n",
    "    - Text\n",
    "\n",
    "        - word2vec\n",
    "\n",
    "        - openai language model\n",
    "\n",
    "        - skipthought\n",
    "\n",
    "    - audio\n",
    "\n",
    "        - VGGs from google\n",
    "\n",
    "        - AudioNet Vondrick and others\n",
    "\n",
    "    - Video\n",
    "\n",
    "        - I3D\n",
    "\n",
    "        - NLN\n",
    "\n",
    "1. LSTM with variable sequence length\n",
    "\n",
    "    - query features\n",
    "\n",
    "    - 0/1 representing time to match\n",
    "\n",
    "1. Test approach in other dataset\n",
    "\n",
    "    - Charades or ActivityNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
